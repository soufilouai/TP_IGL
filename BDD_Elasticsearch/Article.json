{"took":50,"timed_out":false,"_shards":{"total":1,"successful":1,"skipped":0,"failed":0},"hits":{"total":{"value":13,"relation":"eq"},"max_score":1.0,"hits":[{"_index":"article_index","_id":"5","_score":1.0,"_source":{"id":5,"title":"Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model","summary":"The emergence of novel types of communication, such as email, has been brought on by the development of the internet, which radically concentrated the way in that individuals communicate socially and with one another. It is now establishing itself as a crucial aspect of the communication network which has been adopted by a variety of commercial enterprises such as retail outlets. So in this research paper, we have built a unique spam-detection methodology based on email-body sentiment analysis. The proposed hybrid model is put into practice and preprocessing the data, extracting the properties, and categorizing data are all steps in the process. To examine the emotive and sequential aspects of texts, we use word embedding and a bi-directional LSTM network. this model frequently shortens the training period, then utilizes the Convolution Layer to extract text features at a higher level for the BiLSTM network. Our model performs better than previous versions, with an accuracy rate of 97–98%. In addition, we show that our model beats not just some well-known machine learning classifiers but also cutting-edge methods for identifying spam communications, demonstrating its superiority on its own. Suggested Ensemble model’s results are examined in terms of recall, accuracy, and precision","keywords":["Dataset","KNN","Gaussian Naive Bayes","LSTM","SVM","Bidirectional LSTM","GRU","Word-Embeddings","CNN"],"content":"Semantic Analysis and Classification of Emails throughInformative Selection of Features and Ensemble AI ModelShivangi SachanDepartment of CSEIIIT LucknowLucknow, UP, Indiamcs21025@iiitl.ac.inKhushbu DoulaniVardhaman College of EngineeringHyderabad, Indiakhushidoulani@gmail.comMainak AdhikariDepartment of CSEIIIT LucknowUP, Indiamainak.ism@gmail.comABSTRACTThe emergence of novel types of communication, such as email, hasbeen brought on by the development of the internet, which radicallyconcentrated the way in that individuals communicate socially andwith one another. It is now establishing itself as a crucial aspect ofthe communication network which has been adopted by a varietyof commercial enterprises such as retail outlets. So in this researchpaper, we have built a unique spam-detection methodology basedon email-body sentiment analysis. The proposed hybrid model isput into practice and preprocessing the data, extracting the proper-ties, and categorizing data are all steps in the process. To examinethe emotive and sequential aspects of texts, we use word embed-ding and a bi-directional LSTM network. this model frequentlyshortens the training period, then utilizes the Convolution Layer toextract text features at a higher level for the BiLSTM network. Ourmodel performs better than previous versions, with an accuracyrate of 9798%. In addition, we show that our model beats not justsome well-known machine learning classifiers but also cutting-edgemethods for identifying spam communications, demonstrating itssuperiority on its own. Suggested Ensemble models results areexamined in terms of recall, accuracy, and precisionCCS CONCEPTSComputer systems organization Embedded systems ;Re-dundancy ; Robotics; NetworksNetwork reliability.KEYWORDSDataset, KNN, Gaussian Naive Bayes, LSTM, SVM, BidirectionalLSTM, GRU, Word-Embeddings, CNNACM Reference Format:Shivangi Sachan, Khushbu Doulani, and Mainak Adhikari. 2023. SemanticAnalysis and Classification of Emails through Informative Selection ofFeatures and Ensemble AI Model. In 2023 Fifteenth International Conferenceon Contemporary Computing (IC3-2023) (IC3 2023), August 0305, 2023, Noida,India. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3607947.3607979Both authors contributed equally to this research.Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.IC3 2023, August 0305, 2023, Noida, India2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0022-4/23/08. . . $15.00https://doi.org/10.1145/3607947.36079791 INTRODUCTIONOver the past few years, a clear surge of both the amount of spam-mers as well as spam emails. This is likely due to a fact that theinvestment necessary for engaging in the spamming industry isrelatively low. As a result of this, we currently have a system thatidentifies every email as suspicious, which has caused major expen-ditures in the investment of defense systems [ 12]. Emails are usedfor online crimes like fraud, hacking, phishing, E-mail bombing, bul-lying, and spamming. [ 16]. Algorithms that are based on machinelearning (ML) are now the most effective and often used approach tothe recognition of spam. Phishing, which is defined as a fraudulentattempt to acquire private information by masquerading as a trust-worthy party in electronic communication, has rapidly advancedpast use of simple techniques and the tactic of casting a wide net;instead, spear phishing uses a variety of sophisticated techniquesto target a single high-value individual. Other researchers used NB,Decision Trees, and SVM to compare the performance of supervisedML algorithms for spam identification [ 6]. Spam emails clog up re-cipients inboxes with unsolicited communications, which frustratethem and push them into the attackers planned traps [ 7]. As a re-sult, spam messages unquestionably pose a risk to both email usersand the Internet community. In addition, Users may occasionallyread the entire text of an unsolicited message that is delivered tothe target users inboxes without realizing that the message is junkand then choosing to avoid it. Building a framework for email spamdetection is the aim of this project. In this approach, we combine theWord-Embedding Network with the CNN layer, Bi-LSTM, and GRU(BiLSTM+GRU). CNN layers are used to speed up training timebefore the Bi-LSTM network, and more advanced textual character-istics are extracted with the use of this network in comparison tothe straight LSTM network, in less time. Gated recurrent neural net-works (GRUs) are then added because they train more quickly andperform better for language modeling. To evaluate and investigatevarious machine learning algorithms for predicting email spam,and develop a hybrid classification algorithm to filter email spambefore employing an ensemble classification algorithm to forecastit. To put an innovative technique into practice and compare it tothe current method in terms of various metrics. Ensemble learn-ing, a successful machine learning paradigm, combines a group oflearners rather than a single learner to forecast unknown targetattributes. Bagging, boosting, voting, and stacking are the four maintypes of ensemble learning techniques. To increase performance,an integrated method and the combining of two or three algorithmsare also suggested. Extraction of text-based features takes a longtime. Furthermore, it can be challenging to extract all of the crucialinformation from a short text. Over the span associated with this181IC3 2023, August 0305, 2023, Noida, India Sachan et al.research, we utilize Bidirectional Large Short-Term Memories (Bi-LSTM) in conjunction with Convolutional Neural Networks (CNN)to come up with an innovative method to the detection of spam.Bagging and boosting approaches were widely preferred in thisstudy. Contribution and paper organization is as follows: section 1.1describes literature study, section 1.2 describe motivation for thisresearch work, section 2 sketches procedure of details implemen-tation, Section 3 present experimental setup, dataset descriptionand evaluation metrics, and section 4 summarizing outcomes of theexperiment.1.1 Related WorkEmail is indeed the second most frequently utilized Internet appli-cation as well as the third most common method of cyberbullying,claims one study. Cybercriminals exploit it in a number of ways,including as sending obscene or abusive messages, adding virusesto emails, snatching the private information of victims, and ex-posing it to a broad audience. Spam letters made up 53.95% of allemail traffic in March 2020. We examine three main types of un-lawful emails in our study. First are fake emails, which are sentto manipulate recipients to submit sensitive information. The sec-ond as being cyberbullyings use of harassing emails to threatenindividuals. Suspicious emails that describe illegal activities belongto the third category. Many researchers have earlier contributedmassively to this subject. The researcher claims there is some proofthat suspicious emails were sent before to the events of 9/11. [ 14].When it comes to data labeling, there are also convinced rule-basedapproaches and technologies ( like VADER) that are used, eventhough their efficiency of the are together is adversely affected. Ahidden layer, which itself is essential for vectorization, is the toplayer of the model. We use oversampling methods for this minorityclass because of the absence of data. Sampling techniques can helpwith multicollinearity, but they have an impact on simulation re-sults. Oversampling causes data to be randomly repeated, whichaffects test data because dividing data may result in duplicates. Un-dersampling may result in the loss of some strong information. Inorder to advance email research, it is crucial to provide datasets oncriminal activity. P. Garg et al. (2021) [ 5], which revealed that spamin an email was detected in 70 percent of business emails, spam wasestablished as an obstacle for email administrators. Recognizingspam and getting rid of it were the primary concerns, as spam canbe offensive, may lead to other internet sites being tricked, whichcan offer harmful data, and can feature those who are not particu-lar with their content using NLP. To select the best-trained model,each mail transmission protocol requires precise and effective emailclassification, a machine learning comparison is done. Our studyhas suggested that innovative deep learning outperforms learningalgorithms like SVM and RF. Current studies on the classificationof emails use a variety of machine learning (ML) techniques, witha few of them focusing on the study of the sentiments consisted ofwithin email databases. The lack of datasets is a significant obstacleto email classification. There are few publicly accessible E-maildatasets, thus researchers must use these datasets to test their hy-potheses or gather data on their own. Authors[ 15] describe suppliedtwo-phased outlier detection models to enhance the IIOT networksdependability. Artificial Neural Network, SVM, Gaussian NB, andRF (random forest) ensemble techniques were performed to forecastclass labels, and the outputs were input into a classifying unit toincrease accuracy. A method for content-based phishing detectionwas presented by the authors in [ 2], to classify phishing emails,they employed RF. They categorize spam and phishing emails. Theyenhanced phishing email classifiers with more accurate predictionsby extracting features. They showed some effective Machine learn-ing spam filtering techniques. When the PCA method is used, it willlower the number of features in the dataset. The collected featuresgo through the PCA algorithm to reduce the number of features.The PCA method is used to make a straightforward representationof the information which illustrates the amount of variability thereis in the data. The authors of [ 20] presented the Fuzzy C-meansmethod for classifying spam email. To stop spam, they implementeda membership threshold value. A methodology to identify unla-beled data was put forth by the authors of [ 1] and applied motiveanalysis to the Enron data collection. They divided the data intocategories that were favorable, negative, and neutral. They groupedthe data using k-means clustering, an unsupervised ML techniqueand then classified it using the supervised ML techniques SVM andNB. Hina, Maryam, and colleagues (2021) implemented Sefaced:Deep learning-based semantic analysis and categorization of e-maildata using a forensic technique. For multiclass email classification,SeFACED employs a Gated Recurrent Neural Network (GRU) basedon Long Short-Term Memory (LSTM). Different random weight ini-tializations affect LSTMs [ 9]. Zhang, Yan, et al.(2019) Experimentson three-way game-theoretic rough set (GTRS) email spam filter-ing show that it is feasible to significantly boost coverage withoutdecreasing accuracy [ 23]. According to Xia et al. [ 22], SMS spamhas been identified using machine learning model such as naivebayes , vector-space modeling, support vector machines (SVM),long selective memory machines (LSTM), and convolutional neuralnetworks including every instance of a method for categorizingdata. Elshoush, Huwaida, et al. (2019) Using adaboost and stochasticgradient descent (sgd) algorithms for e-mail filtering with R andorange software spam [ 3]. Orange software was used to create theclassifications, which included Adaboost and SGD. The majority ofresearchers focused on text-based email spam classification meth-ods because image-based spam can be filtered in the early stagesof pre-processing. There are widely used word bag (BoW) model,which believes that documents are merely unordered collectionsof words, is the foundation for these techniques. Kumaresan [ 11]explains SVM with a cuckoo search algorithm was used to extracttextual features for spam detection. Renuka and Visalakshi madeuse of svm [ 17] spam email identification, followed by selectingfeatures using Latent Semantic Indexing (LSI). Here we have usedlabeled dataset to train the hybrid classifier. We used TF-IDF forfeature extraction [ 20] and Textual features for spam detectionwere extracted using SVM and a cuckoo search algorithm. [ 4] forfiltering out the spam email. Combining the integrated strategy tothe pure SVM and NB methods, overall accuracy is really improved.Moreover, accurate detection for spam email has been proposedusing the Negative Selection Algorithm (NSA) and Particle SwarmOptimizations (PSO) algorithm. PSO is used in this instance toimprove the effectiveness of the classifier.182Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model IC3 2023, August 0305, 2023, Noida, India1.2 Motivation and NoveltyEmail is most common form of communication between peoplein this digital age. Many users have been victims of spam emails,and their personal information has been compromised. The emailClassification technique is employed to identify and filter junkmail, junk, and virus-infected emails prior to reach a users inbox.Existing email classification methods result in irrelevant emailsand/or the loss of valuable information. Keeping these constraintsin mind, the following contributions are made in this paper:Text-based feature extraction is a lengthy process. Further-more, extracting every important feature from text is difficult.In this paper, we show how to employ GRU with Convo-lutional Neural Networks and Bidirectional-LSTM to findspam.Used Word-Embeddings, BiLSTM, and Gated Recurrent Neu-ral Networks to examine the relationships, sentimental con-tent, and sequential way of email contents.Applied CNN before the Bi-LSTM network, training time canbe sped up. This network can also extract more advancedtextual features faster than the Bi-LSTM network alone whencombined with the GRU network.We use Enorn Corpora datasets and compute precision, re-call, and f-score to assess how well the suggested techniqueperforms. Our model outperforms several well-known ma-chine learning techniques as well as more contemporarymethods for spam message detection.2 PROPOSED SYSTEM ARCHITECTURE ANDMODELE-mail is a valuable tool for communicating with other users. Emailallows the sender to efficiently forward millions of advertisementsat no cost. Unfortunately, this scheme is now being used in a varietyof organizations. As a result, a massive amount of redundant emailsis known as spam or junk mail, many people are confused about theemails in their E- Mailboxes. Each learning sequence is given for-ward as well as backward to two different LSTM networks that areattached to the same outputs layer in order for bidirectional Lstmsto function. This indicates that the Bi-LSTM has detailed sequentialinformation about all points before and following each point in aspecific sequence. In other words, we concatenate the outputs fromboth the forward and the backward LSTM at each time step ratherthan just encoding the sequence in the forward direction. Eachwords encoded form now comprehends the words that come beforeand after it. This is a problem for the Internet community. The di-agram depicts various stages that aid in the prediction of email spam:Because real-world data is messy and contains unnecessary infor-mation and duplication, data preprocessing is critical in naturallanguage processing (NLP). The major preprocessing steps are de-picted below.2.1 NLP TokenizationTokenization of documents into words follows predefined rules.The tokenization step is carried out in Python with spacy library.2.2 Stop Words RemovalStop words appear infrequently or frequently in the document, butthey are less significant in terms of importance. As a result, theseare removed to improve data processing.2.3 Text NormalizationA words lexicon form or order may differ. Thus, they must all bechanged to their root word to be correctly analyzed. Lemmatizationand stemming are the two methods that can be used for normal-ization. When a words final few characters are removed to createa shorter form, even if that form has no meaning, the procedureis known as stemming. lemmatization [ 21] is a mixture of corpus-based an rule-based methods, and it retains the context of a termwhile changing it back to its root.2.4 Feature Extractionfeature extraction which transforms the initial text into its featuresso that it may be used for modeling after being cleaned up andnormalized. Before predicting them, we use a specific way to giveweights to specific terms in our document. While it is simple for acomputer to process numbers, we choose to represent individualwords numerically. In such cases, we choose word embeddings. IDFis the count of documents containing the term divided by the totalnumber of documents, and occurrence is the amount of instances aword appears in a document. We derive characteristics based onequations. 1,2,3,4,5, and 6. We use equations to derive properties. =1(1) =Inverse() (2)(,,)=(,).(,) (3)(,)=log||(4)A word2vec neural network-based approach is the method that isutilized for this goal as the tool. The following equation, referredto as 5, shows how word2vec handles word context through theuse of probability-accurate measurements. Here letter D stands forthe paired-wise display of a set of words, while the letters w and c0or c1 represent paired word context that originated from a largercollection of set D.(=1|, 11:)=11+(11+12+...+1)(5)(=1|, 1:)=11+(0)(6)183IC3 2023, August 0305, 2023, Noida, India Sachan et al.2.5 Word-EmbeddingsWord-Embedding helps to improve on the typical \"bag-of-words\"worldview, which requires a massive sparse feature vector to scoreevery word individually to represent this same entire vocabulary.This perception is sparse because the vocabulary is large, and eachword or document is defined by a massive vector. Using a wordmap-based dictionary, word embedding needs to be converted terms(words) into real value feature vectors. There are two basic issueswith standard feature engineering techniques for deep learning.Data is represented using sparse vectors, and the second is thatsome of the meanings of words are not taken into consideration.Similar phrases will have values in embedding vectors that arealmost real-valued. The Input length in our proposed study is setto 700 for our suggested model. If the texts seemed to be integerencoded with value systems between 10 and 20, the vocabularydistance would be 11. Our data is encoded as integers, and the inputand output dimensions are both set to 50,000. The embedding layeroutcome will be used in successive layers and for BiLSTM and GRUlayers.2.6 Machine Learning ModelWithin the scope of the research, we are using the subsequent ma-chine learning techniques, to examine and compare the overallefficacy of our suggested Bi-LSTM strategy: Support Vector Ma-chine, Gaussian NB, Logistic Regression, K - nearest neighbors, andRandom Forest (RF).2.7 Convolution NetworkThe popular RNN model generally performs well but takes toolong to train the model incorporating the textual sequential data.When a layer is added after the RNN layer, the models learningduration is considerably decreased. Higher-level feature extractionis another benefit. [ 19] additionally possible using the convolutionallayer. In essence, the convolution layer looks for combinations ofthe various words or paragraphs in the document that involve thefilters. We use features with 128 dimensions and a size 10 for each.For this task, the Relu activation function is utilized. After that, theone-dimensional largest pooling layers with a pooling size of 4 areput on the data in order to obtain higher-level features.2.8 BiLSTM Network with GRURecurrent Neural Network (RNN) technique of text sentiment anal-ysis is particularly well-liked and frequently applied. Recurrentneural networks (RNN) surpass conventional neural networks. be-cause it can remember the information from earlier time stepsthanks to its memory. A state vector is combined with an RNNsdata to create a new state vector. The resulting state vector uses thepresent to recollect past knowledge. The RNN is straightforwardand is based on the following equations:=tanh(1+) (7)= (8)The vanilla RNN[ 18]is not very good at remembering previoussequences. In addition to that, RNN struggles with diminishinggradient descent. A kind of RNN is a long short-term recall network(LSTM), solves a vanishing gradient descent problem and learnslong-term dependencies[ 10]. LSTM was actually created to addressthe problem of long-term reliance. LSTM has the unique ability torecall. The cell state is the LSTM models central concept. Withonly a small amount of linear interaction, the cell state follows thesequence essentially unmodified from beginning to end. gate ofan LSTM is also significant. Under the command of these gates,information is safely inserted to or eliminated from the cell stated.The following equations are used by the LSTM model to updateeach cell:=[1,]+(9)In this case, Xt denotes input, and ht is the hidden state at the ttime step. The following is the revised cell state Ct:t=([1,]+) (10)=tanh([1,]+) (11)=1+ (12)Here, we may compute the output and hidden state at t time stepsusing the point-wise multiplication operator *.=([1,]+) (13)=tanh() (14)Due to the reality it only considers all prior contexts from thepresent one, LSTM does have a few drawbacks. As a result of this,it may accept data from preceding time steps through LSTM as wellas RNN. Therefore, in order to avoid this issue, further improve-ments are carried out with the help of a bidirectional recurrentneural network(Bi-RNN). BiRNN [ 13] can handle two pieces of in-formation from both the front and the back. Bi-LSTM is createdby combining the Bi-RNN and LSTM. As a result, operating LSTMhas advantages such as cell state storage so that BiRNN have wayto acknowledge from the context before and after. As a conse-quence of this, it provides the Bi-LSTM with the advantages of anLSTM with feedback for the next layer. Remembering long-termdependencies is a significant new benefit of Bi-LSTM. The output,which is a feature vector, will be based on the call state. Finally,we forecast the probability of email content as Normal, Fraudu-lent, Harassment, and Suspicious Emails using as an input to thesoftmax activation function, which is a weighted sum of the denselayers outputs. To regulate the information flow, GRU employsthe point-wise multiplying function and logistic sigmoid activation.The GRU has hidden states of storage memory and does not havedistinct memory cells or units for state control. The W, U, and bvectors, which stand for weights, gates, and biases, respectively, arecrucial variables that must be calculated during the creation of theGRU model. For training reasons, the pre-trained word embeddingknown as the Glove vector is used. They made it clear that GRUis the superior model when there is a large amount of trainingdata for textual groups and word embedding is available. BiLSTM,CNN, and GRU is required so as to compensate for the deletionof the documents long-term and short-term connections. In ourcase, the embedding dimension, maximum sequence length, andlexicon size were used to start the LSTM embedding layer in threeseparate LSTM models. The input vector was modified to make itappropriate for such a Conv1D layer, prior situations sequences arereturned by LSTM layer. The \"return sequences\" of the LSTM layermust be set to False when the subsequent state is free of the gated184Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model IC3 2023, August 0305, 2023, Noida, Indiaarchitecture. Quantity of learning parameters must be taken intoconsideration. A 350-unit LSTM layer was set - up, and differentLSTM unit combinations were tested. More importantly, becauseit has more parts, the model made with BiLSTM will take longerto train. Bidirectional LSTM is the name of a particular kind ofrecurrent neural network that is primarily used for the processingof natural languages. (BiLSTM). It is able to use data from bothsides, and, in contrast to regular LSTM, it enables input flow inboth directions. It is an effective instrument for demonstrating thelogical relationships between words and phrases, and this involvesboth the forward and backward directions of the sequence. In con-clusion, BiLSTM works by adding one extra layer of LSTM, causingthe information flow to travel in the other direction. It only denotesthat the input sequence runs in reverse at the next LSTM layer. Mul-tiple operations, including averaging, summation, multiplication,and concatenation, are then applied to the results of the two LSTMlayers. The gated design of Bi-LSTM and GRU networks solvesthe disappearing gradient and exploding problems. A good way tohandle more long sequences is to use Bi-LSMT and GRU together.GRU works well with datasets that dont have text. In two to threerounds, the complicated CNN+BiLSTM+GRU model learns the longsequence of email text well. We have used word embedding, cnn,bidirectional lstm and gru networks as our three building blocksto separate email messages based on their sentiment and textssequential features. Also, we succinctly demonstrate below whythese blocks help identify email spam:First, We have used the Sequence - to - sequence Lstm as thecurrent block in the networks since it can retrieve both theprevious and next sequences from the current. More so thana straightforward LSTM network, it can also recognize andextract text sentiment and sequential properties.Second, we extract the more complex and advanced charac-teristics for Bi-LSTM network using Convolutional Networkblock, which is the networks second block after the Bi-LSTMblock. Bi-LSTM takes a long time to extract text-based fea-tures, hence one of the reasons for using this block is toreduce the networks overall training time.3 EXPERIMENTAL EVALUATION3.1 Experimental SetupWe divided the information into training and testing groups of80/20. We divided the remaining 20% of the 80 percent trainingdata into test data for the model. Construct, compute, and evaluatethe efficacy of the suggested method using the Pythonic packagesKeras, as TensorFlow and Scikit learn.3.2 Dataset DescriptionEmail spam detection is the foundation of this research project. Thedataset includes normal emails from the Enron corpora, deceptiveemails from phished email corpora, harassment emails chosen fromhate speech, and the offensive dataset. Only the content of the emailbody is used for analysis; all header information, including sender,topic, CC, and BCC, are eliminated. Word2vector, TF-IDF, and WordEmbedding are used to extract characteristics from the email mes-sage and classify them. This dataset[ 8] is publicly available. Thepresented model is implemented using Python, and several metrics,including accuracy, precision, and recall, are used to examine theoutcomes.3.3 Evaluation Metrics and ResultsClassifier performance is assessed Using metrics such as accuracy,precision, and recall. Four terms make up a confusion matrix thatis used to calculate these metrics.True positives (TP) are positive values that have been accu-rately assigned the positive label.The negative values that are accurately identified as negativeare known as True Negatives (TN).True Negative values are those that can be accurately identi-fied as being negative (TN).Positive readings that have been mistakenly labeled as nega-tive are known as False Negatives (FN).Assess the efficacy of the suggested model is listed below:3.3.1 Accuracy. Accuracy reveals how frequently the ML modelwas overall correct.Accuracy =++++(15)3.3.2 Precision. The accuracy of the model gauges how effectivelyit can predict a specific category.Precision =+(16)3.3.3 Recall. Recall tells us how often the model was able to rec-ognize a specific category.Recall =+(17)Model Accuracy Precision RecallGaussian NB 91.3 90.1 91.8Random Forest 88.41 90 88KNN 86.6 89 87SVM 92.4 91 92LSTM 95.2 95 95.7Proposed Ensemble(CNN,BiLSTM+GRU)97.32 95.6 95.3Table 1: Differet Models Score on Test DataAccuracy, Precision, and Recall metrics are computed. In thegiven Table 1 where six different classifiers are Gaussian NB, Ran-dom Forest, KNN, SVM, LSTM, and Propose Ensemble HybridModel (CNN+BiLSTM+GRU) have been used in this work. In theCNN, Bi-LSTM, and GRU architectures which enable sequence pre-diction, CNN strands for feature extraction on data input which arecombined with LSTM. It requires less time training and a higherexpandable model. Any bottlenecks are created by predictions andthe increasing number of distinct units of information. This modelis useful for dealing with issue-related classifications that consistof two or more than two classes. So suggested Ensemble model, outof these six classifiers, produces more accurate findings.185IC3 2023, August 0305, 2023, Noida, India Sachan et al.Figure 1: Performance Analysis3.4 Comparative AnalysisAmodels ability to fit new data is measured by the validationloss, whereas its ability to fit training data is determined by thetraining loss. The two main variables that decide whether in whichlearning is efficient or not are validation loss and training loss.LSTM and Suggested Ensemble hybrid Models have equivalent lossand accuracy. In this context, we are contrasting the LSTM with theproposed model (CNN, Bilstm, and GRU) in terms of their respectivevalidation accuracies and losses. The models accuracy was at itshighest after 14 epochs of operation when it achieved an accuracyof roughly 97-98% while minimizing model loss.Figure 2: LSTM Model Training and Validation AccuracyFigure 3: LSTM Model Training and Validation LossFigure 4: Ensemble Model (CNN,BiLSTM+GRU) Trainingand Validation AccuracyFigure 5: Ensemble Model (CNN,BiLSTM+GRU)Trainingand Validation Loss186Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model IC3 2023, August 0305, 2023, Noida, IndiaIn this Proposed ensemble hybrid models train accuracy is 98.7%Validation accuracy is 97.32% and LSTM has train accuracy of 97.41%and validation accuracy is 95.2%. So based on figures 3 and 5 indicatethe validation loss for LSTM and the proposed ensemble hybridmodel to be 0.93 and 0.84, respectively, and figures 2 and 4 show thevalidation accuracy to be 95.2% and 97.3%, respectively. LSTM andthe proposed hybrid model used ensemble artificial intelligence,with the proposed hybrid model outperforming the LSTM. Wedecide on dense architecture as the final model for identifying thetext messages as spam or nonspam based on loss, accuracy, and theaforementioned charts. The loss and accuracy over epochs are morestable than LSTM, and the Proposed classifier has a straightforwardstructure.4 CONCLUSIONThe model is composed of four networks Word-Embeddings, CNN,Bi-LSTM, and GRU. We may train the model more quickly by usingthe convolutional layer first, followed by the word-embedding layer,and then the BiLSTM network. The Bidirectional LSTM networkalso has higher-level properties that we can extract. We have useda bidirectional LSTM(BiLSTM)and GRU network to memorize asentences contextual meaning and sequential structure, which im-proves the models performance accuracy to roughly 97.32 percent.REFERENCES[1]Rayan Salah Hag Ali and Neamat El Gayar. 2019. Sentiment analysis using unla-beled email data. In 2019 International Conference on Computational Intelligenceand Knowledge Economy (ICCIKE) . IEEE, 328333.[2]Ali Shafigh Aski and Navid Khalilzadeh Sourati. 2016. Proposed efficient algo-rithm to filter spam using machine learning techniques. Pacific Science Review A:Natural Science and Engineering 18, 2 (2016), 145149.[3]Huwaida T Elshoush and Esraa A Dinar. 2019. Using adaboost and stochasticgradient descent (sgd) algorithms with R and orange software for filtering e-mailspam. In 2019 11th Computer Science and Electronic Engineering (CEEC) . IEEE,4146.[4]Weimiao Feng, Jianguo Sun, Liguo Zhang, Cuiling Cao, and Qing Yang. 2016. Asupport vector machine based naive Bayes algorithm for spam filtering. In 2016IEEE 35th International Performance Computing and Communications Conference(IPCCC) . IEEE, 18.[5]Pranjul Garg and Nancy Girdhar. 2021. A Systematic Review on Spam FilteringTechniques based on Natural Language Processing Framework. In 2021 11th Inter-national Conference on Cloud Computing, Data Science & Engineering (Confluence) .IEEE, 3035.[6]Adam Kavon Ghazi-Tehrani and Henry N Pontell. 2021. Phishing evolves: Ana-lyzing the enduring cybercrime. Victims & Offenders 16, 3 (2021), 316342.[7]Radicati Group et al .2015. Email Statistics Report 20152019. Radicati Group.Accessed August 13 (2015), 2019.[8]Maryam Hina, Mohsin Ali, and Javed. 2021. Sefaced: Semantic-based forensicanalysis and classification of e-mail data using deep learning. IEEE Access 9(2021), 9839898411.[9]Maryam Hina, Mohsin Ali, Abdul Rehman Javed, Fahad Ghabban, Liaqat AliKhan, and Zunera Jalil. 2021. Sefaced: Semantic-based forensic analysis andclassification of e-mail data using deep learning. IEEE Access 9 (2021), 9839898411.[10] Weicong Kong, Zhao Yang Dong, Youwei Jia, David J Hill, Yan Xu, and YuanZhang. 2017. Short-term residential load forecasting based on LSTM recurrentneural network. IEEE transactions on smart grid 10, 1 (2017), 841851.[11] T Kumaresan and C Palanisamy. 2017. E-mail spam classification using S-cuckoosearch and support vector machine. International Journal of Bio-Inspired Compu-tation 9, 3 (2017), 142156.[12] Nuha H Marza, Mehdi E Manaa, and Hussein A Lafta. 2021. Classification ofspam emails using deep learning. In 2021 1st Babylon International Conference onInformation Technology and Science (BICITS) . IEEE, 6368.[13] Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neuralnetwork language model. In 2012 IEEE Spoken Language Technology Workshop(SLT) . IEEE, 234239.[14] Sarwat Nizamani, Nasrullah Memon, Mathies Glasdam, and Dong Duong Nguyen.2014. Detection of fraudulent emails by employing advanced feature abundance.Egyptian Informatics Journal 15, 3 (2014), 169174.[15] V Priya, I Sumaiya Thaseen, Thippa Reddy Gadekallu, Mohamed K Aboudaif,and Emad Abouel Nasr. 2021. Robust attack detection approach for IIoT usingensemble classifier. arXiv preprint arXiv:2102.01515 (2021).[16] Justinas Rastenis, Simona Ramanauskait e, Justinas Januleviius, Antanas enys,Asta Slotkien e, and Kstutis Pakrijauskas. 2020. E-mail-based phishing attacktaxonomy. Applied Sciences 10, 7 (2020), 2363.[17] Karthika D Renuka and P Visalakshi. 2014. Latent semantic indexing based SVMmodel for email spam classification. (2014).[18] Shuvendu Roy, Sk Imran Hossain, MAH Akhand, and N Siddique. 2018. Sequencemodeling for intelligent typing assistant with Bangla and English keyboard. In2018 International Conference on Innovation in Engineering and Technology (ICIET) .IEEE, 16.[19] Tara N Sainath, Oriol Vinyals, Andrew Senior, and Haim Sak. 2015. Convolu-tional, long short-term memory, fully connected deep neural networks. In 2015IEEE international conference on acoustics, speech and signal processing (ICASSP) .Ieee, 45804584.[20] Anuj Kumar Singh, Shashi Bhushan, and Sonakshi Vij. 2019. Filtering spammessages and mails using fuzzy C means algorithm. In 2019 4th InternationalConference on Internet of Things: Smart Innovation and Usages (IoT-SIU) . IEEE,15.[21] Kristina Toutanova and Colin Cherry. 2009. A global model for joint lemmati-zation and part-of-speech prediction. In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP . 486494.[22] Tian Xia. 2020. A constant time complexity spam detection algorithm for boostingthroughput on rule-based filtering systems. IEEE Access 8 (2020), 8265382661.[23] Yan Zhang, PengFei Liu, and JingTao Yao. 2019. Three-way email spam filteringwith game-theoretic rough sets. In 2019 International conference on computing,networking and communications (ICNC) . IEEE, 552556.Received 15 April 2023187","date":"2023-08-03","authors":[{"full_name":"Shivangi Sachan","institut_name":"Department of CSE, IIIT Lucknow"},{"full_name":"Khushbu Doulani","institut_name":"Vardhaman College of Engineering, Hyderabad, India"},{"full_name":"Mainak Adhikari","institut_name":"Department of CSE, IIIT Lucknow, UP, India"}],"author":[{"full_name":"Shivangi Sachan","institut_name":"Department of CSE, IIIT Lucknow"},{"full_name":"Khushbu Doulani","institut_name":"Vardhaman College of Engineering, Hyderabad, India"},{"full_name":"Mainak Adhikari","institut_name":"Department of CSE, IIIT Lucknow, UP, India"}]}},{"_index":"article_index","_id":"2","_score":1.0,"_source":{"id":2,"title":"test 2","summary":"akzdnalzkndlakz akjzdnjn kdajzd","keywords":"keyword1 , keyword2 , test","content":"this is a contenet 2","date":"2024-02-03","authors":[],"author":[]}},{"_index":"article_index","_id":"3","_score":1.0,"_source":{"id":3,"title":"einstein","summary":"this is a summary ","keywords":"hello,world","content":"content","date":"2023-02-01","authors":[],"author":[]}},{"_index":"article_index","_id":"1","_score":1.0,"_source":{"id":1,"title":"test","summary":"akzdnalzkndlakz akjzdnjn kdajzd","keywords":"keyword1 , keyword2 , test","content":"this is a contenet","date":"2024-01-18","authors":[{"full_name":"einstein","institut_name":"berlin"}],"author":[{"full_name":"einstein","institut_name":"berlin"}]}},{"_index":"article_index","_id":"8","_score":1.0,"_source":{"id":8,"title":"ModelGame: A Quality Model for Gamified Software Modeling Learning","summary":"Gamification has been adopted in software development tasks in recent years. This adoption seeks, for example, to improve the engagement of developers while creating UML models or writing code. Empirical studies report that UML models suffer from incompleteness and inconsistency problems. This study conjectures that gamification mechanics can improve learner engagement while learning software modeling, mitigating such problems concerning UML models. The current literature lacks studies that explore gamification and UML model quality in the context of software modeling learning. This article, therefore, proposes ModelGame, which is a quality model to support software modeling learning in a gamified way. It serves as a reference framework so that instructors can obtain a parameterized way to evaluate UML models created by learners. The quality of UML models can be improved by applying gamified activities and providing guidelines aware of quality issues. A qualitative questionnaire was answered by 19 instructors who teach software modeling at higher education institutions. The results show that (1) 94.7% recognize that the proposed model can improve the quality of UML models, indicating that they would adopt the ModelGame in their learning practices; and (2) 47.4% do not use any gamification mechanics in their classes. The results are encouraging, showing the potential for applying and improving the teaching and learning of software modeling.","keywords":"Model design, learning model, Gamification","content":"ModelGame: A Quality Model for Gamified Software ModelingLearningEd Wilson JniorUniversidade do Vale do Rio dos SinosSo Leopoldo, Rio Grande do Sul, Braziledwjr7@edu.unisinos.brKleinner FariasUniversidade do Vale do Rio dos SinosSo Leopoldo, Rio Grande do Sul, Brazilkleinnerfarias@unisinos.brABSTRACTGamification has been adopted in software development tasks inrecent years. This adoption seeks, for example, to improve the en-gagement of developers while creating UML models or writingcode. Empirical studies report that UML models suffer from incom-pleteness and inconsistency problems. This study conjectures thatgamification mechanics can improve learner engagement whilelearning software modeling, mitigating such problems concern-ing UML models. The current literature lacks studies that exploregamification and UML model quality in the context of softwaremodeling learning. This article, therefore, proposes ModelGame ,which is a quality model to support software modeling learning in agamified way. It serves as a reference framework so that instructorscan obtain a parameterized way to evaluate UML models createdby learners. The quality of UML models can be improved by apply-ing gamified activities and providing guidelines aware of qualityissues. A qualitative questionnaire was answered by 19 instructorswho teach software modeling at higher education institutions. Theresults show that (1) 94.7% recognize that the proposed model canimprove the quality of UML models, indicating that they wouldadopt the ModelGame in their learning practices; and (2) 47.4% donot use any gamification mechanics in their classes. The results areencouraging, showing the potential for applying and improvingthe teaching and learning of software modeling.CCS CONCEPTSSoftware and its engineering Software design engineer-ing.KEYWORDSModel design, learning model, GamificationACM Reference Format:Ed Wilson Jnior and Kleinner Farias. 2021. ModelGame: A Quality Modelfor Gamified Software Modeling Learning. In 15th Brazilian Symposiumon Software Components, Architectures, and Reuse (SBCARS 21), September27-October 1, 2021, Joinville, Brazil. ACM, New York, NY, USA, 10 pages.https://doi.org/10.1145/3483899.3483910Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from permissions@acm.org.SBCARS 21, September 27-October 1, 2021, Joinville, Brazil2021 Association for Computing Machinery.ACM ISBN 978-1-4503-8419-3/21/09. . . $15.00https://doi.org/10.1145/3483899.34839101 INTRODUCTIONGamification has been adopted in software development tasks inrecent years. This adoption seeks, for example, to improve the en-gagement of developers while creating UML models or writingcode. Empirical studies [ 7,9,14] report that UML models sufferfrom incompleteness and inconsistency problems. Lange [ 14] rein-forces that these defects bring potential risks that can cause mis-interpretation and communication failure, representing a risk tosoftware quality. Thus, finding formats that favor student learningand consequently in generating increasingly effective UML mod-els can become one of the main challenges faced by instructorsthat include UML (Unified Modeling Language) as part of softwaremodeling content.Some studies [ 3,12,25] sought to understand how to apply gam-ification in software modeling teaching using some elements suchas points, emblems and levels. However, instructors and researchersstill find limitations when applying, evaluating, and measuring theuse of this tool in the learning of software modeling students and,consequently, in the models developed by them, since in the currentliterature there is no frame of reference that guides them. Thisstudy conjectures that gamification mechanics can improve learnerengagement while learning software modeling, mitigating suchproblems concerning UML models. The current literature lacksstudies that explore gamification and model quality in the contextof software modeling learning.This article, therefore, introduces ModelGame , which is a qualitymodel to support software modeling learning in a gamified way.It serves as a reference framework so that instructors can obtaina parameterized way to evaluate UML models created by learners.The quality of UML models can be improved by applying gami-fied activities and providing guidelines aware of quality issues. Areference framework would help to (1) establish parameters for eval-uating UML models created by learners; (2) provide guidelines toimprove the quality of these artifacts; (3) to analyze which elementsof gamification could be included in each of the phases of modelingusing UML; (4) identify intrinsic and extrinsic aspects of studentsduring the modeling stages, to improve the models; (5) to comparevalidated theories about the inclusion of gamification in softwaremodeling teaching, taking into account the types of learning andmethodologies used; and (6) contributing to the identification ofgamification use objectives in modeling activities.A qualitative questionnaire was answered by 19 instructors whoteach software modeling at higher education institutions. The re-sults show that (1) 94.7% recognize that the proposed model canimprove the quality of UML models, indicating that they wouldadopt it in their learning practices; and (2) 47.4% do not use any100SBCARS 21, September 27-October 1, 2021, Joinville, Brazil Ed Wilson Jnior and Kleinner Fariasgamification mechanics in their classes. These results are encourag-ing, showing the potential for applying and improving the teachingand learning of software modeling.The remainder of the paper is organized as follows. Section 2presents the main concepts discussed throughout the article. Sec-tion 3 discusses the related work, highlighting research opportu-nities. Section 4 introduces the proposed quality model. Section 5presents how the quality model was evaluated. Section 6 points outsome threats to validity. Finally, Section 7 presents some concludingremarks and future work.2 BACKGROUNDThis section presents the essential concepts for understanding thiswork, including gamification and software engineering teaching(Section 2.1), and software modeling and model quality (Section2.2).2.1 Gamification and Software EngineeringTeachingGamification aims to use game elements in the context of not game[5], bringing all positive aspects they provide as a way to encourageand engage players, thereby broadening their motivations.Werbach [ 23] classifies gamification into three dimensions: Dy-namics, Mechanics, and Components. Dynamics include all gameaspects related to the emotional responses of players (e.g., rela-tionship, progression, and narrative). Mechanics offer elements thatpromote the action of a game  usually elaborated via a rule-baseddevelopment , so that the player can interact with such elements,e.g., challenges, feedback, and rewards. Components representthe aesthetic elements of gamification, whose goal is to presentvisual aspects with which players can perform the interaction, forexample, points, scores, and emblems (badges).Knowing that the teaching of Software Engineering should in-volve students to experience the professional practices of the areaso that they can understand which practices and techniques areuseful in several different situations [ 2]. The challenges of teachingnew software engineers are not limited to learning programming,but also include paying attention to detail, considering the qualityof created models, established schedule and defined budgets [ 1]. Inaddition to understanding the technical challenges, these futureprofessionals must be up to date with nontechnical issues, includingteamwork, communication and management.To meet these new demands of the current context, the formatwith exhibition classes is no longer considered enough and mayeven become demotivating and ineffective in learning students. Inthis sense, gamification has been increasingly used in the teach-ing of software engineering as a way to promote behavioral andpsychological changes [ 11] providing an environment that favorscommunication, cooperation, feedback, reward, achievement andother recurring elements that are capable of improving perfor-mance, efficiency and engagement in educational activities , andcan enhance, for example, the learning of software modeling.2.2 Software Modeling and Model QualitySoftware modeling encompasses the set of principles, concepts, andpractices that lead to the development of a high-quality system orproduct. The principles of this activity establish a philosophy thatguides the entire software development process.In this scenario, UML models play a crucial role in softwaredevelopment tasks, for example, documenting project decisions,understanding development details, promoting better communica-tion between teams, and generating greater efficiency in softwaredevelopment [ 19]. However, these models suffer problems of in-consistency and incompleteness [ 10,18], as well as end up beingoverlooked within the modeling process, as pointed out in someempirical studies in the literature [ 14,15]. Class and sequence dia-grams, for example, present inconsistencies when sequence diagramobjects are not found in the class diagram, consequently developersend up living with inconsistencies throughout the developmentprocess.A research challenge still open is how to evaluate these diagrams,both in industry and in the teaching process, in terms of quality,such as syntactic and semantic, for example.3 RELATED WORKThe selection of related works was carried out following two steps:(1) search in digital repositories, such as Google Scholar andScopus(Elsevier) of articles related to gamification, quality modeling, andmodeling learning; and (2) filter selected articles considering thealignment of such works with the objective of the work (Section 4).After selecting the works, they were analyzed (Section 3.1) andcompared (Section 3.2), seeking to identify research opportunities.3.1 Analysis of Related WorksPorto et al. (2021) [4]. This work performed a systematic map-ping with the objective of characterizing how gamification hasbeen adopted in noneducational contexts of software engineeringactivities. The main results of this study show that gamificationprovided benefits for activities such as requirements specification,development, testing, project management, and support process.In addition, he pointed out that the number of publications andnew research initiatives has increased over the years, many posi-tive results have been achieved in software engineering activities.Nevertheless, the study reinforced that gamification can still beexplored for other tasks in this area, as empirical evidence is verylimited.Marin (2021) [17]. It performed the application of gamificationon some topics of a software engineering course to engage studentsand increase their motivation and argued that, with due motiva-tion, students can better exercise the topics and obtain more solidknowledge. There were five games related to risk management,BPMN modeling, Scrum process, design and inspection of classdiagrams, and cosmic functional size measurement to assist in thelearning process of the software engineering course. This study alsopresented the lessons learned about the application of gamificationand serious games in software engineering, including limitationsor disadvantages.Jurgelaitis et al. (2018) [12]. This work conducted a research toinvestigate how gamification could be inserted into an InformationSystems Modeling course, which covers a range of topics on UML.As a result, an implementation of the gamified system modelingcourse in the Moodle environment was presented, using additional101ModelGame: A Quality Model for Gamified Software Modeling Learning SBCARS 21, September 27-October 1, 2021, Joinville, Brazilplugins for the use of the necessary gamified elements. The studyshowed good results and obtained a positive acceptance by theparticipating students.Rodrigues et al. (2018) [22]. They investigated the use of gamesand game elements in software engineering education, through aresearch that had the participation of 88 instructors of this disci-pline. The results showed that most instructors are aware of theseeducational approaches, however, the games were adopted by only21 participants and game elements were adopted only by 19. Gamesare most often used to cover Software Process and Project Man-agement. The most commonly used game elements are points,quizzes, and challenges. The results also show that the main rea-sons for not adopting the resources are the lack of knowledge,information about games relevant to the engineering of teachingsoftware, and the lack of time to plan and include these approachesin the classroom.Cosentino et al. (2017) [3]. They present a model-based ap-proach to learning modeling in a gamified way. The approach in-cludes a new language to model the gamification process itself andan environment where it can be incorporated into current mod-eling tools to allow instructors and students to design and use acomplete modeling framework, including gamification elements. Inaddition, the approach also had as a proposal to provide support tocollect and analyze gamification data, thus facilitating monitoringactivities.Yohannis (2016) [25]. This research presents an exploration ofgame design as an approach to strengthening the students mas-tery in software modeling by developing their abstraction skills. Itbrought together concepts of gamification development, such as thelens of atoms of intrinsic skill and principles of pedagogical designof various theories and models of learning. The research followsthe Design Science Research Methodology and explores the bestpractices of Model Oriented Engineering. As a result, a modelinggame design framework and generation structure and a series ofproduced games are presented.Pedreira et al. (2015) [21]. They developed a systematic map-ping of gamification in Software Engineering based on 29 studies.The mapping revealed that software implementation is the areain which most studies focus, followed by software requirements,few others in different areas, such as project planning and softwaretesting, and even to a lesser extent in activities involving softwaremodeling. However, the highlight of this work was to highlight thatgamification in software engineering is still at a very early stageand the evidence on its impact in this field remains inconclusive.3.2 Comparative Analysis and OpportunitiesFive Comparison Criteria (CC) were defined selecting the most rele-vant variables to assist in the process of identifying similarities anddifferences between the proposed work and the selected articles.This comparison is crucial to make the process of identifying re-search opportunities using objective rather than subjective criteria.The criteria are described below:Context (CC01): Works that explore the use of gamificationin software modeling teaching/learning.Participant profile (CC02): Studies that collected data fromparticipants for screening and profile characterization.Applicability of Gamification in UML (CC03): Studiesthat evaluated how gamification can contribute to UML mod-els.Model creation (CC04): Studies that have developed a modelto improve factors that imply the non-adoption of UML.Instructor participation (CC05): Studies that collectedqualitative data through the participation of software mod-eling instructors.Table 1 shows the comparison of the selected works, confrontingthis work. Some gaps and research opportunities are observed: (1)only the proposed work was the only one to fully meet all compar-ison criteria; (2) although most of them targeted the applicationof gamification in software modeling teaching, they were not di-rected to the use of UML; (3) no study has developed a model toevaluate the learning and improvement of UML models developedby students; and (4) most of them did not have the participationof instructors to identify the difficulties and opportunities in theapplication of gamification in the teaching of software modeling.Thus, the next Section presents a quality model to explore theseidentified opportunities.Related WorkComparison CriterionCC1 CC2 CC3 CC4 CC5Proposed Work      Porto et al (2021) [4] ## ##Marin (2021) [17]  #G #G ##Jurgelaitis et al (2018) [12]  #  #Rodrigues et al (2018) [22]   G ## Cosentino et al (2017) [3]  #  #Yohannis (2016) [25]  #G #G ##Pedreira et al (2015) [21] ##G ### Completely Meets G #Partially Meets #Does not attendTable 1: Comparative analysis of the selected related works4 PROPOSED QUALITY MODELThis section presents the proposed quality model to support soft-ware modeling learning in a gamified way. It serves as a frameof reference so that instructors can evaluate the UML models cre-ated by students through gamified activities. Section 4.1 presentsa proposal of a generic analytical framework. Section 4.2 detailsthe abstract syntax of the proposed quality model. Section 4.3 ex-plains the quality notions related to the gamified software modelinglearning.4.1 Generic Analytical FrameworkFigure 1 presents the generic analytical framework for improvingthe quality of the models and serves as the basis for the creation ofan evaluation scheme. The arrows (\"links\"), labeled as Evaluationand Gamified Modeling, represent the questions that the evidencemust answer; dotted lines represent associations; rectangles rep-resent the Models (rounded corners) or the quality states (squarecorners) by which these bindings are measured. Ellipses representthe adverse effects that can be generated from the evaluation anduse of gamification.The numbers refer to the key questions and are connected withthe concepts and relationships of the abstract syntax of the Quality102SBCARS 21, September 27-October 1, 2021, Joinville, Brazil Ed Wilson Jnior and Kleinner FariasFigure 1: Generic analytical framework for gamified software modeling learning.Model (presented in Section 4.2), as follows: (1) Are there toolsthat assist instructors in evaluating the models developed by stu-dents, thus reducing the poor quality and incompleteness of theseartifacts? (2) What is the prevalence of characteristics that causemodels to be at risk? (3) Are there notions of quality to evaluatethe models as a way to define parameters when performing theircorrection? (4) Applying the use of gamification in models thatneed intervention would be a way to identify factors that couldgenerate models with high quality levels? (5) Does the applicationof gamification improve the quality of the model? (5.a) How are themodels without gamification evaluated in relation to those withgamification? (5.b) Are there reasons to expect that gamificationmodels can have better quality results than those that are gener-ated without gamification? (6) Is the output model really effectivewhen associated with reducing the poor quality of the model? (7)Does the absence of evaluation result in adverse effects? (7.a) is theevaluation acceptable for the model? (7.b) What are the potentialharms, and how often do they occur? (8) Does gamification resultin adverse effects on models?Fact is that it is not enough just to include this \"toolbox\" in theUML learning process, it is necessary to provide the instructor witha model (guide) that can serve as a reference to evaluate the qualityof diagrams elaborated through gamified activities. For example,the instructor could create models predefining inconsistencies bymaking use of these questions raised to evaluate the models createdby the students. The set of questions serves as the starting point forthis evaluation. Knowing that the adaptation of the gamificationapproach requires a significant effort [ 20], in this study we presentThe ModelGame as a way to identify factors that contribute tothe quality of these artifacts and, consequently, to the studentslearning.4.2 Abstract SyntaxFollowing the specification pattern of the UML metamodel, Figure 2presents the abstract syntax of the proposed Quality Model forgamified software modeling learning (ModelGame). It identifiesthe main concepts and relationships. The numbers represent thenotions of quality that are discussed in Section 4.3. The followingare detailed each of these concepts and relationships.Domain. The first concept presented in this study is the domain,which corresponds to a specific context of the application to bedeveloped to solve the problem. In this process, the design templaterepresents the solution given to the domain.Associationcontextualizes : Challenges[*]Each contextualise refers to the domain that will serve as thebasis for the challenges launched.Challenges. This concept represents the phase in which theproblem is contextualized (domain-based), as well as what will bethe missions, phases, scenarios, and other elements presented tothe players, in this case the students, who must use the principlesof software engineering to perform the modeling and reach thefinal goal.Associationinfluences : Design Model[*]Each influence represents that the proposed challenge interferedin aspects of the design model, causing the user to seek to make acontinuous improvement.Modeling Language. Software modeling is an important stepfor development to happen in a way that adheres to the require-ments established by the requester, for this, there is the modelinglanguage, which offers a standardized way to document and designsoftware. Through the use of modeling languages, it is possible toachieve a high level of understanding about the software in ques-tion, improving the communication between all those involved inthe process, thus avoiding implementation errors. It points out thatsoftware engineers use these languages to communicate designdecisions and verify the feasibility of implementing the intendeddesign. The UML was consolidated as the Modeling Language inthe paradigm of object orientation, in which it is possible throughvisual notation generated from the diagrams- presented later in thisstudy as Design Models- to perform the representation of variousperspectives of the system.Association103ModelGame: A Quality Model for Gamified Software Modeling Learning SBCARS 21, September 27-October 1, 2021, Joinville, BrazilFigure 2: Abstract Quality Model Syntax.expresses : Design Model[*]Performs the representation of the intended design templates, inwhich the Modeling Language should be applicable to the domaintype.User. This concept corresponds to the individual who performsthe interpretation of the developed design models, whose objectiveis to be able to understand the domain in question. In the gamifiedcontext, the user has the role of player and it is he who performsthe whole process, being able to perform the interpretation of ex-isting models or even creating new ones. The user can also identifyand resolve inconsistencies that arise from compositions betweenmodels.Associationcreates : Design Model[1..*]Represents the process in which the user creates a design template,which can be one or more.interprets : Design Model[1..*]In this association, the user performs the interpretation of the designtemplate. When interpreting the model, paths for the resolution ofinconsistencies can be identified.detects : Inconsistency [*]Represents the users discovery of design model inconsistencies,for example, those that are generated from identifying conflicts,whether a class is abstract or not.resolves : Inconsistency [*]Each resolves equates to the resolution representation of the incon-sistencies by the user that happens after he analyzes and determinesthe best alternative to perform this action.uses : Modeling Tools [*]Determines that the user can use modeling tools to generate/updatedesign models.AssociationWithout a directed relationship .Modeling Tool. This concept represents the applications thatare used to carry out the construction of design models. There areseveral tools available, online and desktop, and it is up to the userto choose the one that will best meet their needs and adapt to thecontext in question, that is, they work in any domain that is beingconsidered.Design Model. The design model refers to a visual notation(diagram) to represent static and dynamic aspects. These models arebuilt according to a specific objective or task and tend to facilitatethe logical interpretation of the software in several aspects. Themost popular diagrams are Use Cases and Classes, the first beingstatic and representing a set of actions generated from functionalrequirements (use cases) and presenting the interactions generatedwith external users (actors). The second is a static diagram andmakes the representation of the logical structure of the softwareinvolving the classes, their attributes, methods, and relationshipsbetween them [19].Associationdescribes : Domain[1]Each describes makes the representation of a specific domainand means that every design model must describe it.Inconsistency. It corresponds to the defects found in the modelsdeveloped by users. They may occur because of the nonidentifi-cation and correction of possible conflicts and even an erroneousinterpretation.Associationaffects : Design Model[*]This association indicates that with each occurrence of the affect, aproblem is presented harming the quality of the design model.104SBCARS 21, September 27-October 1, 2021, Joinville, Brazil Ed Wilson Jnior and Kleinner FariasPoints. This concept represents one of the most used game me-chanics in software engineering and functions as a quantitativereward for each action developed, in which it is possible to regulatethe number of rewarded points of the player, defined here as user,based on the importance of each action. Through this concept, itis possible to stimulate competition, collaboration, and creativityamong users, stimulating learning. Points appear as a derivationof the association affects, since when each inconsistency error isidentified or not, the user will receive a score and the associationdescribes, because the points will also be applied when makingconnections between the model and the domain.Progress. The concept of progress emerges as a factor that makesthe user able to perceive its evolution in the process, in this case,software modeling. Progress emerges as a derivation of the associa-tion interprets, making the user know when they have performeda correct interpretation of the proposed design model or what stillneeds to be improved.Feedback. Feedback has the role of making the user realize thatthe proposed goal can be achieved and follow its evolution, includ-ing analyzing how to change or creating new strategies to achievethe goal. This concept emerges as a derivation between the associa-tions it creates, causing the user to receive a return to the modelcreation process.4.3 Quality NotionsAs discussed in Section 2, gamification can bring important ele-ments for learning software modeling and, therefore, the objectiveof this section is to produce the notions of quality of the modelof this study. The ModelGame is composed of ten counts, four ofwhich are proposed in this study - scope, use, motivational and en-gagement - extracted from the main benefits that the gamificationelements presented in Figure 2 can bring to the models. The othersare adaptations of previous works [ 6,14,15], they are, syntactic,semantic, social, effort, detection and resolution.Scope Quality (1). It seeks to determine how much the proposedchallenge is contextualized with the design model, as well as the def-inition of the domain, problem, competencies, concepts, behaviorsand attitudes that will be developed throughout the process.Syntactic Quality (2). This notion makes the representation ofthe process of correction of the design models that are producedby the modeling language, because if it is not used correctly, incon-sistencies will arise. It is important to insert this notion of qualityinto our study, since during the process of developing the models,users may come across the composition of two class diagrams, forexample.Semantic Quality (3). It is necessary to verify that the designmodel and the problem domain match, so this notion performsthis type of analysis. Communication problems may occur betweenusers if the semantic elements of the model are affected.Social Quality (4). Design models are used to communicatebetween members of a team to inform all established decisionsabout software development [ 8]. If divergent interpretations occur,this communication will be greatly impaired.Quality of Effort (5). This notion refers to the production chal-lenges of the model that will be generated, including factors suchas time and cost.Quality of Use (6). To produce design templates, users can useunusual tools such as paper, whiteboard, and more. However, mostof the time they choose to use formal tools (CASES) and can beonline or desktop. This notion corresponds to the level of ease andapplicability of the models elaborated when making use of thesetools, it is also important to contribute to communication betweenusers through collaboration-related functionalities.Detection Quality (7). This notion is referenced to the processof locating inconsistencies, since when users arise, they shouldperform traceability of them quickly. If the detection is complicated,it could hinder the process of correcting the models.Resolution Quality (8). It corresponds to the level of qualityrelated to the effort that users take to look for alternatives to solvethe identified problem.Motivational Quality (9). This notion refers to the motiva-tional factors involved during the learning and development ofdesign models, which can be intrinsic and extrinsic. Elements ofgamification such as points, feedback and progress bring the user adegree of satisfaction in continuing their discovery and transfor-mations throughout the process.Quality of Engagement (10). The user in tracking their progresscan feel committed to the objective in question, and this notionrepresents the measurement of the level of commitment of themduring the development of design models.5 EVALUATIONThis section describes the methodology followed to evaluate theproposed quality model. This methodology follows well-establishedempirical guidelines [ 24]. Section 5.1 details the objective and re-search questions (RQ). Section 5.2 presents the questionnaire formu-lated to evaluate the proposed quality model. Section 5.3 explainsthe context and selection of participants. Section 5.4 describes thepresentation of the Model. Section 5.5 presents the analysis of thecollected data.5.1 Objective and Research QuestionsThe objective (O)of this study is twofold: (O1) Introduce Model-Game as a tool for teaching Software Modeling; and (O2) Analyzethe applicability of the quality model regarding the improvementof UML models.To analyze the different facets of the objectives, two ResearchQuestions (RQ) have been formulated:RQ1: How do instructors evaluate the use of gamificationin software modeling?RQ2: What is the acceptance of ModelGame by softwaremodeling instructors?5.2 QuestionnaireData was collected through an online questionnaire created throughGoogle Forms1following well-established guidelines described in[24]. This strategy was chosen because the questionnaire couldbe applied quickly and easily collect data from individuals in geo-graphically diverse locations. The questions of the questionnaire1Questionnaire: https://forms.gle/qjaFDpErEtGdLuWw6105ModelGame: A Quality Model for Gamified Software Modeling Learning SBCARS 21, September 27-October 1, 2021, Joinville, Brazilwere concerned with examining the research gaps of previous stud-ies and apprehending the structures of the previously developedquestionnaire.Part 1: Participant profile. The first part of the questionnaireconsisted of collecting data that are related to the characteristicsand opinions of the participants. The creation of the participantprofile through this data is important to make the selection ofpossible users of ModelGame. Without this profile, participantswith an inadequate profile may generate inconsistent assessments.Participants were asked to provide more general information, suchas age, education level, academic background. Information aboutthe time of experience in teaching was also considered, includingteaching software modeling and level of knowledge about UMLmodels.Part 2: TAM questionnaire. The second part addressed ques-tions about the usability and acceptance of the technique, aimingto explore q3. To this end, this part of our questionnaire is based onthe technology acceptance model (TAM) [ 16]. This part containednine questions, which were answered through the Likert Scale, in-cluding Totally Agree, Partially Agree, Neutral, Partially Disagree,and Totally Disagree. The questions formulated (Q) dealt with sev-eral topics, including perceived ease of use (Q1-3), perceived utility(Q4-7), attitude towards use (Q8), and behavioral intention to use(Q9).5.3 Selection of participantsThe participants were selected based on the following criteria: in-structors and/or professionals working in the teaching of softwaremodeling in higher education institutions in Brazil. Using this cri-terion, we sought to select participants with academic trainingand practical experience in teaching. This finite set of all possibleparticipants represents the target population [ 13]. This popula-tion represents those people who are in a position to answer thequestions formulated and to whom the results of the survey apply[13]. In all, 19 people (n) answered the questionnaire. The partici-pants were invited via e-mail to participate in the study and eachof them previously received the explanation/training about themodel proposed through the researcher and there was no doubt,they could leave for the next step that consisted of completing theTAM questionnaire. We discussed the experimental process in thenext section.5.4 Experimental ProcessFigure 3 presents the experimental process used in this study, whichis composed of three phases discussed below:Phase 1: Presentation. It has an activity, presentation , in whichthe researcher explained to the participants through a video detailabout the quality model. This process took place individually andin a standard way, where space was also made available for par-ticipants to answer possible doubts about the proposed study andmodel, lasting an average of 20 minutes.Phase 2: Application of the TAM questionnaire. It has twoactivities, the first being Collect demographic data . The participantsanswered a list of questions (input) so that we could collect theircharacteristics and opinions about the ModelGame. The demo-graphic data collected (output) became the result of this activity.The second activity Apply TAM questionnaire (input) . Participantsreceived a list of questions about the perception of ease of use, per-ceived utility, attitudes, and intention of behavior, in relation to theModelGame. Qualitative data (output) were generated, regardingthe usability and acceptance of the Model under the perspectiveof professionals who teach software modeling. This questionnairefollowed the guidelines of the TAM [16].Phase 3: Analysis and result report. It has two activities. Thefirst, Analyze data sought to perform a thorough analysis of the datacollected through the questionnaire and the researchers perceptionregarding the participants doubts during the presentation stage.For this, the collected data were analyzed separately, as well as con-fronted, aiming to perform a triangulation of them. Subsequently,there was an Evaluation data , as a way to understand in a moredepth the context, the perceptions of the participants in relation tothe proposed model as well as its applicability.Figure 3: The experimental process.5.5 Result Analysis5.5.1 Profile data of the participants. Table 3 describes the profiledata, reporting the characteristics and opinions of the participants.These data were collected from May 18 to June 5, 2021. In total,we had 19 participants. Our participants are between 20 and 49years old, most of them have a degree in Computer Science (52.6%),Information Systems (26.3%) or Systems Analysis (21.1%) and arespecialists (36.8%), masters (36.8%) and doctors (15.8%). About theworking time in teaching, the majority (42.1%) they have been teach-ing for more than 8 years and teach disciplines related to softwaremodeling, including software engineering, systems analysis andsoftware projects. A total of 47.4% have a full level of knowledgeabout UML and almost half of them (47.4%) has not yet used gamifi-cation in the teaching of software modeling. Therefore, we consider106SBCARS 21, September 27-October 1, 2021, Joinville, Brazil Ed Wilson Jnior and Kleinner FariasTotally agree Partially agree Neutral Partially disagree Totally disagreePerceived ease of useI found the quality model easy to use 8 9 2 0 0I found the quality model easy to learn 10 9 0 0 0I found the quality model easy to master 6 12 0 1 0Perceived usefulnessThe model would make it easier to understand which elements of gamification can be used in modeling . 12 5 2 0 0Using the quality model would help increase productivity. 9 8 2 0 0The model would provide an understanding of how to mitigate the incompleteness of UML diagrams. 5 8 5 1 0The model would help compare theories about gamification in software modeling teaching. 13 4 2Attitude towards useUsing the Quality Model for Gamified Software Modeling Learning is a good idea. 13 5 1 0 0Behavioral intention to useI would use the quality model in software modeling classes. 10 7 2 0 0Table 2: Collected data related to TAM questionnaire.that although small, our sample is adequate to carry out an initialevaluation of the proposed approach.Characteristic and Opinion (n=19) Answer # %Age < 20 years 0 0.0%20-29 years 4 21.1%30-39 years 8 42.1%40-49 years 5 26.3%> 49 years 2 10.5%Education Undergraduate* 0 0.0%Specialization* 7 36.8%Master* 7 36.8%PhD* 3 15.8%Others 2 10.6%Undergraduate course Information Systems 5 26.3%Computer Science 10 52.6%Computer Engineering 0 0.0%System Analysis 4 21,1%Others 0 0.0%Time of experience in teaching < 2 years 4 21.1%2-4 years 2 10.5%5-6 years 3 15.8%7-8 years 2 10.5%> 8 years 8 42.1%Experience in teaching < 2 years 3 15.8%software modeling 2-4 years 5 26.3%5-6 years 3 15.8%7-8 years 2 10.5%> 8 years 6 31,6%Level of knowledge Beginner 2 10.5%about UML models Junior 5 26.3%Full 9 47.4%Senior 3 15.8%Used gamification in teaching Yes 9 47.4%No 9 47,4%Maybe 1 5.3%Gamification can contribute to the quality Totally agree 10 52.6%of the models of UML diagrams generated Partially agree 8 42.1%by students Neutral 1 5.3%Partially disagree 0 0.0%Totally disagree 0 0.0%Table 3: The profile data of the participants.5.5.2 RQ1: How do instructors evaluate the use of gamification insoftware modeling? Table 3 presents the collected data related tothe RQ formulated. First, we begin the analysis by verifying howinstructors visualize gamification in software modeling teaching.Although most of them (47.4%) have not yet used gamificationelements (scores, challenge, emblem, among others) in their classes,most (52.6%) totally agree and (42.1%) partially agree that the useof these can contribute to the quality of the models developed bythe students.We consider the percentage of instructors who have not yetused gamification in their classes to be high and this may be tiedto factors such as lack of knowledge, information about the tool,and even time to plan and include these approaches [ 22]. Althoughthey were based on software modeling teaching context, previousstudies [ 3,4,12,17,25] they did not count on the participation ofinstructors and we understand that this participation is fundamentalto understand the perceptions of these professionals since they willbe at the forefront of the use of gamification.The ModelGame proposed in this study could help them insertgamification into their classes, according to the software modelinglearning design [ 25], based on the assumption that for this, it isnecessary to develop a better understanding of the tasks, activities,skills and operations that the different elements of gamification canoffer and how they can correspond to the desired learning outcomesby developing a more concrete and motivating presentation thatcan involve students and facilitate deep learning with UML.5.5.3 RQ2: What is the acceptance of the ModelGame by softwaremodeling instructors? Using the TAM questionnaire, we tried toevaluate the ease of use, perceived usefulness, attitude, and behav-ioral intention to use the Quality Model. Table 2 shows the dataobtained. Our data obtained show that no one disagreed that theModelGame is easy to use, learn, and master. On the contrary, al-most 90% of participants find the model easy to use (42.1% totallyagree and 47.4% partially agrees and 10.5% neutral), learn (52.6%107ModelGame: A Quality Model for Gamified Software Modeling Learning SBCARS 21, September 27-October 1, 2021, Joinville, Brazilfully agree and 47.4% partially agree) and master (31.6% fully agree,63.2% partially agree and 5.3% partially disagree).The results are also favorable considering the perception of util-ity. Most participants realized that the ModelGame would make iteasier to understand which elements of gamification can be used ineach of the phases of modeling using UML(63.3% totally agree, 26.3%partially agree and 10.5% neutral), increase productivity (47.4% fullyagree, 42.1% partially agree and 10.5% neutral), and the use of thequality model would provide an understanding of how to mitigatethe incompleteness of UML diagrams (26.3% agree totalmen 42.1%partially agree, 26.3% neutral and 5.3% partially disagree). Still inthe useful aspect, we tried to know if the quality model would helpto compare validated theories about the inclusion of gamificationin software modeling teaching (68.4% totally agree, 21.1% partiallyagree and 10.5% neutral).Considering the attitude towards use, participants believe thatusing the ModelGame is a good idea (68.4% totally agree, 26.3%partially agree and 5.3% neutral), just as they are confident andwould use the Model in software modeling classes (52.6% totallyagree, 36.8% partially agree and 10.5% neutral). These findings showthe potential for acceptance by people with profiles similar to thoseof participants. The results are encouraging and show the potentialto use the proposed approach in the educational scenario.6 THREATS TO VALIDITYThis section discusses the possible threats to the validity of thestudy.Internal validity. The main point affecting the internal validityof our study concerns the total time used for the exploratory phase.To mitigate this threat, we performed the video recording of a pilotexplaining the operating details and objectives of the ModelGame.In relation to the methods used, the threats related to internalvalidity relate to how we extract the perceptions of the discussionsand whether they represent the perceptions of teachers about theuse of the Model. We try to reduce this threat by applying the TAMquestionnaire.External validity. We identified threats related to external va-lidity, such as the number of participants who never applied the useof gamification. This study was limited to 19 participants (teachers)from various educational institutions, of which 9 (47.4%) never usedany element of gamification in their classes, this factor can interferein the data, since the model intends to evaluate the quality of UMLdiagrams from gamified activities.Conclusion validity. Threats related to the validity of the con-clusion are related to treatment and outcome. We try to make thereduction by combining quantitative and qualitative data throughdifferent resources. These data were obtained through audio andquestionnaires. We analyze this data to answer the research ques-tions.7 CONCLUSIONS AND FUTURE WORKThis study proposed an initial quality model (ModelGame) thatserves as a reference framework for instructors for qualitative eval-uations of UML models developed from gamified activities, theapplication of an empirical study with 19 participants was carriedout to understand their vision in relation to gamification and theacceptance of the proposed Model. It was identified that most havenot yet used gamification in their classes, but agree that their usecan contribute to the quality of the models developed by the stu-dents and were open to using the model. Our findings can enhancethe adoption of new teaching practices through gamification, result-ing in the improvement of software modeling learning using UML,and consequently the creation of models developed by students.These approaches can stimulate students immersion in the designof systems as future professionals during learning.Finally, we hope to carry out in the future a series of experimentalstudies to analyze each stage of application of the ModelGameand that this work represents a first step to better support theapplication of empirical studies on models of evaluation of theuse of gamification in software modeling. We also hope that thequestions described throughout the article will encourage otherresearchers to extend our study to different modeling languagesand teaching methodologies.REFERENCES[1]Rick Adcock, Edward Alef, Bruce Amato, Mark Ardis, Larry Bernstein, BarryBoehm, Pierre Bourque, John Brackett, Murray Cantor, Lillian Cassel, et al .2009.Curriculum guidelines for graduate degree programs in software engineering . ACM.[2]Mark Ardis, David Budgen, Gregory W Hislop, Jeff Offutt, Mark Sebern, andWillem Visser. 2015. SE 2014: Curriculum guidelines for undergraduate degreeprograms in software engineering. Computer 48, 11 (2015), 106109.[3]Valerio Cosentino, Sbastien Grard, and Jordi Cabot Sagrera. 2017. A model-based approach to gamify the learning of modeling. CEUR Workshop Proceed-ings.[4]Daniel de Paula Porto, Gabriela Martins de Jesus, Fabiano Cutigi Ferrari, andSandra Camargo Pinto Ferraz Fabbri. 2021. Initiatives and challenges of usinggamification in software engineering: A Systematic Mapping. Journal of Systemsand Software 173 (2021), 110870.[5]Sebastian Deterding, Miguel Sicart, Lennart Nacke, Kenton OHara, and DanDixon. 2011. Gamification. using game-design elements in non-gaming contexts.InCHI11 extended abstracts on human factors in computing systems . 24252428.[6]Ana Fernndez-Saez et al. 2012. A systematic literature review on the quality ofUML models. J. Data. Manage 22, 3 (2012), 4670.[7]Kleinner Farias et al. 2012. Evaluating the impact of aspects on inconsistencydetection effort: a controlled experiment. In International Conference on ModelDriven Engineering Languages and Systems . Springer, 219234.[8]Kleinner Frias et al. 2014. Towards a quality model for model composition effort.In29th Annual ACM Symposium on Applied Computing . 11811183.[9]Kleinner Farias et al. 2015. Evaluating the effort of composing design models: acontrolled experiment. Software & Systems Modeling 14, 4 (2015), 13491365.[10] Kleinner Farias et al. 2019. UML2Merge: a UML extension for model merging.IET Software 13, 6 (2019), 575586.[11] Juho Hamari, Jonna Koivisto, and Harri Sarsa. 2014. Does gamification work?a literature review of empirical studies on gamification. In 2014 47th Hawaiiinternational conference on system sciences . Ieee, 30253034.[12] Mantas Jurgelaitis, Vaidotas Drungilas, and Lina eponien e. 2018. GamifiedMoodle course for teaching UML. Baltic journal of modern computing 6, 2 (2018),119127.[13] Barbara A Kitchenham and Shari L Pfleeger. 2008. Personal opinion surveys. InGuide to advanced empirical software engineering . Springer, 6392.[14] Christian Franz Josef Lange. 2007. Assessing and Improving the Quality ofModeling: A series of Empirical Studies about the UML. (2007).[15] Odd Ivar Lindland, Guttorm Sindre, and Arne Solvberg. 1994. Understandingquality in conceptual modeling. IEEE software 11, 2 (1994), 4249.[16] Nikola Maranguni and Andrina Grani. 2015. Technology acceptance model: aliterature review from 1986 to 2013. Universal access in the information society14, 1 (2015), 8195.108SBCARS 21, September 27-October 1, 2021, Joinville, Brazil Ed Wilson Jnior and Kleinner Farias[17] Beatriz Marn. 2021. Lessons Learned About Gamification in Software Engineer-ing Education. In Latin American Women and Research Contributions to the ITField . IGI Global, 174197.[18] Kleinner Oliveira, Alessandro Garcia, and Jon Whittle. 2008. On the quantitativeassessment of class model compositions: An exploratory study. 1th ESMDE atMODELS (2008).[19] OMG. 2017. UML: Infrastructure specification.https://www.omg.org/spec/UML/2.5.1/PDF.[20] Sofia Ouhbi and Nuno Pombo. 2020. Software Engineering Education: Challengesand Perspectives. In IEEE Global Engineering Education Conference . 202209.[21] Oscar Pedreira, Flix Garca, Nieves Brisaboa, and Mario Piattini. 2015. Gamifica-tion in software engineeringA systematic mapping. Information and softwaretechnology 57 (2015), 157168.[22] Pedro Rodrigues, Mauricio Souza, and Eduardo Figueiredo. 2018. Games andgamification in software engineering education: A survey with educators. In 2018IEEE Frontiers in Education Conference (FIE) . IEEE, 19.[23] Kevin Werbach and Dan Hunter. 2012. For the win: How game thinking canrevolutionize your business . Wharton digital press.[24] Claes Wohlin, Per Runeson, Martin Hst, Magnus C Ohlsson, Bjrn Regnell, andAnders Wessln. 2012. Experimentation in software engineering . Springer Science& Business Media.[25] Alfa Yohannis. 2016. Gamification of Software Modelling Learning.. In DS@MoDELS .109","date":"2024-02-05T14:24:07.500635","authors":[{"full_name":"Ed Wilson Júnior","institut_name":"Universidade do Vale do Rio dos Sinos"},{"full_name":"Kleinner Farias","institut_name":"Universidade do Vale do Rio dos Sinos"}],"author":[{"full_name":"Ed Wilson Júnior","institut_name":"Universidade do Vale do Rio dos Sinos"},{"full_name":"Kleinner Farias","institut_name":"Universidade do Vale do Rio dos Sinos"}]}},{"_index":"article_index","_id":"7","_score":1.0,"_source":{"id":7,"title":"Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model","summary":"The emergence of novel types of communication, such as email, has been brought on by the development of the internet, which radically concentrated the way in that individuals communicate socially and with one another. It is now establishing itself as a crucial aspect of the communication network which has been adopted by a variety of commercial enterprises such as retail outlets. So in this research paper, we have built a unique spam-detection methodology based on email-body sentiment analysis. The proposed hybrid model is put into practice and preprocessing the data, extracting the properties, and categorizing data are all steps in the process. To examine the emotive and sequential aspects of texts, we use word embedding and a bi-directional LSTM network. this model frequently shortens the training period, then utilizes the Convolution Layer to extract text features at a higher level for the BiLSTM network. Our model performs better than previous versions, with an accuracy rate of 97–98%. In addition, we show that our model beats not just some well-known machine learning classifiers but also cutting-edge methods for identifying spam communications, demonstrating its superiority on its own. Suggested Ensemble model’s results are examined in terms of recall, accuracy, and precision.","keywords":"Dataset, KNN, Gaussian Naive Bayes, LSTM, SVM, Bidirectional LSTM, GRU, Word-Embeddings, CNN","content":"Semantic Analysis and Classification of Emails throughInformative Selection of Features and Ensemble AI ModelShivangi SachanDepartment of CSEIIIT LucknowLucknow, UP, Indiamcs21025@iiitl.ac.inKhushbu DoulaniVardhaman College of EngineeringHyderabad, Indiakhushidoulani@gmail.comMainak AdhikariDepartment of CSEIIIT LucknowUP, Indiamainak.ism@gmail.comABSTRACTThe emergence of novel types of communication, such as email, hasbeen brought on by the development of the internet, which radicallyconcentrated the way in that individuals communicate socially andwith one another. It is now establishing itself as a crucial aspect ofthe communication network which has been adopted by a varietyof commercial enterprises such as retail outlets. So in this researchpaper, we have built a unique spam-detection methodology basedon email-body sentiment analysis. The proposed hybrid model isput into practice and preprocessing the data, extracting the proper-ties, and categorizing data are all steps in the process. To examinethe emotive and sequential aspects of texts, we use word embed-ding and a bi-directional LSTM network. this model frequentlyshortens the training period, then utilizes the Convolution Layer toextract text features at a higher level for the BiLSTM network. Ourmodel performs better than previous versions, with an accuracyrate of 9798%. In addition, we show that our model beats not justsome well-known machine learning classifiers but also cutting-edgemethods for identifying spam communications, demonstrating itssuperiority on its own. Suggested Ensemble models results areexamined in terms of recall, accuracy, and precisionCCS CONCEPTSComputer systems organization Embedded systems ;Re-dundancy ; Robotics; NetworksNetwork reliability.KEYWORDSDataset, KNN, Gaussian Naive Bayes, LSTM, SVM, BidirectionalLSTM, GRU, Word-Embeddings, CNNACM Reference Format:Shivangi Sachan, Khushbu Doulani, and Mainak Adhikari. 2023. SemanticAnalysis and Classification of Emails through Informative Selection ofFeatures and Ensemble AI Model. In 2023 Fifteenth International Conferenceon Contemporary Computing (IC3-2023) (IC3 2023), August 0305, 2023, Noida,India. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3607947.3607979Both authors contributed equally to this research.Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.IC3 2023, August 0305, 2023, Noida, India2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0022-4/23/08. . . $15.00https://doi.org/10.1145/3607947.36079791 INTRODUCTIONOver the past few years, a clear surge of both the amount of spam-mers as well as spam emails. This is likely due to a fact that theinvestment necessary for engaging in the spamming industry isrelatively low. As a result of this, we currently have a system thatidentifies every email as suspicious, which has caused major expen-ditures in the investment of defense systems [ 12]. Emails are usedfor online crimes like fraud, hacking, phishing, E-mail bombing, bul-lying, and spamming. [ 16]. Algorithms that are based on machinelearning (ML) are now the most effective and often used approach tothe recognition of spam. Phishing, which is defined as a fraudulentattempt to acquire private information by masquerading as a trust-worthy party in electronic communication, has rapidly advancedpast use of simple techniques and the tactic of casting a wide net;instead, spear phishing uses a variety of sophisticated techniquesto target a single high-value individual. Other researchers used NB,Decision Trees, and SVM to compare the performance of supervisedML algorithms for spam identification [ 6]. Spam emails clog up re-cipients inboxes with unsolicited communications, which frustratethem and push them into the attackers planned traps [ 7]. As a re-sult, spam messages unquestionably pose a risk to both email usersand the Internet community. In addition, Users may occasionallyread the entire text of an unsolicited message that is delivered tothe target users inboxes without realizing that the message is junkand then choosing to avoid it. Building a framework for email spamdetection is the aim of this project. In this approach, we combine theWord-Embedding Network with the CNN layer, Bi-LSTM, and GRU(BiLSTM+GRU). CNN layers are used to speed up training timebefore the Bi-LSTM network, and more advanced textual character-istics are extracted with the use of this network in comparison tothe straight LSTM network, in less time. Gated recurrent neural net-works (GRUs) are then added because they train more quickly andperform better for language modeling. To evaluate and investigatevarious machine learning algorithms for predicting email spam,and develop a hybrid classification algorithm to filter email spambefore employing an ensemble classification algorithm to forecastit. To put an innovative technique into practice and compare it tothe current method in terms of various metrics. Ensemble learn-ing, a successful machine learning paradigm, combines a group oflearners rather than a single learner to forecast unknown targetattributes. Bagging, boosting, voting, and stacking are the four maintypes of ensemble learning techniques. To increase performance,an integrated method and the combining of two or three algorithmsare also suggested. Extraction of text-based features takes a longtime. Furthermore, it can be challenging to extract all of the crucialinformation from a short text. Over the span associated with this181IC3 2023, August 0305, 2023, Noida, India Sachan et al.research, we utilize Bidirectional Large Short-Term Memories (Bi-LSTM) in conjunction with Convolutional Neural Networks (CNN)to come up with an innovative method to the detection of spam.Bagging and boosting approaches were widely preferred in thisstudy. Contribution and paper organization is as follows: section 1.1describes literature study, section 1.2 describe motivation for thisresearch work, section 2 sketches procedure of details implemen-tation, Section 3 present experimental setup, dataset descriptionand evaluation metrics, and section 4 summarizing outcomes of theexperiment.1.1 Related WorkEmail is indeed the second most frequently utilized Internet appli-cation as well as the third most common method of cyberbullying,claims one study. Cybercriminals exploit it in a number of ways,including as sending obscene or abusive messages, adding virusesto emails, snatching the private information of victims, and ex-posing it to a broad audience. Spam letters made up 53.95% of allemail traffic in March 2020. We examine three main types of un-lawful emails in our study. First are fake emails, which are sentto manipulate recipients to submit sensitive information. The sec-ond as being cyberbullyings use of harassing emails to threatenindividuals. Suspicious emails that describe illegal activities belongto the third category. Many researchers have earlier contributedmassively to this subject. The researcher claims there is some proofthat suspicious emails were sent before to the events of 9/11. [ 14].When it comes to data labeling, there are also convinced rule-basedapproaches and technologies ( like VADER) that are used, eventhough their efficiency of the are together is adversely affected. Ahidden layer, which itself is essential for vectorization, is the toplayer of the model. We use oversampling methods for this minorityclass because of the absence of data. Sampling techniques can helpwith multicollinearity, but they have an impact on simulation re-sults. Oversampling causes data to be randomly repeated, whichaffects test data because dividing data may result in duplicates. Un-dersampling may result in the loss of some strong information. Inorder to advance email research, it is crucial to provide datasets oncriminal activity. P. Garg et al. (2021) [ 5], which revealed that spamin an email was detected in 70 percent of business emails, spam wasestablished as an obstacle for email administrators. Recognizingspam and getting rid of it were the primary concerns, as spam canbe offensive, may lead to other internet sites being tricked, whichcan offer harmful data, and can feature those who are not particu-lar with their content using NLP. To select the best-trained model,each mail transmission protocol requires precise and effective emailclassification, a machine learning comparison is done. Our studyhas suggested that innovative deep learning outperforms learningalgorithms like SVM and RF. Current studies on the classificationof emails use a variety of machine learning (ML) techniques, witha few of them focusing on the study of the sentiments consisted ofwithin email databases. The lack of datasets is a significant obstacleto email classification. There are few publicly accessible E-maildatasets, thus researchers must use these datasets to test their hy-potheses or gather data on their own. Authors[ 15] describe suppliedtwo-phased outlier detection models to enhance the IIOT networksdependability. Artificial Neural Network, SVM, Gaussian NB, andRF (random forest) ensemble techniques were performed to forecastclass labels, and the outputs were input into a classifying unit toincrease accuracy. A method for content-based phishing detectionwas presented by the authors in [ 2], to classify phishing emails,they employed RF. They categorize spam and phishing emails. Theyenhanced phishing email classifiers with more accurate predictionsby extracting features. They showed some effective Machine learn-ing spam filtering techniques. When the PCA method is used, it willlower the number of features in the dataset. The collected featuresgo through the PCA algorithm to reduce the number of features.The PCA method is used to make a straightforward representationof the information which illustrates the amount of variability thereis in the data. The authors of [ 20] presented the Fuzzy C-meansmethod for classifying spam email. To stop spam, they implementeda membership threshold value. A methodology to identify unla-beled data was put forth by the authors of [ 1] and applied motiveanalysis to the Enron data collection. They divided the data intocategories that were favorable, negative, and neutral. They groupedthe data using k-means clustering, an unsupervised ML techniqueand then classified it using the supervised ML techniques SVM andNB. Hina, Maryam, and colleagues (2021) implemented Sefaced:Deep learning-based semantic analysis and categorization of e-maildata using a forensic technique. For multiclass email classification,SeFACED employs a Gated Recurrent Neural Network (GRU) basedon Long Short-Term Memory (LSTM). Different random weight ini-tializations affect LSTMs [ 9]. Zhang, Yan, et al.(2019) Experimentson three-way game-theoretic rough set (GTRS) email spam filter-ing show that it is feasible to significantly boost coverage withoutdecreasing accuracy [ 23]. According to Xia et al. [ 22], SMS spamhas been identified using machine learning model such as naivebayes , vector-space modeling, support vector machines (SVM),long selective memory machines (LSTM), and convolutional neuralnetworks including every instance of a method for categorizingdata. Elshoush, Huwaida, et al. (2019) Using adaboost and stochasticgradient descent (sgd) algorithms for e-mail filtering with R andorange software spam [ 3]. Orange software was used to create theclassifications, which included Adaboost and SGD. The majority ofresearchers focused on text-based email spam classification meth-ods because image-based spam can be filtered in the early stagesof pre-processing. There are widely used word bag (BoW) model,which believes that documents are merely unordered collectionsof words, is the foundation for these techniques. Kumaresan [ 11]explains SVM with a cuckoo search algorithm was used to extracttextual features for spam detection. Renuka and Visalakshi madeuse of svm [ 17] spam email identification, followed by selectingfeatures using Latent Semantic Indexing (LSI). Here we have usedlabeled dataset to train the hybrid classifier. We used TF-IDF forfeature extraction [ 20] and Textual features for spam detectionwere extracted using SVM and a cuckoo search algorithm. [ 4] forfiltering out the spam email. Combining the integrated strategy tothe pure SVM and NB methods, overall accuracy is really improved.Moreover, accurate detection for spam email has been proposedusing the Negative Selection Algorithm (NSA) and Particle SwarmOptimizations (PSO) algorithm. PSO is used in this instance toimprove the effectiveness of the classifier.182Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model IC3 2023, August 0305, 2023, Noida, India1.2 Motivation and NoveltyEmail is most common form of communication between peoplein this digital age. Many users have been victims of spam emails,and their personal information has been compromised. The emailClassification technique is employed to identify and filter junkmail, junk, and virus-infected emails prior to reach a users inbox.Existing email classification methods result in irrelevant emailsand/or the loss of valuable information. Keeping these constraintsin mind, the following contributions are made in this paper:Text-based feature extraction is a lengthy process. Further-more, extracting every important feature from text is difficult.In this paper, we show how to employ GRU with Convo-lutional Neural Networks and Bidirectional-LSTM to findspam.Used Word-Embeddings, BiLSTM, and Gated Recurrent Neu-ral Networks to examine the relationships, sentimental con-tent, and sequential way of email contents.Applied CNN before the Bi-LSTM network, training time canbe sped up. This network can also extract more advancedtextual features faster than the Bi-LSTM network alone whencombined with the GRU network.We use Enorn Corpora datasets and compute precision, re-call, and f-score to assess how well the suggested techniqueperforms. Our model outperforms several well-known ma-chine learning techniques as well as more contemporarymethods for spam message detection.2 PROPOSED SYSTEM ARCHITECTURE ANDMODELE-mail is a valuable tool for communicating with other users. Emailallows the sender to efficiently forward millions of advertisementsat no cost. Unfortunately, this scheme is now being used in a varietyof organizations. As a result, a massive amount of redundant emailsis known as spam or junk mail, many people are confused about theemails in their E- Mailboxes. Each learning sequence is given for-ward as well as backward to two different LSTM networks that areattached to the same outputs layer in order for bidirectional Lstmsto function. This indicates that the Bi-LSTM has detailed sequentialinformation about all points before and following each point in aspecific sequence. In other words, we concatenate the outputs fromboth the forward and the backward LSTM at each time step ratherthan just encoding the sequence in the forward direction. Eachwords encoded form now comprehends the words that come beforeand after it. This is a problem for the Internet community. The di-agram depicts various stages that aid in the prediction of email spam:Because real-world data is messy and contains unnecessary infor-mation and duplication, data preprocessing is critical in naturallanguage processing (NLP). The major preprocessing steps are de-picted below.2.1 NLP TokenizationTokenization of documents into words follows predefined rules.The tokenization step is carried out in Python with spacy library.2.2 Stop Words RemovalStop words appear infrequently or frequently in the document, butthey are less significant in terms of importance. As a result, theseare removed to improve data processing.2.3 Text NormalizationA words lexicon form or order may differ. Thus, they must all bechanged to their root word to be correctly analyzed. Lemmatizationand stemming are the two methods that can be used for normal-ization. When a words final few characters are removed to createa shorter form, even if that form has no meaning, the procedureis known as stemming. lemmatization [ 21] is a mixture of corpus-based an rule-based methods, and it retains the context of a termwhile changing it back to its root.2.4 Feature Extractionfeature extraction which transforms the initial text into its featuresso that it may be used for modeling after being cleaned up andnormalized. Before predicting them, we use a specific way to giveweights to specific terms in our document. While it is simple for acomputer to process numbers, we choose to represent individualwords numerically. In such cases, we choose word embeddings. IDFis the count of documents containing the term divided by the totalnumber of documents, and occurrence is the amount of instances aword appears in a document. We derive characteristics based onequations. 1,2,3,4,5, and 6. We use equations to derive properties. =1(1) =Inverse() (2)(,,)=(,).(,) (3)(,)=log||(4)A word2vec neural network-based approach is the method that isutilized for this goal as the tool. The following equation, referredto as 5, shows how word2vec handles word context through theuse of probability-accurate measurements. Here letter D stands forthe paired-wise display of a set of words, while the letters w and c0or c1 represent paired word context that originated from a largercollection of set D.(=1|, 11:)=11+(11+12+...+1)(5)(=1|, 1:)=11+(0)(6)183IC3 2023, August 0305, 2023, Noida, India Sachan et al.2.5 Word-EmbeddingsWord-Embedding helps to improve on the typical \"bag-of-words\"worldview, which requires a massive sparse feature vector to scoreevery word individually to represent this same entire vocabulary.This perception is sparse because the vocabulary is large, and eachword or document is defined by a massive vector. Using a wordmap-based dictionary, word embedding needs to be converted terms(words) into real value feature vectors. There are two basic issueswith standard feature engineering techniques for deep learning.Data is represented using sparse vectors, and the second is thatsome of the meanings of words are not taken into consideration.Similar phrases will have values in embedding vectors that arealmost real-valued. The Input length in our proposed study is setto 700 for our suggested model. If the texts seemed to be integerencoded with value systems between 10 and 20, the vocabularydistance would be 11. Our data is encoded as integers, and the inputand output dimensions are both set to 50,000. The embedding layeroutcome will be used in successive layers and for BiLSTM and GRUlayers.2.6 Machine Learning ModelWithin the scope of the research, we are using the subsequent ma-chine learning techniques, to examine and compare the overallefficacy of our suggested Bi-LSTM strategy: Support Vector Ma-chine, Gaussian NB, Logistic Regression, K - nearest neighbors, andRandom Forest (RF).2.7 Convolution NetworkThe popular RNN model generally performs well but takes toolong to train the model incorporating the textual sequential data.When a layer is added after the RNN layer, the models learningduration is considerably decreased. Higher-level feature extractionis another benefit. [ 19] additionally possible using the convolutionallayer. In essence, the convolution layer looks for combinations ofthe various words or paragraphs in the document that involve thefilters. We use features with 128 dimensions and a size 10 for each.For this task, the Relu activation function is utilized. After that, theone-dimensional largest pooling layers with a pooling size of 4 areput on the data in order to obtain higher-level features.2.8 BiLSTM Network with GRURecurrent Neural Network (RNN) technique of text sentiment anal-ysis is particularly well-liked and frequently applied. Recurrentneural networks (RNN) surpass conventional neural networks. be-cause it can remember the information from earlier time stepsthanks to its memory. A state vector is combined with an RNNsdata to create a new state vector. The resulting state vector uses thepresent to recollect past knowledge. The RNN is straightforwardand is based on the following equations:=tanh(1+) (7)= (8)The vanilla RNN[ 18]is not very good at remembering previoussequences. In addition to that, RNN struggles with diminishinggradient descent. A kind of RNN is a long short-term recall network(LSTM), solves a vanishing gradient descent problem and learnslong-term dependencies[ 10]. LSTM was actually created to addressthe problem of long-term reliance. LSTM has the unique ability torecall. The cell state is the LSTM models central concept. Withonly a small amount of linear interaction, the cell state follows thesequence essentially unmodified from beginning to end. gate ofan LSTM is also significant. Under the command of these gates,information is safely inserted to or eliminated from the cell stated.The following equations are used by the LSTM model to updateeach cell:=[1,]+(9)In this case, Xt denotes input, and ht is the hidden state at the ttime step. The following is the revised cell state Ct:t=([1,]+) (10)=tanh([1,]+) (11)=1+ (12)Here, we may compute the output and hidden state at t time stepsusing the point-wise multiplication operator *.=([1,]+) (13)=tanh() (14)Due to the reality it only considers all prior contexts from thepresent one, LSTM does have a few drawbacks. As a result of this,it may accept data from preceding time steps through LSTM as wellas RNN. Therefore, in order to avoid this issue, further improve-ments are carried out with the help of a bidirectional recurrentneural network(Bi-RNN). BiRNN [ 13] can handle two pieces of in-formation from both the front and the back. Bi-LSTM is createdby combining the Bi-RNN and LSTM. As a result, operating LSTMhas advantages such as cell state storage so that BiRNN have wayto acknowledge from the context before and after. As a conse-quence of this, it provides the Bi-LSTM with the advantages of anLSTM with feedback for the next layer. Remembering long-termdependencies is a significant new benefit of Bi-LSTM. The output,which is a feature vector, will be based on the call state. Finally,we forecast the probability of email content as Normal, Fraudu-lent, Harassment, and Suspicious Emails using as an input to thesoftmax activation function, which is a weighted sum of the denselayers outputs. To regulate the information flow, GRU employsthe point-wise multiplying function and logistic sigmoid activation.The GRU has hidden states of storage memory and does not havedistinct memory cells or units for state control. The W, U, and bvectors, which stand for weights, gates, and biases, respectively, arecrucial variables that must be calculated during the creation of theGRU model. For training reasons, the pre-trained word embeddingknown as the Glove vector is used. They made it clear that GRUis the superior model when there is a large amount of trainingdata for textual groups and word embedding is available. BiLSTM,CNN, and GRU is required so as to compensate for the deletionof the documents long-term and short-term connections. In ourcase, the embedding dimension, maximum sequence length, andlexicon size were used to start the LSTM embedding layer in threeseparate LSTM models. The input vector was modified to make itappropriate for such a Conv1D layer, prior situations sequences arereturned by LSTM layer. The \"return sequences\" of the LSTM layermust be set to False when the subsequent state is free of the gated184Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model IC3 2023, August 0305, 2023, Noida, Indiaarchitecture. Quantity of learning parameters must be taken intoconsideration. A 350-unit LSTM layer was set - up, and differentLSTM unit combinations were tested. More importantly, becauseit has more parts, the model made with BiLSTM will take longerto train. Bidirectional LSTM is the name of a particular kind ofrecurrent neural network that is primarily used for the processingof natural languages. (BiLSTM). It is able to use data from bothsides, and, in contrast to regular LSTM, it enables input flow inboth directions. It is an effective instrument for demonstrating thelogical relationships between words and phrases, and this involvesboth the forward and backward directions of the sequence. In con-clusion, BiLSTM works by adding one extra layer of LSTM, causingthe information flow to travel in the other direction. It only denotesthat the input sequence runs in reverse at the next LSTM layer. Mul-tiple operations, including averaging, summation, multiplication,and concatenation, are then applied to the results of the two LSTMlayers. The gated design of Bi-LSTM and GRU networks solvesthe disappearing gradient and exploding problems. A good way tohandle more long sequences is to use Bi-LSMT and GRU together.GRU works well with datasets that dont have text. In two to threerounds, the complicated CNN+BiLSTM+GRU model learns the longsequence of email text well. We have used word embedding, cnn,bidirectional lstm and gru networks as our three building blocksto separate email messages based on their sentiment and textssequential features. Also, we succinctly demonstrate below whythese blocks help identify email spam:First, We have used the Sequence - to - sequence Lstm as thecurrent block in the networks since it can retrieve both theprevious and next sequences from the current. More so thana straightforward LSTM network, it can also recognize andextract text sentiment and sequential properties.Second, we extract the more complex and advanced charac-teristics for Bi-LSTM network using Convolutional Networkblock, which is the networks second block after the Bi-LSTMblock. Bi-LSTM takes a long time to extract text-based fea-tures, hence one of the reasons for using this block is toreduce the networks overall training time.3 EXPERIMENTAL EVALUATION3.1 Experimental SetupWe divided the information into training and testing groups of80/20. We divided the remaining 20% of the 80 percent trainingdata into test data for the model. Construct, compute, and evaluatethe efficacy of the suggested method using the Pythonic packagesKeras, as TensorFlow and Scikit learn.3.2 Dataset DescriptionEmail spam detection is the foundation of this research project. Thedataset includes normal emails from the Enron corpora, deceptiveemails from phished email corpora, harassment emails chosen fromhate speech, and the offensive dataset. Only the content of the emailbody is used for analysis; all header information, including sender,topic, CC, and BCC, are eliminated. Word2vector, TF-IDF, and WordEmbedding are used to extract characteristics from the email mes-sage and classify them. This dataset[ 8] is publicly available. Thepresented model is implemented using Python, and several metrics,including accuracy, precision, and recall, are used to examine theoutcomes.3.3 Evaluation Metrics and ResultsClassifier performance is assessed Using metrics such as accuracy,precision, and recall. Four terms make up a confusion matrix thatis used to calculate these metrics.True positives (TP) are positive values that have been accu-rately assigned the positive label.The negative values that are accurately identified as negativeare known as True Negatives (TN).True Negative values are those that can be accurately identi-fied as being negative (TN).Positive readings that have been mistakenly labeled as nega-tive are known as False Negatives (FN).Assess the efficacy of the suggested model is listed below:3.3.1 Accuracy. Accuracy reveals how frequently the ML modelwas overall correct.Accuracy =++++(15)3.3.2 Precision. The accuracy of the model gauges how effectivelyit can predict a specific category.Precision =+(16)3.3.3 Recall. Recall tells us how often the model was able to rec-ognize a specific category.Recall =+(17)Model Accuracy Precision RecallGaussian NB 91.3 90.1 91.8Random Forest 88.41 90 88KNN 86.6 89 87SVM 92.4 91 92LSTM 95.2 95 95.7Proposed Ensemble(CNN,BiLSTM+GRU)97.32 95.6 95.3Table 1: Differet Models Score on Test DataAccuracy, Precision, and Recall metrics are computed. In thegiven Table 1 where six different classifiers are Gaussian NB, Ran-dom Forest, KNN, SVM, LSTM, and Propose Ensemble HybridModel (CNN+BiLSTM+GRU) have been used in this work. In theCNN, Bi-LSTM, and GRU architectures which enable sequence pre-diction, CNN strands for feature extraction on data input which arecombined with LSTM. It requires less time training and a higherexpandable model. Any bottlenecks are created by predictions andthe increasing number of distinct units of information. This modelis useful for dealing with issue-related classifications that consistof two or more than two classes. So suggested Ensemble model, outof these six classifiers, produces more accurate findings.185IC3 2023, August 0305, 2023, Noida, India Sachan et al.Figure 1: Performance Analysis3.4 Comparative AnalysisAmodels ability to fit new data is measured by the validationloss, whereas its ability to fit training data is determined by thetraining loss. The two main variables that decide whether in whichlearning is efficient or not are validation loss and training loss.LSTM and Suggested Ensemble hybrid Models have equivalent lossand accuracy. In this context, we are contrasting the LSTM with theproposed model (CNN, Bilstm, and GRU) in terms of their respectivevalidation accuracies and losses. The models accuracy was at itshighest after 14 epochs of operation when it achieved an accuracyof roughly 97-98% while minimizing model loss.Figure 2: LSTM Model Training and Validation AccuracyFigure 3: LSTM Model Training and Validation LossFigure 4: Ensemble Model (CNN,BiLSTM+GRU) Trainingand Validation AccuracyFigure 5: Ensemble Model (CNN,BiLSTM+GRU)Trainingand Validation Loss186Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model IC3 2023, August 0305, 2023, Noida, IndiaIn this Proposed ensemble hybrid models train accuracy is 98.7%Validation accuracy is 97.32% and LSTM has train accuracy of 97.41%and validation accuracy is 95.2%. So based on figures 3 and 5 indicatethe validation loss for LSTM and the proposed ensemble hybridmodel to be 0.93 and 0.84, respectively, and figures 2 and 4 show thevalidation accuracy to be 95.2% and 97.3%, respectively. LSTM andthe proposed hybrid model used ensemble artificial intelligence,with the proposed hybrid model outperforming the LSTM. Wedecide on dense architecture as the final model for identifying thetext messages as spam or nonspam based on loss, accuracy, and theaforementioned charts. The loss and accuracy over epochs are morestable than LSTM, and the Proposed classifier has a straightforwardstructure.4 CONCLUSIONThe model is composed of four networks Word-Embeddings, CNN,Bi-LSTM, and GRU. We may train the model more quickly by usingthe convolutional layer first, followed by the word-embedding layer,and then the BiLSTM network. The Bidirectional LSTM networkalso has higher-level properties that we can extract. We have useda bidirectional LSTM(BiLSTM)and GRU network to memorize asentences contextual meaning and sequential structure, which im-proves the models performance accuracy to roughly 97.32 percent.REFERENCES[1]Rayan Salah Hag Ali and Neamat El Gayar. 2019. Sentiment analysis using unla-beled email data. In 2019 International Conference on Computational Intelligenceand Knowledge Economy (ICCIKE) . IEEE, 328333.[2]Ali Shafigh Aski and Navid Khalilzadeh Sourati. 2016. Proposed efficient algo-rithm to filter spam using machine learning techniques. Pacific Science Review A:Natural Science and Engineering 18, 2 (2016), 145149.[3]Huwaida T Elshoush and Esraa A Dinar. 2019. Using adaboost and stochasticgradient descent (sgd) algorithms with R and orange software for filtering e-mailspam. In 2019 11th Computer Science and Electronic Engineering (CEEC) . IEEE,4146.[4]Weimiao Feng, Jianguo Sun, Liguo Zhang, Cuiling Cao, and Qing Yang. 2016. Asupport vector machine based naive Bayes algorithm for spam filtering. In 2016IEEE 35th International Performance Computing and Communications Conference(IPCCC) . IEEE, 18.[5]Pranjul Garg and Nancy Girdhar. 2021. A Systematic Review on Spam FilteringTechniques based on Natural Language Processing Framework. In 2021 11th Inter-national Conference on Cloud Computing, Data Science & Engineering (Confluence) .IEEE, 3035.[6]Adam Kavon Ghazi-Tehrani and Henry N Pontell. 2021. Phishing evolves: Ana-lyzing the enduring cybercrime. Victims & Offenders 16, 3 (2021), 316342.[7]Radicati Group et al .2015. Email Statistics Report 20152019. Radicati Group.Accessed August 13 (2015), 2019.[8]Maryam Hina, Mohsin Ali, and Javed. 2021. Sefaced: Semantic-based forensicanalysis and classification of e-mail data using deep learning. IEEE Access 9(2021), 9839898411.[9]Maryam Hina, Mohsin Ali, Abdul Rehman Javed, Fahad Ghabban, Liaqat AliKhan, and Zunera Jalil. 2021. Sefaced: Semantic-based forensic analysis andclassification of e-mail data using deep learning. IEEE Access 9 (2021), 9839898411.[10] Weicong Kong, Zhao Yang Dong, Youwei Jia, David J Hill, Yan Xu, and YuanZhang. 2017. Short-term residential load forecasting based on LSTM recurrentneural network. IEEE transactions on smart grid 10, 1 (2017), 841851.[11] T Kumaresan and C Palanisamy. 2017. E-mail spam classification using S-cuckoosearch and support vector machine. International Journal of Bio-Inspired Compu-tation 9, 3 (2017), 142156.[12] Nuha H Marza, Mehdi E Manaa, and Hussein A Lafta. 2021. Classification ofspam emails using deep learning. In 2021 1st Babylon International Conference onInformation Technology and Science (BICITS) . IEEE, 6368.[13] Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neuralnetwork language model. In 2012 IEEE Spoken Language Technology Workshop(SLT) . IEEE, 234239.[14] Sarwat Nizamani, Nasrullah Memon, Mathies Glasdam, and Dong Duong Nguyen.2014. Detection of fraudulent emails by employing advanced feature abundance.Egyptian Informatics Journal 15, 3 (2014), 169174.[15] V Priya, I Sumaiya Thaseen, Thippa Reddy Gadekallu, Mohamed K Aboudaif,and Emad Abouel Nasr. 2021. Robust attack detection approach for IIoT usingensemble classifier. arXiv preprint arXiv:2102.01515 (2021).[16] Justinas Rastenis, Simona Ramanauskait e, Justinas Januleviius, Antanas enys,Asta Slotkien e, and Kstutis Pakrijauskas. 2020. E-mail-based phishing attacktaxonomy. Applied Sciences 10, 7 (2020), 2363.[17] Karthika D Renuka and P Visalakshi. 2014. Latent semantic indexing based SVMmodel for email spam classification. (2014).[18] Shuvendu Roy, Sk Imran Hossain, MAH Akhand, and N Siddique. 2018. Sequencemodeling for intelligent typing assistant with Bangla and English keyboard. In2018 International Conference on Innovation in Engineering and Technology (ICIET) .IEEE, 16.[19] Tara N Sainath, Oriol Vinyals, Andrew Senior, and Haim Sak. 2015. Convolu-tional, long short-term memory, fully connected deep neural networks. In 2015IEEE international conference on acoustics, speech and signal processing (ICASSP) .Ieee, 45804584.[20] Anuj Kumar Singh, Shashi Bhushan, and Sonakshi Vij. 2019. Filtering spammessages and mails using fuzzy C means algorithm. In 2019 4th InternationalConference on Internet of Things: Smart Innovation and Usages (IoT-SIU) . IEEE,15.[21] Kristina Toutanova and Colin Cherry. 2009. A global model for joint lemmati-zation and part-of-speech prediction. In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP . 486494.[22] Tian Xia. 2020. A constant time complexity spam detection algorithm for boostingthroughput on rule-based filtering systems. IEEE Access 8 (2020), 8265382661.[23] Yan Zhang, PengFei Liu, and JingTao Yao. 2019. Three-way email spam filteringwith game-theoretic rough sets. In 2019 International conference on computing,networking and communications (ICNC) . IEEE, 552556.Received 15 April 2023187","date":"2024-02-05T14:21:58.795324","authors":[{"full_name":"Khushbu Doulani","institut_name":"Vardhaman College of Engineering, Hyderabad, India"},{"full_name":"Mainak Adhikari","institut_name":"Department of CSE, IIIT Lucknow, UP, India"},{"full_name":"Shivangi Sachan","institut_name":"Department of CSE, IIIT Lucknow, Lucknow, UP, India"}],"author":[{"full_name":"Khushbu Doulani","institut_name":"Vardhaman College of Engineering, Hyderabad, India"},{"full_name":"Mainak Adhikari","institut_name":"Department of CSE, IIIT Lucknow, UP, India"},{"full_name":"Shivangi Sachan","institut_name":"Department of CSE, IIIT Lucknow, Lucknow, UP, India"}]}},{"_index":"article_index","_id":"9","_score":1.0,"_source":{"id":9,"title":"Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model","summary":"The emergence of novel types of communication, such as email, has been brought on by the development of the internet, which radically concentrated the way in that individuals communicate socially and with one another. It is now establishing itself as a crucial aspect of the communication network which has been adopted by a variety of commercial enterprises such as retail outlets. So in this research paper, we have built a unique spam-detection methodology based on email-body sentiment analysis. The proposed hybrid model is put into practice and preprocessing the data, extracting the properties, and categorizing data are all steps in the process. To examine the emotive and sequential aspects of texts, we use word embedding and a bi-directional LSTM network. this model frequently shortens the training period, then utilizes the Convolution Layer to extract text features at a higher level for the BiLSTM network. Our model performs better than previous versions, with an accuracy rate of 97–98%. In addition, we show that our model beats not just some well-known machine learning classifiers but also cutting-edge methods for identifying spam communications, demonstrating its superiority on its own. Suggested Ensemble model’s results are examined in terms of recall, accuracy, and precision","keywords":"Dataset, KNN, Gaussian Naive Bayes, LSTM, SVM, Bidirectional LSTM, GRU, Word-Embeddings, CNN","content":"Semantic Analysis and Classification of Emails throughInformative Selection of Features and Ensemble AI ModelShivangi SachanDepartment of CSEIIIT LucknowLucknow, UP, Indiamcs21025@iiitl.ac.inKhushbu DoulaniVardhaman College of EngineeringHyderabad, Indiakhushidoulani@gmail.comMainak AdhikariDepartment of CSEIIIT LucknowUP, Indiamainak.ism@gmail.comABSTRACTThe emergence of novel types of communication, such as email, hasbeen brought on by the development of the internet, which radicallyconcentrated the way in that individuals communicate socially andwith one another. It is now establishing itself as a crucial aspect ofthe communication network which has been adopted by a varietyof commercial enterprises such as retail outlets. So in this researchpaper, we have built a unique spam-detection methodology basedon email-body sentiment analysis. The proposed hybrid model isput into practice and preprocessing the data, extracting the proper-ties, and categorizing data are all steps in the process. To examinethe emotive and sequential aspects of texts, we use word embed-ding and a bi-directional LSTM network. this model frequentlyshortens the training period, then utilizes the Convolution Layer toextract text features at a higher level for the BiLSTM network. Ourmodel performs better than previous versions, with an accuracyrate of 9798%. In addition, we show that our model beats not justsome well-known machine learning classifiers but also cutting-edgemethods for identifying spam communications, demonstrating itssuperiority on its own. Suggested Ensemble models results areexamined in terms of recall, accuracy, and precisionCCS CONCEPTSComputer systems organization Embedded systems ;Re-dundancy ; Robotics; NetworksNetwork reliability.KEYWORDSDataset, KNN, Gaussian Naive Bayes, LSTM, SVM, BidirectionalLSTM, GRU, Word-Embeddings, CNNACM Reference Format:Shivangi Sachan, Khushbu Doulani, and Mainak Adhikari. 2023. SemanticAnalysis and Classification of Emails through Informative Selection ofFeatures and Ensemble AI Model. In 2023 Fifteenth International Conferenceon Contemporary Computing (IC3-2023) (IC3 2023), August 0305, 2023, Noida,India. ACM, New York, NY, USA, 7 pages. https://doi.org/10.1145/3607947.3607979Both authors contributed equally to this research.Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than theauthor(s) must be honored. Abstracting with credit is permitted. To copy otherwise, orrepublish, to post on servers or to redistribute to lists, requires prior specific permissionand/or a fee. Request permissions from permissions@acm.org.IC3 2023, August 0305, 2023, Noida, India2023 Copyright held by the owner/author(s). Publication rights licensed to ACM.ACM ISBN 979-8-4007-0022-4/23/08. . . $15.00https://doi.org/10.1145/3607947.36079791 INTRODUCTIONOver the past few years, a clear surge of both the amount of spam-mers as well as spam emails. This is likely due to a fact that theinvestment necessary for engaging in the spamming industry isrelatively low. As a result of this, we currently have a system thatidentifies every email as suspicious, which has caused major expen-ditures in the investment of defense systems [ 12]. Emails are usedfor online crimes like fraud, hacking, phishing, E-mail bombing, bul-lying, and spamming. [ 16]. Algorithms that are based on machinelearning (ML) are now the most effective and often used approach tothe recognition of spam. Phishing, which is defined as a fraudulentattempt to acquire private information by masquerading as a trust-worthy party in electronic communication, has rapidly advancedpast use of simple techniques and the tactic of casting a wide net;instead, spear phishing uses a variety of sophisticated techniquesto target a single high-value individual. Other researchers used NB,Decision Trees, and SVM to compare the performance of supervisedML algorithms for spam identification [ 6]. Spam emails clog up re-cipients inboxes with unsolicited communications, which frustratethem and push them into the attackers planned traps [ 7]. As a re-sult, spam messages unquestionably pose a risk to both email usersand the Internet community. In addition, Users may occasionallyread the entire text of an unsolicited message that is delivered tothe target users inboxes without realizing that the message is junkand then choosing to avoid it. Building a framework for email spamdetection is the aim of this project. In this approach, we combine theWord-Embedding Network with the CNN layer, Bi-LSTM, and GRU(BiLSTM+GRU). CNN layers are used to speed up training timebefore the Bi-LSTM network, and more advanced textual character-istics are extracted with the use of this network in comparison tothe straight LSTM network, in less time. Gated recurrent neural net-works (GRUs) are then added because they train more quickly andperform better for language modeling. To evaluate and investigatevarious machine learning algorithms for predicting email spam,and develop a hybrid classification algorithm to filter email spambefore employing an ensemble classification algorithm to forecastit. To put an innovative technique into practice and compare it tothe current method in terms of various metrics. Ensemble learn-ing, a successful machine learning paradigm, combines a group oflearners rather than a single learner to forecast unknown targetattributes. Bagging, boosting, voting, and stacking are the four maintypes of ensemble learning techniques. To increase performance,an integrated method and the combining of two or three algorithmsare also suggested. Extraction of text-based features takes a longtime. Furthermore, it can be challenging to extract all of the crucialinformation from a short text. Over the span associated with this181IC3 2023, August 0305, 2023, Noida, India Sachan et al.research, we utilize Bidirectional Large Short-Term Memories (Bi-LSTM) in conjunction with Convolutional Neural Networks (CNN)to come up with an innovative method to the detection of spam.Bagging and boosting approaches were widely preferred in thisstudy. Contribution and paper organization is as follows: section 1.1describes literature study, section 1.2 describe motivation for thisresearch work, section 2 sketches procedure of details implemen-tation, Section 3 present experimental setup, dataset descriptionand evaluation metrics, and section 4 summarizing outcomes of theexperiment.1.1 Related WorkEmail is indeed the second most frequently utilized Internet appli-cation as well as the third most common method of cyberbullying,claims one study. Cybercriminals exploit it in a number of ways,including as sending obscene or abusive messages, adding virusesto emails, snatching the private information of victims, and ex-posing it to a broad audience. Spam letters made up 53.95% of allemail traffic in March 2020. We examine three main types of un-lawful emails in our study. First are fake emails, which are sentto manipulate recipients to submit sensitive information. The sec-ond as being cyberbullyings use of harassing emails to threatenindividuals. Suspicious emails that describe illegal activities belongto the third category. Many researchers have earlier contributedmassively to this subject. The researcher claims there is some proofthat suspicious emails were sent before to the events of 9/11. [ 14].When it comes to data labeling, there are also convinced rule-basedapproaches and technologies ( like VADER) that are used, eventhough their efficiency of the are together is adversely affected. Ahidden layer, which itself is essential for vectorization, is the toplayer of the model. We use oversampling methods for this minorityclass because of the absence of data. Sampling techniques can helpwith multicollinearity, but they have an impact on simulation re-sults. Oversampling causes data to be randomly repeated, whichaffects test data because dividing data may result in duplicates. Un-dersampling may result in the loss of some strong information. Inorder to advance email research, it is crucial to provide datasets oncriminal activity. P. Garg et al. (2021) [ 5], which revealed that spamin an email was detected in 70 percent of business emails, spam wasestablished as an obstacle for email administrators. Recognizingspam and getting rid of it were the primary concerns, as spam canbe offensive, may lead to other internet sites being tricked, whichcan offer harmful data, and can feature those who are not particu-lar with their content using NLP. To select the best-trained model,each mail transmission protocol requires precise and effective emailclassification, a machine learning comparison is done. Our studyhas suggested that innovative deep learning outperforms learningalgorithms like SVM and RF. Current studies on the classificationof emails use a variety of machine learning (ML) techniques, witha few of them focusing on the study of the sentiments consisted ofwithin email databases. The lack of datasets is a significant obstacleto email classification. There are few publicly accessible E-maildatasets, thus researchers must use these datasets to test their hy-potheses or gather data on their own. Authors[ 15] describe suppliedtwo-phased outlier detection models to enhance the IIOT networksdependability. Artificial Neural Network, SVM, Gaussian NB, andRF (random forest) ensemble techniques were performed to forecastclass labels, and the outputs were input into a classifying unit toincrease accuracy. A method for content-based phishing detectionwas presented by the authors in [ 2], to classify phishing emails,they employed RF. They categorize spam and phishing emails. Theyenhanced phishing email classifiers with more accurate predictionsby extracting features. They showed some effective Machine learn-ing spam filtering techniques. When the PCA method is used, it willlower the number of features in the dataset. The collected featuresgo through the PCA algorithm to reduce the number of features.The PCA method is used to make a straightforward representationof the information which illustrates the amount of variability thereis in the data. The authors of [ 20] presented the Fuzzy C-meansmethod for classifying spam email. To stop spam, they implementeda membership threshold value. A methodology to identify unla-beled data was put forth by the authors of [ 1] and applied motiveanalysis to the Enron data collection. They divided the data intocategories that were favorable, negative, and neutral. They groupedthe data using k-means clustering, an unsupervised ML techniqueand then classified it using the supervised ML techniques SVM andNB. Hina, Maryam, and colleagues (2021) implemented Sefaced:Deep learning-based semantic analysis and categorization of e-maildata using a forensic technique. For multiclass email classification,SeFACED employs a Gated Recurrent Neural Network (GRU) basedon Long Short-Term Memory (LSTM). Different random weight ini-tializations affect LSTMs [ 9]. Zhang, Yan, et al.(2019) Experimentson three-way game-theoretic rough set (GTRS) email spam filter-ing show that it is feasible to significantly boost coverage withoutdecreasing accuracy [ 23]. According to Xia et al. [ 22], SMS spamhas been identified using machine learning model such as naivebayes , vector-space modeling, support vector machines (SVM),long selective memory machines (LSTM), and convolutional neuralnetworks including every instance of a method for categorizingdata. Elshoush, Huwaida, et al. (2019) Using adaboost and stochasticgradient descent (sgd) algorithms for e-mail filtering with R andorange software spam [ 3]. Orange software was used to create theclassifications, which included Adaboost and SGD. The majority ofresearchers focused on text-based email spam classification meth-ods because image-based spam can be filtered in the early stagesof pre-processing. There are widely used word bag (BoW) model,which believes that documents are merely unordered collectionsof words, is the foundation for these techniques. Kumaresan [ 11]explains SVM with a cuckoo search algorithm was used to extracttextual features for spam detection. Renuka and Visalakshi madeuse of svm [ 17] spam email identification, followed by selectingfeatures using Latent Semantic Indexing (LSI). Here we have usedlabeled dataset to train the hybrid classifier. We used TF-IDF forfeature extraction [ 20] and Textual features for spam detectionwere extracted using SVM and a cuckoo search algorithm. [ 4] forfiltering out the spam email. Combining the integrated strategy tothe pure SVM and NB methods, overall accuracy is really improved.Moreover, accurate detection for spam email has been proposedusing the Negative Selection Algorithm (NSA) and Particle SwarmOptimizations (PSO) algorithm. PSO is used in this instance toimprove the effectiveness of the classifier.182Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model IC3 2023, August 0305, 2023, Noida, India1.2 Motivation and NoveltyEmail is most common form of communication between peoplein this digital age. Many users have been victims of spam emails,and their personal information has been compromised. The emailClassification technique is employed to identify and filter junkmail, junk, and virus-infected emails prior to reach a users inbox.Existing email classification methods result in irrelevant emailsand/or the loss of valuable information. Keeping these constraintsin mind, the following contributions are made in this paper:Text-based feature extraction is a lengthy process. Further-more, extracting every important feature from text is difficult.In this paper, we show how to employ GRU with Convo-lutional Neural Networks and Bidirectional-LSTM to findspam.Used Word-Embeddings, BiLSTM, and Gated Recurrent Neu-ral Networks to examine the relationships, sentimental con-tent, and sequential way of email contents.Applied CNN before the Bi-LSTM network, training time canbe sped up. This network can also extract more advancedtextual features faster than the Bi-LSTM network alone whencombined with the GRU network.We use Enorn Corpora datasets and compute precision, re-call, and f-score to assess how well the suggested techniqueperforms. Our model outperforms several well-known ma-chine learning techniques as well as more contemporarymethods for spam message detection.2 PROPOSED SYSTEM ARCHITECTURE ANDMODELE-mail is a valuable tool for communicating with other users. Emailallows the sender to efficiently forward millions of advertisementsat no cost. Unfortunately, this scheme is now being used in a varietyof organizations. As a result, a massive amount of redundant emailsis known as spam or junk mail, many people are confused about theemails in their E- Mailboxes. Each learning sequence is given for-ward as well as backward to two different LSTM networks that areattached to the same outputs layer in order for bidirectional Lstmsto function. This indicates that the Bi-LSTM has detailed sequentialinformation about all points before and following each point in aspecific sequence. In other words, we concatenate the outputs fromboth the forward and the backward LSTM at each time step ratherthan just encoding the sequence in the forward direction. Eachwords encoded form now comprehends the words that come beforeand after it. This is a problem for the Internet community. The di-agram depicts various stages that aid in the prediction of email spam:Because real-world data is messy and contains unnecessary infor-mation and duplication, data preprocessing is critical in naturallanguage processing (NLP). The major preprocessing steps are de-picted below.2.1 NLP TokenizationTokenization of documents into words follows predefined rules.The tokenization step is carried out in Python with spacy library.2.2 Stop Words RemovalStop words appear infrequently or frequently in the document, butthey are less significant in terms of importance. As a result, theseare removed to improve data processing.2.3 Text NormalizationA words lexicon form or order may differ. Thus, they must all bechanged to their root word to be correctly analyzed. Lemmatizationand stemming are the two methods that can be used for normal-ization. When a words final few characters are removed to createa shorter form, even if that form has no meaning, the procedureis known as stemming. lemmatization [ 21] is a mixture of corpus-based an rule-based methods, and it retains the context of a termwhile changing it back to its root.2.4 Feature Extractionfeature extraction which transforms the initial text into its featuresso that it may be used for modeling after being cleaned up andnormalized. Before predicting them, we use a specific way to giveweights to specific terms in our document. While it is simple for acomputer to process numbers, we choose to represent individualwords numerically. In such cases, we choose word embeddings. IDFis the count of documents containing the term divided by the totalnumber of documents, and occurrence is the amount of instances aword appears in a document. We derive characteristics based onequations. 1,2,3,4,5, and 6. We use equations to derive properties. =1(1) =Inverse() (2)(,,)=(,).(,) (3)(,)=log||(4)A word2vec neural network-based approach is the method that isutilized for this goal as the tool. The following equation, referredto as 5, shows how word2vec handles word context through theuse of probability-accurate measurements. Here letter D stands forthe paired-wise display of a set of words, while the letters w and c0or c1 represent paired word context that originated from a largercollection of set D.(=1|, 11:)=11+(11+12+...+1)(5)(=1|, 1:)=11+(0)(6)183IC3 2023, August 0305, 2023, Noida, India Sachan et al.2.5 Word-EmbeddingsWord-Embedding helps to improve on the typical \"bag-of-words\"worldview, which requires a massive sparse feature vector to scoreevery word individually to represent this same entire vocabulary.This perception is sparse because the vocabulary is large, and eachword or document is defined by a massive vector. Using a wordmap-based dictionary, word embedding needs to be converted terms(words) into real value feature vectors. There are two basic issueswith standard feature engineering techniques for deep learning.Data is represented using sparse vectors, and the second is thatsome of the meanings of words are not taken into consideration.Similar phrases will have values in embedding vectors that arealmost real-valued. The Input length in our proposed study is setto 700 for our suggested model. If the texts seemed to be integerencoded with value systems between 10 and 20, the vocabularydistance would be 11. Our data is encoded as integers, and the inputand output dimensions are both set to 50,000. The embedding layeroutcome will be used in successive layers and for BiLSTM and GRUlayers.2.6 Machine Learning ModelWithin the scope of the research, we are using the subsequent ma-chine learning techniques, to examine and compare the overallefficacy of our suggested Bi-LSTM strategy: Support Vector Ma-chine, Gaussian NB, Logistic Regression, K - nearest neighbors, andRandom Forest (RF).2.7 Convolution NetworkThe popular RNN model generally performs well but takes toolong to train the model incorporating the textual sequential data.When a layer is added after the RNN layer, the models learningduration is considerably decreased. Higher-level feature extractionis another benefit. [ 19] additionally possible using the convolutionallayer. In essence, the convolution layer looks for combinations ofthe various words or paragraphs in the document that involve thefilters. We use features with 128 dimensions and a size 10 for each.For this task, the Relu activation function is utilized. After that, theone-dimensional largest pooling layers with a pooling size of 4 areput on the data in order to obtain higher-level features.2.8 BiLSTM Network with GRURecurrent Neural Network (RNN) technique of text sentiment anal-ysis is particularly well-liked and frequently applied. Recurrentneural networks (RNN) surpass conventional neural networks. be-cause it can remember the information from earlier time stepsthanks to its memory. A state vector is combined with an RNNsdata to create a new state vector. The resulting state vector uses thepresent to recollect past knowledge. The RNN is straightforwardand is based on the following equations:=tanh(1+) (7)= (8)The vanilla RNN[ 18]is not very good at remembering previoussequences. In addition to that, RNN struggles with diminishinggradient descent. A kind of RNN is a long short-term recall network(LSTM), solves a vanishing gradient descent problem and learnslong-term dependencies[ 10]. LSTM was actually created to addressthe problem of long-term reliance. LSTM has the unique ability torecall. The cell state is the LSTM models central concept. Withonly a small amount of linear interaction, the cell state follows thesequence essentially unmodified from beginning to end. gate ofan LSTM is also significant. Under the command of these gates,information is safely inserted to or eliminated from the cell stated.The following equations are used by the LSTM model to updateeach cell:=[1,]+(9)In this case, Xt denotes input, and ht is the hidden state at the ttime step. The following is the revised cell state Ct:t=([1,]+) (10)=tanh([1,]+) (11)=1+ (12)Here, we may compute the output and hidden state at t time stepsusing the point-wise multiplication operator *.=([1,]+) (13)=tanh() (14)Due to the reality it only considers all prior contexts from thepresent one, LSTM does have a few drawbacks. As a result of this,it may accept data from preceding time steps through LSTM as wellas RNN. Therefore, in order to avoid this issue, further improve-ments are carried out with the help of a bidirectional recurrentneural network(Bi-RNN). BiRNN [ 13] can handle two pieces of in-formation from both the front and the back. Bi-LSTM is createdby combining the Bi-RNN and LSTM. As a result, operating LSTMhas advantages such as cell state storage so that BiRNN have wayto acknowledge from the context before and after. As a conse-quence of this, it provides the Bi-LSTM with the advantages of anLSTM with feedback for the next layer. Remembering long-termdependencies is a significant new benefit of Bi-LSTM. The output,which is a feature vector, will be based on the call state. Finally,we forecast the probability of email content as Normal, Fraudu-lent, Harassment, and Suspicious Emails using as an input to thesoftmax activation function, which is a weighted sum of the denselayers outputs. To regulate the information flow, GRU employsthe point-wise multiplying function and logistic sigmoid activation.The GRU has hidden states of storage memory and does not havedistinct memory cells or units for state control. The W, U, and bvectors, which stand for weights, gates, and biases, respectively, arecrucial variables that must be calculated during the creation of theGRU model. For training reasons, the pre-trained word embeddingknown as the Glove vector is used. They made it clear that GRUis the superior model when there is a large amount of trainingdata for textual groups and word embedding is available. BiLSTM,CNN, and GRU is required so as to compensate for the deletionof the documents long-term and short-term connections. In ourcase, the embedding dimension, maximum sequence length, andlexicon size were used to start the LSTM embedding layer in threeseparate LSTM models. The input vector was modified to make itappropriate for such a Conv1D layer, prior situations sequences arereturned by LSTM layer. The \"return sequences\" of the LSTM layermust be set to False when the subsequent state is free of the gated184Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model IC3 2023, August 0305, 2023, Noida, Indiaarchitecture. Quantity of learning parameters must be taken intoconsideration. A 350-unit LSTM layer was set - up, and differentLSTM unit combinations were tested. More importantly, becauseit has more parts, the model made with BiLSTM will take longerto train. Bidirectional LSTM is the name of a particular kind ofrecurrent neural network that is primarily used for the processingof natural languages. (BiLSTM). It is able to use data from bothsides, and, in contrast to regular LSTM, it enables input flow inboth directions. It is an effective instrument for demonstrating thelogical relationships between words and phrases, and this involvesboth the forward and backward directions of the sequence. In con-clusion, BiLSTM works by adding one extra layer of LSTM, causingthe information flow to travel in the other direction. It only denotesthat the input sequence runs in reverse at the next LSTM layer. Mul-tiple operations, including averaging, summation, multiplication,and concatenation, are then applied to the results of the two LSTMlayers. The gated design of Bi-LSTM and GRU networks solvesthe disappearing gradient and exploding problems. A good way tohandle more long sequences is to use Bi-LSMT and GRU together.GRU works well with datasets that dont have text. In two to threerounds, the complicated CNN+BiLSTM+GRU model learns the longsequence of email text well. We have used word embedding, cnn,bidirectional lstm and gru networks as our three building blocksto separate email messages based on their sentiment and textssequential features. Also, we succinctly demonstrate below whythese blocks help identify email spam:First, We have used the Sequence - to - sequence Lstm as thecurrent block in the networks since it can retrieve both theprevious and next sequences from the current. More so thana straightforward LSTM network, it can also recognize andextract text sentiment and sequential properties.Second, we extract the more complex and advanced charac-teristics for Bi-LSTM network using Convolutional Networkblock, which is the networks second block after the Bi-LSTMblock. Bi-LSTM takes a long time to extract text-based fea-tures, hence one of the reasons for using this block is toreduce the networks overall training time.3 EXPERIMENTAL EVALUATION3.1 Experimental SetupWe divided the information into training and testing groups of80/20. We divided the remaining 20% of the 80 percent trainingdata into test data for the model. Construct, compute, and evaluatethe efficacy of the suggested method using the Pythonic packagesKeras, as TensorFlow and Scikit learn.3.2 Dataset DescriptionEmail spam detection is the foundation of this research project. Thedataset includes normal emails from the Enron corpora, deceptiveemails from phished email corpora, harassment emails chosen fromhate speech, and the offensive dataset. Only the content of the emailbody is used for analysis; all header information, including sender,topic, CC, and BCC, are eliminated. Word2vector, TF-IDF, and WordEmbedding are used to extract characteristics from the email mes-sage and classify them. This dataset[ 8] is publicly available. Thepresented model is implemented using Python, and several metrics,including accuracy, precision, and recall, are used to examine theoutcomes.3.3 Evaluation Metrics and ResultsClassifier performance is assessed Using metrics such as accuracy,precision, and recall. Four terms make up a confusion matrix thatis used to calculate these metrics.True positives (TP) are positive values that have been accu-rately assigned the positive label.The negative values that are accurately identified as negativeare known as True Negatives (TN).True Negative values are those that can be accurately identi-fied as being negative (TN).Positive readings that have been mistakenly labeled as nega-tive are known as False Negatives (FN).Assess the efficacy of the suggested model is listed below:3.3.1 Accuracy. Accuracy reveals how frequently the ML modelwas overall correct.Accuracy =++++(15)3.3.2 Precision. The accuracy of the model gauges how effectivelyit can predict a specific category.Precision =+(16)3.3.3 Recall. Recall tells us how often the model was able to rec-ognize a specific category.Recall =+(17)Model Accuracy Precision RecallGaussian NB 91.3 90.1 91.8Random Forest 88.41 90 88KNN 86.6 89 87SVM 92.4 91 92LSTM 95.2 95 95.7Proposed Ensemble(CNN,BiLSTM+GRU)97.32 95.6 95.3Table 1: Differet Models Score on Test DataAccuracy, Precision, and Recall metrics are computed. In thegiven Table 1 where six different classifiers are Gaussian NB, Ran-dom Forest, KNN, SVM, LSTM, and Propose Ensemble HybridModel (CNN+BiLSTM+GRU) have been used in this work. In theCNN, Bi-LSTM, and GRU architectures which enable sequence pre-diction, CNN strands for feature extraction on data input which arecombined with LSTM. It requires less time training and a higherexpandable model. Any bottlenecks are created by predictions andthe increasing number of distinct units of information. This modelis useful for dealing with issue-related classifications that consistof two or more than two classes. So suggested Ensemble model, outof these six classifiers, produces more accurate findings.185IC3 2023, August 0305, 2023, Noida, India Sachan et al.Figure 1: Performance Analysis3.4 Comparative AnalysisAmodels ability to fit new data is measured by the validationloss, whereas its ability to fit training data is determined by thetraining loss. The two main variables that decide whether in whichlearning is efficient or not are validation loss and training loss.LSTM and Suggested Ensemble hybrid Models have equivalent lossand accuracy. In this context, we are contrasting the LSTM with theproposed model (CNN, Bilstm, and GRU) in terms of their respectivevalidation accuracies and losses. The models accuracy was at itshighest after 14 epochs of operation when it achieved an accuracyof roughly 97-98% while minimizing model loss.Figure 2: LSTM Model Training and Validation AccuracyFigure 3: LSTM Model Training and Validation LossFigure 4: Ensemble Model (CNN,BiLSTM+GRU) Trainingand Validation AccuracyFigure 5: Ensemble Model (CNN,BiLSTM+GRU)Trainingand Validation Loss186Semantic Analysis and Classification of Emails through Informative Selection of Features and Ensemble AI Model IC3 2023, August 0305, 2023, Noida, IndiaIn this Proposed ensemble hybrid models train accuracy is 98.7%Validation accuracy is 97.32% and LSTM has train accuracy of 97.41%and validation accuracy is 95.2%. So based on figures 3 and 5 indicatethe validation loss for LSTM and the proposed ensemble hybridmodel to be 0.93 and 0.84, respectively, and figures 2 and 4 show thevalidation accuracy to be 95.2% and 97.3%, respectively. LSTM andthe proposed hybrid model used ensemble artificial intelligence,with the proposed hybrid model outperforming the LSTM. Wedecide on dense architecture as the final model for identifying thetext messages as spam or nonspam based on loss, accuracy, and theaforementioned charts. The loss and accuracy over epochs are morestable than LSTM, and the Proposed classifier has a straightforwardstructure.4 CONCLUSIONThe model is composed of four networks Word-Embeddings, CNN,Bi-LSTM, and GRU. We may train the model more quickly by usingthe convolutional layer first, followed by the word-embedding layer,and then the BiLSTM network. The Bidirectional LSTM networkalso has higher-level properties that we can extract. We have useda bidirectional LSTM(BiLSTM)and GRU network to memorize asentences contextual meaning and sequential structure, which im-proves the models performance accuracy to roughly 97.32 percent.REFERENCES[1]Rayan Salah Hag Ali and Neamat El Gayar. 2019. Sentiment analysis using unla-beled email data. In 2019 International Conference on Computational Intelligenceand Knowledge Economy (ICCIKE) . IEEE, 328333.[2]Ali Shafigh Aski and Navid Khalilzadeh Sourati. 2016. Proposed efficient algo-rithm to filter spam using machine learning techniques. Pacific Science Review A:Natural Science and Engineering 18, 2 (2016), 145149.[3]Huwaida T Elshoush and Esraa A Dinar. 2019. Using adaboost and stochasticgradient descent (sgd) algorithms with R and orange software for filtering e-mailspam. In 2019 11th Computer Science and Electronic Engineering (CEEC) . IEEE,4146.[4]Weimiao Feng, Jianguo Sun, Liguo Zhang, Cuiling Cao, and Qing Yang. 2016. Asupport vector machine based naive Bayes algorithm for spam filtering. In 2016IEEE 35th International Performance Computing and Communications Conference(IPCCC) . IEEE, 18.[5]Pranjul Garg and Nancy Girdhar. 2021. A Systematic Review on Spam FilteringTechniques based on Natural Language Processing Framework. In 2021 11th Inter-national Conference on Cloud Computing, Data Science & Engineering (Confluence) .IEEE, 3035.[6]Adam Kavon Ghazi-Tehrani and Henry N Pontell. 2021. Phishing evolves: Ana-lyzing the enduring cybercrime. Victims & Offenders 16, 3 (2021), 316342.[7]Radicati Group et al .2015. Email Statistics Report 20152019. Radicati Group.Accessed August 13 (2015), 2019.[8]Maryam Hina, Mohsin Ali, and Javed. 2021. Sefaced: Semantic-based forensicanalysis and classification of e-mail data using deep learning. IEEE Access 9(2021), 9839898411.[9]Maryam Hina, Mohsin Ali, Abdul Rehman Javed, Fahad Ghabban, Liaqat AliKhan, and Zunera Jalil. 2021. Sefaced: Semantic-based forensic analysis andclassification of e-mail data using deep learning. IEEE Access 9 (2021), 9839898411.[10] Weicong Kong, Zhao Yang Dong, Youwei Jia, David J Hill, Yan Xu, and YuanZhang. 2017. Short-term residential load forecasting based on LSTM recurrentneural network. IEEE transactions on smart grid 10, 1 (2017), 841851.[11] T Kumaresan and C Palanisamy. 2017. E-mail spam classification using S-cuckoosearch and support vector machine. International Journal of Bio-Inspired Compu-tation 9, 3 (2017), 142156.[12] Nuha H Marza, Mehdi E Manaa, and Hussein A Lafta. 2021. Classification ofspam emails using deep learning. In 2021 1st Babylon International Conference onInformation Technology and Science (BICITS) . IEEE, 6368.[13] Tomas Mikolov and Geoffrey Zweig. 2012. Context dependent recurrent neuralnetwork language model. In 2012 IEEE Spoken Language Technology Workshop(SLT) . IEEE, 234239.[14] Sarwat Nizamani, Nasrullah Memon, Mathies Glasdam, and Dong Duong Nguyen.2014. Detection of fraudulent emails by employing advanced feature abundance.Egyptian Informatics Journal 15, 3 (2014), 169174.[15] V Priya, I Sumaiya Thaseen, Thippa Reddy Gadekallu, Mohamed K Aboudaif,and Emad Abouel Nasr. 2021. Robust attack detection approach for IIoT usingensemble classifier. arXiv preprint arXiv:2102.01515 (2021).[16] Justinas Rastenis, Simona Ramanauskait e, Justinas Januleviius, Antanas enys,Asta Slotkien e, and Kstutis Pakrijauskas. 2020. E-mail-based phishing attacktaxonomy. Applied Sciences 10, 7 (2020), 2363.[17] Karthika D Renuka and P Visalakshi. 2014. Latent semantic indexing based SVMmodel for email spam classification. (2014).[18] Shuvendu Roy, Sk Imran Hossain, MAH Akhand, and N Siddique. 2018. Sequencemodeling for intelligent typing assistant with Bangla and English keyboard. In2018 International Conference on Innovation in Engineering and Technology (ICIET) .IEEE, 16.[19] Tara N Sainath, Oriol Vinyals, Andrew Senior, and Haim Sak. 2015. Convolu-tional, long short-term memory, fully connected deep neural networks. In 2015IEEE international conference on acoustics, speech and signal processing (ICASSP) .Ieee, 45804584.[20] Anuj Kumar Singh, Shashi Bhushan, and Sonakshi Vij. 2019. Filtering spammessages and mails using fuzzy C means algorithm. In 2019 4th InternationalConference on Internet of Things: Smart Innovation and Usages (IoT-SIU) . IEEE,15.[21] Kristina Toutanova and Colin Cherry. 2009. A global model for joint lemmati-zation and part-of-speech prediction. In Proceedings of the Joint Conference ofthe 47th Annual Meeting of the ACL and the 4th International Joint Conference onNatural Language Processing of the AFNLP . 486494.[22] Tian Xia. 2020. A constant time complexity spam detection algorithm for boostingthroughput on rule-based filtering systems. IEEE Access 8 (2020), 8265382661.[23] Yan Zhang, PengFei Liu, and JingTao Yao. 2019. Three-way email spam filteringwith game-theoretic rough sets. In 2019 International conference on computing,networking and communications (ICNC) . IEEE, 552556.Received 15 April 2023187","date":"2024-02-05T14:33:33.983340","authors":[{"full_name":"Shivangi Sachan","institut_name":"Department of CSE, IIIT Lucknow"},{"full_name":"Khushbu Doulani","institut_name":"Vardhaman College of Engineering, Hyderabad, India"},{"full_name":"Mainak Adhikari","institut_name":"Department of CSE, IIIT Lucknow, UP, India"}],"author":[{"full_name":"Shivangi Sachan","institut_name":"Department of CSE, IIIT Lucknow"},{"full_name":"Khushbu Doulani","institut_name":"Vardhaman College of Engineering, Hyderabad, India"},{"full_name":"Mainak Adhikari","institut_name":"Department of CSE, IIIT Lucknow, UP, India"}]}},{"_index":"article_index","_id":"10","_score":1.0,"_source":{"id":10,"title":"ModelGame: A Quality Model for Gamified Software Modeling Learning","summary":"Gamification has been adopted in software development tasks in recent years. This adoption seeks, for example, to improve the engagement of developers while creating UML models or writing code. Empirical studies report that UML models suffer from incompleteness and inconsistency problems. This study conjectures that gamification mechanics can improve learner engagement while learning software modeling, mitigating such problems concerning UML models. The current literature lacks studies that explore gamification and UML model quality in the context of software modeling learning. This article, therefore, proposes ModelGame, which is a quality model to support software modeling learning in a gamified way. It serves as a reference framework so that instructors can obtain a parameterized way to evaluate UML models created by learners. The quality of UML models can be improved by applying gamified activities and providing guidelines aware of quality issues. A qualitative questionnaire was answered by 19 instructors who teach software modeling at higher education institutions. The results show that (1) 94.7% recognize that the proposed model can improve the quality of UML models, indicating that they would adopt the ModelGame in their learning practices; and (2) 47.4% do not use any gamification mechanics in their classes. The results are encouraging, showing the potential for applying and improving the teaching and learning of software modeling.","keywords":["Model design","learning model","Gamification"],"content":"ModelGame: A Quality Model for Gamified Software ModelingLearningEd Wilson JniorUniversidade do Vale do Rio dos SinosSo Leopoldo, Rio Grande do Sul, Braziledwjr7@edu.unisinos.brKleinner FariasUniversidade do Vale do Rio dos SinosSo Leopoldo, Rio Grande do Sul, Brazilkleinnerfarias@unisinos.brABSTRACTGamification has been adopted in software development tasks inrecent years. This adoption seeks, for example, to improve the en-gagement of developers while creating UML models or writingcode. Empirical studies report that UML models suffer from incom-pleteness and inconsistency problems. This study conjectures thatgamification mechanics can improve learner engagement whilelearning software modeling, mitigating such problems concern-ing UML models. The current literature lacks studies that exploregamification and UML model quality in the context of softwaremodeling learning. This article, therefore, proposes ModelGame ,which is a quality model to support software modeling learning in agamified way. It serves as a reference framework so that instructorscan obtain a parameterized way to evaluate UML models createdby learners. The quality of UML models can be improved by apply-ing gamified activities and providing guidelines aware of qualityissues. A qualitative questionnaire was answered by 19 instructorswho teach software modeling at higher education institutions. Theresults show that (1) 94.7% recognize that the proposed model canimprove the quality of UML models, indicating that they wouldadopt the ModelGame in their learning practices; and (2) 47.4% donot use any gamification mechanics in their classes. The results areencouraging, showing the potential for applying and improvingthe teaching and learning of software modeling.CCS CONCEPTSSoftware and its engineering Software design engineer-ing.KEYWORDSModel design, learning model, GamificationACM Reference Format:Ed Wilson Jnior and Kleinner Farias. 2021. ModelGame: A Quality Modelfor Gamified Software Modeling Learning. In 15th Brazilian Symposiumon Software Components, Architectures, and Reuse (SBCARS 21), September27-October 1, 2021, Joinville, Brazil. ACM, New York, NY, USA, 10 pages.https://doi.org/10.1145/3483899.3483910Permission to make digital or hard copies of all or part of this work for personal orclassroom use is granted without fee provided that copies are not made or distributedfor profit or commercial advantage and that copies bear this notice and the full citationon the first page. Copyrights for components of this work owned by others than ACMmust be honored. Abstracting with credit is permitted. To copy otherwise, or republish,to post on servers or to redistribute to lists, requires prior specific permission and/or afee. Request permissions from permissions@acm.org.SBCARS 21, September 27-October 1, 2021, Joinville, Brazil2021 Association for Computing Machinery.ACM ISBN 978-1-4503-8419-3/21/09. . . $15.00https://doi.org/10.1145/3483899.34839101 INTRODUCTIONGamification has been adopted in software development tasks inrecent years. This adoption seeks, for example, to improve the en-gagement of developers while creating UML models or writingcode. Empirical studies [ 7,9,14] report that UML models sufferfrom incompleteness and inconsistency problems. Lange [ 14] rein-forces that these defects bring potential risks that can cause mis-interpretation and communication failure, representing a risk tosoftware quality. Thus, finding formats that favor student learningand consequently in generating increasingly effective UML mod-els can become one of the main challenges faced by instructorsthat include UML (Unified Modeling Language) as part of softwaremodeling content.Some studies [ 3,12,25] sought to understand how to apply gam-ification in software modeling teaching using some elements suchas points, emblems and levels. However, instructors and researchersstill find limitations when applying, evaluating, and measuring theuse of this tool in the learning of software modeling students and,consequently, in the models developed by them, since in the currentliterature there is no frame of reference that guides them. Thisstudy conjectures that gamification mechanics can improve learnerengagement while learning software modeling, mitigating suchproblems concerning UML models. The current literature lacksstudies that explore gamification and model quality in the contextof software modeling learning.This article, therefore, introduces ModelGame , which is a qualitymodel to support software modeling learning in a gamified way.It serves as a reference framework so that instructors can obtaina parameterized way to evaluate UML models created by learners.The quality of UML models can be improved by applying gami-fied activities and providing guidelines aware of quality issues. Areference framework would help to (1) establish parameters for eval-uating UML models created by learners; (2) provide guidelines toimprove the quality of these artifacts; (3) to analyze which elementsof gamification could be included in each of the phases of modelingusing UML; (4) identify intrinsic and extrinsic aspects of studentsduring the modeling stages, to improve the models; (5) to comparevalidated theories about the inclusion of gamification in softwaremodeling teaching, taking into account the types of learning andmethodologies used; and (6) contributing to the identification ofgamification use objectives in modeling activities.A qualitative questionnaire was answered by 19 instructors whoteach software modeling at higher education institutions. The re-sults show that (1) 94.7% recognize that the proposed model canimprove the quality of UML models, indicating that they wouldadopt it in their learning practices; and (2) 47.4% do not use any100SBCARS 21, September 27-October 1, 2021, Joinville, Brazil Ed Wilson Jnior and Kleinner Fariasgamification mechanics in their classes. These results are encourag-ing, showing the potential for applying and improving the teachingand learning of software modeling.The remainder of the paper is organized as follows. Section 2presents the main concepts discussed throughout the article. Sec-tion 3 discusses the related work, highlighting research opportu-nities. Section 4 introduces the proposed quality model. Section 5presents how the quality model was evaluated. Section 6 points outsome threats to validity. Finally, Section 7 presents some concludingremarks and future work.2 BACKGROUNDThis section presents the essential concepts for understanding thiswork, including gamification and software engineering teaching(Section 2.1), and software modeling and model quality (Section2.2).2.1 Gamification and Software EngineeringTeachingGamification aims to use game elements in the context of not game[5], bringing all positive aspects they provide as a way to encourageand engage players, thereby broadening their motivations.Werbach [ 23] classifies gamification into three dimensions: Dy-namics, Mechanics, and Components. Dynamics include all gameaspects related to the emotional responses of players (e.g., rela-tionship, progression, and narrative). Mechanics offer elements thatpromote the action of a game  usually elaborated via a rule-baseddevelopment , so that the player can interact with such elements,e.g., challenges, feedback, and rewards. Components representthe aesthetic elements of gamification, whose goal is to presentvisual aspects with which players can perform the interaction, forexample, points, scores, and emblems (badges).Knowing that the teaching of Software Engineering should in-volve students to experience the professional practices of the areaso that they can understand which practices and techniques areuseful in several different situations [ 2]. The challenges of teachingnew software engineers are not limited to learning programming,but also include paying attention to detail, considering the qualityof created models, established schedule and defined budgets [ 1]. Inaddition to understanding the technical challenges, these futureprofessionals must be up to date with nontechnical issues, includingteamwork, communication and management.To meet these new demands of the current context, the formatwith exhibition classes is no longer considered enough and mayeven become demotivating and ineffective in learning students. Inthis sense, gamification has been increasingly used in the teach-ing of software engineering as a way to promote behavioral andpsychological changes [ 11] providing an environment that favorscommunication, cooperation, feedback, reward, achievement andother recurring elements that are capable of improving perfor-mance, efficiency and engagement in educational activities , andcan enhance, for example, the learning of software modeling.2.2 Software Modeling and Model QualitySoftware modeling encompasses the set of principles, concepts, andpractices that lead to the development of a high-quality system orproduct. The principles of this activity establish a philosophy thatguides the entire software development process.In this scenario, UML models play a crucial role in softwaredevelopment tasks, for example, documenting project decisions,understanding development details, promoting better communica-tion between teams, and generating greater efficiency in softwaredevelopment [ 19]. However, these models suffer problems of in-consistency and incompleteness [ 10,18], as well as end up beingoverlooked within the modeling process, as pointed out in someempirical studies in the literature [ 14,15]. Class and sequence dia-grams, for example, present inconsistencies when sequence diagramobjects are not found in the class diagram, consequently developersend up living with inconsistencies throughout the developmentprocess.A research challenge still open is how to evaluate these diagrams,both in industry and in the teaching process, in terms of quality,such as syntactic and semantic, for example.3 RELATED WORKThe selection of related works was carried out following two steps:(1) search in digital repositories, such as Google Scholar andScopus(Elsevier) of articles related to gamification, quality modeling, andmodeling learning; and (2) filter selected articles considering thealignment of such works with the objective of the work (Section 4).After selecting the works, they were analyzed (Section 3.1) andcompared (Section 3.2), seeking to identify research opportunities.3.1 Analysis of Related WorksPorto et al. (2021) [4]. This work performed a systematic map-ping with the objective of characterizing how gamification hasbeen adopted in noneducational contexts of software engineeringactivities. The main results of this study show that gamificationprovided benefits for activities such as requirements specification,development, testing, project management, and support process.In addition, he pointed out that the number of publications andnew research initiatives has increased over the years, many posi-tive results have been achieved in software engineering activities.Nevertheless, the study reinforced that gamification can still beexplored for other tasks in this area, as empirical evidence is verylimited.Marin (2021) [17]. It performed the application of gamificationon some topics of a software engineering course to engage studentsand increase their motivation and argued that, with due motiva-tion, students can better exercise the topics and obtain more solidknowledge. There were five games related to risk management,BPMN modeling, Scrum process, design and inspection of classdiagrams, and cosmic functional size measurement to assist in thelearning process of the software engineering course. This study alsopresented the lessons learned about the application of gamificationand serious games in software engineering, including limitationsor disadvantages.Jurgelaitis et al. (2018) [12]. This work conducted a research toinvestigate how gamification could be inserted into an InformationSystems Modeling course, which covers a range of topics on UML.As a result, an implementation of the gamified system modelingcourse in the Moodle environment was presented, using additional101ModelGame: A Quality Model for Gamified Software Modeling Learning SBCARS 21, September 27-October 1, 2021, Joinville, Brazilplugins for the use of the necessary gamified elements. The studyshowed good results and obtained a positive acceptance by theparticipating students.Rodrigues et al. (2018) [22]. They investigated the use of gamesand game elements in software engineering education, through aresearch that had the participation of 88 instructors of this disci-pline. The results showed that most instructors are aware of theseeducational approaches, however, the games were adopted by only21 participants and game elements were adopted only by 19. Gamesare most often used to cover Software Process and Project Man-agement. The most commonly used game elements are points,quizzes, and challenges. The results also show that the main rea-sons for not adopting the resources are the lack of knowledge,information about games relevant to the engineering of teachingsoftware, and the lack of time to plan and include these approachesin the classroom.Cosentino et al. (2017) [3]. They present a model-based ap-proach to learning modeling in a gamified way. The approach in-cludes a new language to model the gamification process itself andan environment where it can be incorporated into current mod-eling tools to allow instructors and students to design and use acomplete modeling framework, including gamification elements. Inaddition, the approach also had as a proposal to provide support tocollect and analyze gamification data, thus facilitating monitoringactivities.Yohannis (2016) [25]. This research presents an exploration ofgame design as an approach to strengthening the students mas-tery in software modeling by developing their abstraction skills. Itbrought together concepts of gamification development, such as thelens of atoms of intrinsic skill and principles of pedagogical designof various theories and models of learning. The research followsthe Design Science Research Methodology and explores the bestpractices of Model Oriented Engineering. As a result, a modelinggame design framework and generation structure and a series ofproduced games are presented.Pedreira et al. (2015) [21]. They developed a systematic map-ping of gamification in Software Engineering based on 29 studies.The mapping revealed that software implementation is the areain which most studies focus, followed by software requirements,few others in different areas, such as project planning and softwaretesting, and even to a lesser extent in activities involving softwaremodeling. However, the highlight of this work was to highlight thatgamification in software engineering is still at a very early stageand the evidence on its impact in this field remains inconclusive.3.2 Comparative Analysis and OpportunitiesFive Comparison Criteria (CC) were defined selecting the most rele-vant variables to assist in the process of identifying similarities anddifferences between the proposed work and the selected articles.This comparison is crucial to make the process of identifying re-search opportunities using objective rather than subjective criteria.The criteria are described below:Context (CC01): Works that explore the use of gamificationin software modeling teaching/learning.Participant profile (CC02): Studies that collected data fromparticipants for screening and profile characterization.Applicability of Gamification in UML (CC03): Studiesthat evaluated how gamification can contribute to UML mod-els.Model creation (CC04): Studies that have developed a modelto improve factors that imply the non-adoption of UML.Instructor participation (CC05): Studies that collectedqualitative data through the participation of software mod-eling instructors.Table 1 shows the comparison of the selected works, confrontingthis work. Some gaps and research opportunities are observed: (1)only the proposed work was the only one to fully meet all compar-ison criteria; (2) although most of them targeted the applicationof gamification in software modeling teaching, they were not di-rected to the use of UML; (3) no study has developed a model toevaluate the learning and improvement of UML models developedby students; and (4) most of them did not have the participationof instructors to identify the difficulties and opportunities in theapplication of gamification in the teaching of software modeling.Thus, the next Section presents a quality model to explore theseidentified opportunities.Related WorkComparison CriterionCC1 CC2 CC3 CC4 CC5Proposed Work      Porto et al (2021) [4] ## ##Marin (2021) [17]  #G #G ##Jurgelaitis et al (2018) [12]  #  #Rodrigues et al (2018) [22]   G ## Cosentino et al (2017) [3]  #  #Yohannis (2016) [25]  #G #G ##Pedreira et al (2015) [21] ##G ### Completely Meets G #Partially Meets #Does not attendTable 1: Comparative analysis of the selected related works4 PROPOSED QUALITY MODELThis section presents the proposed quality model to support soft-ware modeling learning in a gamified way. It serves as a frameof reference so that instructors can evaluate the UML models cre-ated by students through gamified activities. Section 4.1 presentsa proposal of a generic analytical framework. Section 4.2 detailsthe abstract syntax of the proposed quality model. Section 4.3 ex-plains the quality notions related to the gamified software modelinglearning.4.1 Generic Analytical FrameworkFigure 1 presents the generic analytical framework for improvingthe quality of the models and serves as the basis for the creation ofan evaluation scheme. The arrows (\"links\"), labeled as Evaluationand Gamified Modeling, represent the questions that the evidencemust answer; dotted lines represent associations; rectangles rep-resent the Models (rounded corners) or the quality states (squarecorners) by which these bindings are measured. Ellipses representthe adverse effects that can be generated from the evaluation anduse of gamification.The numbers refer to the key questions and are connected withthe concepts and relationships of the abstract syntax of the Quality102SBCARS 21, September 27-October 1, 2021, Joinville, Brazil Ed Wilson Jnior and Kleinner FariasFigure 1: Generic analytical framework for gamified software modeling learning.Model (presented in Section 4.2), as follows: (1) Are there toolsthat assist instructors in evaluating the models developed by stu-dents, thus reducing the poor quality and incompleteness of theseartifacts? (2) What is the prevalence of characteristics that causemodels to be at risk? (3) Are there notions of quality to evaluatethe models as a way to define parameters when performing theircorrection? (4) Applying the use of gamification in models thatneed intervention would be a way to identify factors that couldgenerate models with high quality levels? (5) Does the applicationof gamification improve the quality of the model? (5.a) How are themodels without gamification evaluated in relation to those withgamification? (5.b) Are there reasons to expect that gamificationmodels can have better quality results than those that are gener-ated without gamification? (6) Is the output model really effectivewhen associated with reducing the poor quality of the model? (7)Does the absence of evaluation result in adverse effects? (7.a) is theevaluation acceptable for the model? (7.b) What are the potentialharms, and how often do they occur? (8) Does gamification resultin adverse effects on models?Fact is that it is not enough just to include this \"toolbox\" in theUML learning process, it is necessary to provide the instructor witha model (guide) that can serve as a reference to evaluate the qualityof diagrams elaborated through gamified activities. For example,the instructor could create models predefining inconsistencies bymaking use of these questions raised to evaluate the models createdby the students. The set of questions serves as the starting point forthis evaluation. Knowing that the adaptation of the gamificationapproach requires a significant effort [ 20], in this study we presentThe ModelGame as a way to identify factors that contribute tothe quality of these artifacts and, consequently, to the studentslearning.4.2 Abstract SyntaxFollowing the specification pattern of the UML metamodel, Figure 2presents the abstract syntax of the proposed Quality Model forgamified software modeling learning (ModelGame). It identifiesthe main concepts and relationships. The numbers represent thenotions of quality that are discussed in Section 4.3. The followingare detailed each of these concepts and relationships.Domain. The first concept presented in this study is the domain,which corresponds to a specific context of the application to bedeveloped to solve the problem. In this process, the design templaterepresents the solution given to the domain.Associationcontextualizes : Challenges[*]Each contextualise refers to the domain that will serve as thebasis for the challenges launched.Challenges. This concept represents the phase in which theproblem is contextualized (domain-based), as well as what will bethe missions, phases, scenarios, and other elements presented tothe players, in this case the students, who must use the principlesof software engineering to perform the modeling and reach thefinal goal.Associationinfluences : Design Model[*]Each influence represents that the proposed challenge interferedin aspects of the design model, causing the user to seek to make acontinuous improvement.Modeling Language. Software modeling is an important stepfor development to happen in a way that adheres to the require-ments established by the requester, for this, there is the modelinglanguage, which offers a standardized way to document and designsoftware. Through the use of modeling languages, it is possible toachieve a high level of understanding about the software in ques-tion, improving the communication between all those involved inthe process, thus avoiding implementation errors. It points out thatsoftware engineers use these languages to communicate designdecisions and verify the feasibility of implementing the intendeddesign. The UML was consolidated as the Modeling Language inthe paradigm of object orientation, in which it is possible throughvisual notation generated from the diagrams- presented later in thisstudy as Design Models- to perform the representation of variousperspectives of the system.Association103ModelGame: A Quality Model for Gamified Software Modeling Learning SBCARS 21, September 27-October 1, 2021, Joinville, BrazilFigure 2: Abstract Quality Model Syntax.expresses : Design Model[*]Performs the representation of the intended design templates, inwhich the Modeling Language should be applicable to the domaintype.User. This concept corresponds to the individual who performsthe interpretation of the developed design models, whose objectiveis to be able to understand the domain in question. In the gamifiedcontext, the user has the role of player and it is he who performsthe whole process, being able to perform the interpretation of ex-isting models or even creating new ones. The user can also identifyand resolve inconsistencies that arise from compositions betweenmodels.Associationcreates : Design Model[1..*]Represents the process in which the user creates a design template,which can be one or more.interprets : Design Model[1..*]In this association, the user performs the interpretation of the designtemplate. When interpreting the model, paths for the resolution ofinconsistencies can be identified.detects : Inconsistency [*]Represents the users discovery of design model inconsistencies,for example, those that are generated from identifying conflicts,whether a class is abstract or not.resolves : Inconsistency [*]Each resolves equates to the resolution representation of the incon-sistencies by the user that happens after he analyzes and determinesthe best alternative to perform this action.uses : Modeling Tools [*]Determines that the user can use modeling tools to generate/updatedesign models.AssociationWithout a directed relationship .Modeling Tool. This concept represents the applications thatare used to carry out the construction of design models. There areseveral tools available, online and desktop, and it is up to the userto choose the one that will best meet their needs and adapt to thecontext in question, that is, they work in any domain that is beingconsidered.Design Model. The design model refers to a visual notation(diagram) to represent static and dynamic aspects. These models arebuilt according to a specific objective or task and tend to facilitatethe logical interpretation of the software in several aspects. Themost popular diagrams are Use Cases and Classes, the first beingstatic and representing a set of actions generated from functionalrequirements (use cases) and presenting the interactions generatedwith external users (actors). The second is a static diagram andmakes the representation of the logical structure of the softwareinvolving the classes, their attributes, methods, and relationshipsbetween them [19].Associationdescribes : Domain[1]Each describes makes the representation of a specific domainand means that every design model must describe it.Inconsistency. It corresponds to the defects found in the modelsdeveloped by users. They may occur because of the nonidentifi-cation and correction of possible conflicts and even an erroneousinterpretation.Associationaffects : Design Model[*]This association indicates that with each occurrence of the affect, aproblem is presented harming the quality of the design model.104SBCARS 21, September 27-October 1, 2021, Joinville, Brazil Ed Wilson Jnior and Kleinner FariasPoints. This concept represents one of the most used game me-chanics in software engineering and functions as a quantitativereward for each action developed, in which it is possible to regulatethe number of rewarded points of the player, defined here as user,based on the importance of each action. Through this concept, itis possible to stimulate competition, collaboration, and creativityamong users, stimulating learning. Points appear as a derivationof the association affects, since when each inconsistency error isidentified or not, the user will receive a score and the associationdescribes, because the points will also be applied when makingconnections between the model and the domain.Progress. The concept of progress emerges as a factor that makesthe user able to perceive its evolution in the process, in this case,software modeling. Progress emerges as a derivation of the associa-tion interprets, making the user know when they have performeda correct interpretation of the proposed design model or what stillneeds to be improved.Feedback. Feedback has the role of making the user realize thatthe proposed goal can be achieved and follow its evolution, includ-ing analyzing how to change or creating new strategies to achievethe goal. This concept emerges as a derivation between the associa-tions it creates, causing the user to receive a return to the modelcreation process.4.3 Quality NotionsAs discussed in Section 2, gamification can bring important ele-ments for learning software modeling and, therefore, the objectiveof this section is to produce the notions of quality of the modelof this study. The ModelGame is composed of ten counts, four ofwhich are proposed in this study - scope, use, motivational and en-gagement - extracted from the main benefits that the gamificationelements presented in Figure 2 can bring to the models. The othersare adaptations of previous works [ 6,14,15], they are, syntactic,semantic, social, effort, detection and resolution.Scope Quality (1). It seeks to determine how much the proposedchallenge is contextualized with the design model, as well as the def-inition of the domain, problem, competencies, concepts, behaviorsand attitudes that will be developed throughout the process.Syntactic Quality (2). This notion makes the representation ofthe process of correction of the design models that are producedby the modeling language, because if it is not used correctly, incon-sistencies will arise. It is important to insert this notion of qualityinto our study, since during the process of developing the models,users may come across the composition of two class diagrams, forexample.Semantic Quality (3). It is necessary to verify that the designmodel and the problem domain match, so this notion performsthis type of analysis. Communication problems may occur betweenusers if the semantic elements of the model are affected.Social Quality (4). Design models are used to communicatebetween members of a team to inform all established decisionsabout software development [ 8]. If divergent interpretations occur,this communication will be greatly impaired.Quality of Effort (5). This notion refers to the production chal-lenges of the model that will be generated, including factors suchas time and cost.Quality of Use (6). To produce design templates, users can useunusual tools such as paper, whiteboard, and more. However, mostof the time they choose to use formal tools (CASES) and can beonline or desktop. This notion corresponds to the level of ease andapplicability of the models elaborated when making use of thesetools, it is also important to contribute to communication betweenusers through collaboration-related functionalities.Detection Quality (7). This notion is referenced to the processof locating inconsistencies, since when users arise, they shouldperform traceability of them quickly. If the detection is complicated,it could hinder the process of correcting the models.Resolution Quality (8). It corresponds to the level of qualityrelated to the effort that users take to look for alternatives to solvethe identified problem.Motivational Quality (9). This notion refers to the motiva-tional factors involved during the learning and development ofdesign models, which can be intrinsic and extrinsic. Elements ofgamification such as points, feedback and progress bring the user adegree of satisfaction in continuing their discovery and transfor-mations throughout the process.Quality of Engagement (10). The user in tracking their progresscan feel committed to the objective in question, and this notionrepresents the measurement of the level of commitment of themduring the development of design models.5 EVALUATIONThis section describes the methodology followed to evaluate theproposed quality model. This methodology follows well-establishedempirical guidelines [ 24]. Section 5.1 details the objective and re-search questions (RQ). Section 5.2 presents the questionnaire formu-lated to evaluate the proposed quality model. Section 5.3 explainsthe context and selection of participants. Section 5.4 describes thepresentation of the Model. Section 5.5 presents the analysis of thecollected data.5.1 Objective and Research QuestionsThe objective (O)of this study is twofold: (O1) Introduce Model-Game as a tool for teaching Software Modeling; and (O2) Analyzethe applicability of the quality model regarding the improvementof UML models.To analyze the different facets of the objectives, two ResearchQuestions (RQ) have been formulated:RQ1: How do instructors evaluate the use of gamificationin software modeling?RQ2: What is the acceptance of ModelGame by softwaremodeling instructors?5.2 QuestionnaireData was collected through an online questionnaire created throughGoogle Forms1following well-established guidelines described in[24]. This strategy was chosen because the questionnaire couldbe applied quickly and easily collect data from individuals in geo-graphically diverse locations. The questions of the questionnaire1Questionnaire: https://forms.gle/qjaFDpErEtGdLuWw6105ModelGame: A Quality Model for Gamified Software Modeling Learning SBCARS 21, September 27-October 1, 2021, Joinville, Brazilwere concerned with examining the research gaps of previous stud-ies and apprehending the structures of the previously developedquestionnaire.Part 1: Participant profile. The first part of the questionnaireconsisted of collecting data that are related to the characteristicsand opinions of the participants. The creation of the participantprofile through this data is important to make the selection ofpossible users of ModelGame. Without this profile, participantswith an inadequate profile may generate inconsistent assessments.Participants were asked to provide more general information, suchas age, education level, academic background. Information aboutthe time of experience in teaching was also considered, includingteaching software modeling and level of knowledge about UMLmodels.Part 2: TAM questionnaire. The second part addressed ques-tions about the usability and acceptance of the technique, aimingto explore q3. To this end, this part of our questionnaire is based onthe technology acceptance model (TAM) [ 16]. This part containednine questions, which were answered through the Likert Scale, in-cluding Totally Agree, Partially Agree, Neutral, Partially Disagree,and Totally Disagree. The questions formulated (Q) dealt with sev-eral topics, including perceived ease of use (Q1-3), perceived utility(Q4-7), attitude towards use (Q8), and behavioral intention to use(Q9).5.3 Selection of participantsThe participants were selected based on the following criteria: in-structors and/or professionals working in the teaching of softwaremodeling in higher education institutions in Brazil. Using this cri-terion, we sought to select participants with academic trainingand practical experience in teaching. This finite set of all possibleparticipants represents the target population [ 13]. This popula-tion represents those people who are in a position to answer thequestions formulated and to whom the results of the survey apply[13]. In all, 19 people (n) answered the questionnaire. The partici-pants were invited via e-mail to participate in the study and eachof them previously received the explanation/training about themodel proposed through the researcher and there was no doubt,they could leave for the next step that consisted of completing theTAM questionnaire. We discussed the experimental process in thenext section.5.4 Experimental ProcessFigure 3 presents the experimental process used in this study, whichis composed of three phases discussed below:Phase 1: Presentation. It has an activity, presentation , in whichthe researcher explained to the participants through a video detailabout the quality model. This process took place individually andin a standard way, where space was also made available for par-ticipants to answer possible doubts about the proposed study andmodel, lasting an average of 20 minutes.Phase 2: Application of the TAM questionnaire. It has twoactivities, the first being Collect demographic data . The participantsanswered a list of questions (input) so that we could collect theircharacteristics and opinions about the ModelGame. The demo-graphic data collected (output) became the result of this activity.The second activity Apply TAM questionnaire (input) . Participantsreceived a list of questions about the perception of ease of use, per-ceived utility, attitudes, and intention of behavior, in relation to theModelGame. Qualitative data (output) were generated, regardingthe usability and acceptance of the Model under the perspectiveof professionals who teach software modeling. This questionnairefollowed the guidelines of the TAM [16].Phase 3: Analysis and result report. It has two activities. Thefirst, Analyze data sought to perform a thorough analysis of the datacollected through the questionnaire and the researchers perceptionregarding the participants doubts during the presentation stage.For this, the collected data were analyzed separately, as well as con-fronted, aiming to perform a triangulation of them. Subsequently,there was an Evaluation data , as a way to understand in a moredepth the context, the perceptions of the participants in relation tothe proposed model as well as its applicability.Figure 3: The experimental process.5.5 Result Analysis5.5.1 Profile data of the participants. Table 3 describes the profiledata, reporting the characteristics and opinions of the participants.These data were collected from May 18 to June 5, 2021. In total,we had 19 participants. Our participants are between 20 and 49years old, most of them have a degree in Computer Science (52.6%),Information Systems (26.3%) or Systems Analysis (21.1%) and arespecialists (36.8%), masters (36.8%) and doctors (15.8%). About theworking time in teaching, the majority (42.1%) they have been teach-ing for more than 8 years and teach disciplines related to softwaremodeling, including software engineering, systems analysis andsoftware projects. A total of 47.4% have a full level of knowledgeabout UML and almost half of them (47.4%) has not yet used gamifi-cation in the teaching of software modeling. Therefore, we consider106SBCARS 21, September 27-October 1, 2021, Joinville, Brazil Ed Wilson Jnior and Kleinner FariasTotally agree Partially agree Neutral Partially disagree Totally disagreePerceived ease of useI found the quality model easy to use 8 9 2 0 0I found the quality model easy to learn 10 9 0 0 0I found the quality model easy to master 6 12 0 1 0Perceived usefulnessThe model would make it easier to understand which elements of gamification can be used in modeling . 12 5 2 0 0Using the quality model would help increase productivity. 9 8 2 0 0The model would provide an understanding of how to mitigate the incompleteness of UML diagrams. 5 8 5 1 0The model would help compare theories about gamification in software modeling teaching. 13 4 2Attitude towards useUsing the Quality Model for Gamified Software Modeling Learning is a good idea. 13 5 1 0 0Behavioral intention to useI would use the quality model in software modeling classes. 10 7 2 0 0Table 2: Collected data related to TAM questionnaire.that although small, our sample is adequate to carry out an initialevaluation of the proposed approach.Characteristic and Opinion (n=19) Answer # %Age < 20 years 0 0.0%20-29 years 4 21.1%30-39 years 8 42.1%40-49 years 5 26.3%> 49 years 2 10.5%Education Undergraduate* 0 0.0%Specialization* 7 36.8%Master* 7 36.8%PhD* 3 15.8%Others 2 10.6%Undergraduate course Information Systems 5 26.3%Computer Science 10 52.6%Computer Engineering 0 0.0%System Analysis 4 21,1%Others 0 0.0%Time of experience in teaching < 2 years 4 21.1%2-4 years 2 10.5%5-6 years 3 15.8%7-8 years 2 10.5%> 8 years 8 42.1%Experience in teaching < 2 years 3 15.8%software modeling 2-4 years 5 26.3%5-6 years 3 15.8%7-8 years 2 10.5%> 8 years 6 31,6%Level of knowledge Beginner 2 10.5%about UML models Junior 5 26.3%Full 9 47.4%Senior 3 15.8%Used gamification in teaching Yes 9 47.4%No 9 47,4%Maybe 1 5.3%Gamification can contribute to the quality Totally agree 10 52.6%of the models of UML diagrams generated Partially agree 8 42.1%by students Neutral 1 5.3%Partially disagree 0 0.0%Totally disagree 0 0.0%Table 3: The profile data of the participants.5.5.2 RQ1: How do instructors evaluate the use of gamification insoftware modeling? Table 3 presents the collected data related tothe RQ formulated. First, we begin the analysis by verifying howinstructors visualize gamification in software modeling teaching.Although most of them (47.4%) have not yet used gamificationelements (scores, challenge, emblem, among others) in their classes,most (52.6%) totally agree and (42.1%) partially agree that the useof these can contribute to the quality of the models developed bythe students.We consider the percentage of instructors who have not yetused gamification in their classes to be high and this may be tiedto factors such as lack of knowledge, information about the tool,and even time to plan and include these approaches [ 22]. Althoughthey were based on software modeling teaching context, previousstudies [ 3,4,12,17,25] they did not count on the participation ofinstructors and we understand that this participation is fundamentalto understand the perceptions of these professionals since they willbe at the forefront of the use of gamification.The ModelGame proposed in this study could help them insertgamification into their classes, according to the software modelinglearning design [ 25], based on the assumption that for this, it isnecessary to develop a better understanding of the tasks, activities,skills and operations that the different elements of gamification canoffer and how they can correspond to the desired learning outcomesby developing a more concrete and motivating presentation thatcan involve students and facilitate deep learning with UML.5.5.3 RQ2: What is the acceptance of the ModelGame by softwaremodeling instructors? Using the TAM questionnaire, we tried toevaluate the ease of use, perceived usefulness, attitude, and behav-ioral intention to use the Quality Model. Table 2 shows the dataobtained. Our data obtained show that no one disagreed that theModelGame is easy to use, learn, and master. On the contrary, al-most 90% of participants find the model easy to use (42.1% totallyagree and 47.4% partially agrees and 10.5% neutral), learn (52.6%107ModelGame: A Quality Model for Gamified Software Modeling Learning SBCARS 21, September 27-October 1, 2021, Joinville, Brazilfully agree and 47.4% partially agree) and master (31.6% fully agree,63.2% partially agree and 5.3% partially disagree).The results are also favorable considering the perception of util-ity. Most participants realized that the ModelGame would make iteasier to understand which elements of gamification can be used ineach of the phases of modeling using UML(63.3% totally agree, 26.3%partially agree and 10.5% neutral), increase productivity (47.4% fullyagree, 42.1% partially agree and 10.5% neutral), and the use of thequality model would provide an understanding of how to mitigatethe incompleteness of UML diagrams (26.3% agree totalmen 42.1%partially agree, 26.3% neutral and 5.3% partially disagree). Still inthe useful aspect, we tried to know if the quality model would helpto compare validated theories about the inclusion of gamificationin software modeling teaching (68.4% totally agree, 21.1% partiallyagree and 10.5% neutral).Considering the attitude towards use, participants believe thatusing the ModelGame is a good idea (68.4% totally agree, 26.3%partially agree and 5.3% neutral), just as they are confident andwould use the Model in software modeling classes (52.6% totallyagree, 36.8% partially agree and 10.5% neutral). These findings showthe potential for acceptance by people with profiles similar to thoseof participants. The results are encouraging and show the potentialto use the proposed approach in the educational scenario.6 THREATS TO VALIDITYThis section discusses the possible threats to the validity of thestudy.Internal validity. The main point affecting the internal validityof our study concerns the total time used for the exploratory phase.To mitigate this threat, we performed the video recording of a pilotexplaining the operating details and objectives of the ModelGame.In relation to the methods used, the threats related to internalvalidity relate to how we extract the perceptions of the discussionsand whether they represent the perceptions of teachers about theuse of the Model. We try to reduce this threat by applying the TAMquestionnaire.External validity. We identified threats related to external va-lidity, such as the number of participants who never applied the useof gamification. This study was limited to 19 participants (teachers)from various educational institutions, of which 9 (47.4%) never usedany element of gamification in their classes, this factor can interferein the data, since the model intends to evaluate the quality of UMLdiagrams from gamified activities.Conclusion validity. Threats related to the validity of the con-clusion are related to treatment and outcome. We try to make thereduction by combining quantitative and qualitative data throughdifferent resources. These data were obtained through audio andquestionnaires. We analyze this data to answer the research ques-tions.7 CONCLUSIONS AND FUTURE WORKThis study proposed an initial quality model (ModelGame) thatserves as a reference framework for instructors for qualitative eval-uations of UML models developed from gamified activities, theapplication of an empirical study with 19 participants was carriedout to understand their vision in relation to gamification and theacceptance of the proposed Model. It was identified that most havenot yet used gamification in their classes, but agree that their usecan contribute to the quality of the models developed by the stu-dents and were open to using the model. Our findings can enhancethe adoption of new teaching practices through gamification, result-ing in the improvement of software modeling learning using UML,and consequently the creation of models developed by students.These approaches can stimulate students immersion in the designof systems as future professionals during learning.Finally, we hope to carry out in the future a series of experimentalstudies to analyze each stage of application of the ModelGameand that this work represents a first step to better support theapplication of empirical studies on models of evaluation of theuse of gamification in software modeling. We also hope that thequestions described throughout the article will encourage otherresearchers to extend our study to different modeling languagesand teaching methodologies.REFERENCES[1]Rick Adcock, Edward Alef, Bruce Amato, Mark Ardis, Larry Bernstein, BarryBoehm, Pierre Bourque, John Brackett, Murray Cantor, Lillian Cassel, et al .2009.Curriculum guidelines for graduate degree programs in software engineering . ACM.[2]Mark Ardis, David Budgen, Gregory W Hislop, Jeff Offutt, Mark Sebern, andWillem Visser. 2015. SE 2014: Curriculum guidelines for undergraduate degreeprograms in software engineering. Computer 48, 11 (2015), 106109.[3]Valerio Cosentino, Sbastien Grard, and Jordi Cabot Sagrera. 2017. A model-based approach to gamify the learning of modeling. CEUR Workshop Proceed-ings.[4]Daniel de Paula Porto, Gabriela Martins de Jesus, Fabiano Cutigi Ferrari, andSandra Camargo Pinto Ferraz Fabbri. 2021. Initiatives and challenges of usinggamification in software engineering: A Systematic Mapping. Journal of Systemsand Software 173 (2021), 110870.[5]Sebastian Deterding, Miguel Sicart, Lennart Nacke, Kenton OHara, and DanDixon. 2011. Gamification. using game-design elements in non-gaming contexts.InCHI11 extended abstracts on human factors in computing systems . 24252428.[6]Ana Fernndez-Saez et al. 2012. A systematic literature review on the quality ofUML models. J. Data. Manage 22, 3 (2012), 4670.[7]Kleinner Farias et al. 2012. Evaluating the impact of aspects on inconsistencydetection effort: a controlled experiment. In International Conference on ModelDriven Engineering Languages and Systems . Springer, 219234.[8]Kleinner Frias et al. 2014. Towards a quality model for model composition effort.In29th Annual ACM Symposium on Applied Computing . 11811183.[9]Kleinner Farias et al. 2015. Evaluating the effort of composing design models: acontrolled experiment. Software & Systems Modeling 14, 4 (2015), 13491365.[10] Kleinner Farias et al. 2019. UML2Merge: a UML extension for model merging.IET Software 13, 6 (2019), 575586.[11] Juho Hamari, Jonna Koivisto, and Harri Sarsa. 2014. Does gamification work?a literature review of empirical studies on gamification. In 2014 47th Hawaiiinternational conference on system sciences . Ieee, 30253034.[12] Mantas Jurgelaitis, Vaidotas Drungilas, and Lina eponien e. 2018. GamifiedMoodle course for teaching UML. Baltic journal of modern computing 6, 2 (2018),119127.[13] Barbara A Kitchenham and Shari L Pfleeger. 2008. Personal opinion surveys. InGuide to advanced empirical software engineering . Springer, 6392.[14] Christian Franz Josef Lange. 2007. Assessing and Improving the Quality ofModeling: A series of Empirical Studies about the UML. (2007).[15] Odd Ivar Lindland, Guttorm Sindre, and Arne Solvberg. 1994. Understandingquality in conceptual modeling. IEEE software 11, 2 (1994), 4249.[16] Nikola Maranguni and Andrina Grani. 2015. Technology acceptance model: aliterature review from 1986 to 2013. Universal access in the information society14, 1 (2015), 8195.108SBCARS 21, September 27-October 1, 2021, Joinville, Brazil Ed Wilson Jnior and Kleinner Farias[17] Beatriz Marn. 2021. Lessons Learned About Gamification in Software Engineer-ing Education. In Latin American Women and Research Contributions to the ITField . IGI Global, 174197.[18] Kleinner Oliveira, Alessandro Garcia, and Jon Whittle. 2008. On the quantitativeassessment of class model compositions: An exploratory study. 1th ESMDE atMODELS (2008).[19] OMG. 2017. UML: Infrastructure specification.https://www.omg.org/spec/UML/2.5.1/PDF.[20] Sofia Ouhbi and Nuno Pombo. 2020. Software Engineering Education: Challengesand Perspectives. In IEEE Global Engineering Education Conference . 202209.[21] Oscar Pedreira, Flix Garca, Nieves Brisaboa, and Mario Piattini. 2015. Gamifica-tion in software engineeringA systematic mapping. Information and softwaretechnology 57 (2015), 157168.[22] Pedro Rodrigues, Mauricio Souza, and Eduardo Figueiredo. 2018. Games andgamification in software engineering education: A survey with educators. In 2018IEEE Frontiers in Education Conference (FIE) . IEEE, 19.[23] Kevin Werbach and Dan Hunter. 2012. For the win: How game thinking canrevolutionize your business . Wharton digital press.[24] Claes Wohlin, Per Runeson, Martin Hst, Magnus C Ohlsson, Bjrn Regnell, andAnders Wessln. 2012. Experimentation in software engineering . Springer Science& Business Media.[25] Alfa Yohannis. 2016. Gamification of Software Modelling Learning.. In DS@MoDELS .109","date":"2021-09-27","authors":[{"full_name":"Ed Wilson Júnior","institut_name":"Universidade do Vale do Rio dos Sinos"},{"full_name":"Kleinner Farias","institut_name":"Universidade do Vale do Rio dos Sinos"}],"author":[{"full_name":"Ed Wilson Júnior","institut_name":"Universidade do Vale do Rio dos Sinos"},{"full_name":"Kleinner Farias","institut_name":"Universidade do Vale do Rio dos Sinos"}]}},{"_index":"article_index","_id":"6","_score":1.0,"_source":{"id":6,"title":"Fuzzy logic based MPPT control for a PV system using SEPIC converter","summary":"In this study, a novel single-ended primary inductor (SEPIC) converter-based fuzzy logic controller for maximum power point tracking is presented. By adding rules to the perturb and observing search strategy, the new controller enhances it while fuzzifying and removing its flaws. When compared to traditional maximum power point tracking techniques, fuzzy logic trackers enable an accurate and quick convergence to maximum power point under both steady-state and variable weather situations. The performance of the proposed maximum power point tracker is demonstrated in simulation.","keywords":"Fuzzy logic controller (FLC), Maximum power point tracker (MPPT), Photovoltaic (PV)","content":" Fuzzy logic based MPPT con trol for a PV system using SEPIC  converter  Moulay Abdellah Alhachemi  Mohammed Habbab  Abdeldjebar Hazzab  Laboratory of CAOSEE  Laboratoire de CAOSEE  cole de Technologie Suprieure  Universit Tahri Mohamed  Universit Tahri Mohamed  (ETS) 1100 Notre -dame  Bechar, Algeria  Bechar, Algeria  Montral, Canada  alhachemi.abdellah@univ -bechar.dz  habbab.mohamed@univ -bechar.dz  Hazzab.abdeldjebar@etsmtl.ca     Mansour Becahr  Hicham Gouabi   Laboratory of CAOSEE  cole de Technologie Suprieure   Universit Tahri Mohamed  (ETS) 1100 Notre -dame   Bechar, Algeria  Montral, Canada   bechar.mensour@univ -bechar.dz  hicham.gouabi.1@ens.etsmtl.ca       AbstractIn this study, a novel single -ended primary  inductor (SEPIC) converter -based fuzzy logic controller for maximum power point tracking is presented. By adding rules to the perturb and observ ing search strategy, the new controller enhances it while fuzzifying and removing its flaws. When compared to traditional maximum power point tracking techniques, fuzzy logic trackers enable an accurate and quick convergence to maximum power point under both steady -state and variable weather situations. The performance of the proposed maximum power point tracker is  demonstrated in simulation .  Keywords : Fuzzy logic controller (FLC), Maximum power point tracker (MPPT), Photovoltaic (PV)  I. INTRODUCTION      The non-renewable energy sources are rapidly running out, while the electricity demand  is increasing daily. To solve this problem, efficient and efficient electric power generation from renewable energy sources is required [1]. Renewable energy is one of the forms of energy that society can rely on because It is unpolluted, pure , and has no limits. One type of power generation that uses renewable energy is the photovoltaic (PV) system [2]. To utilize less conventional energy, the PV system must subsequently be linked to the grid, either directly or via a backup battery bank. Since the power produced by PV systems depends on radiation and temperature change, the PV framework has destitute productivity,  [2].    For the control of the PV systems, there are different sorts of DC-DC converters such as Buck converters, Boost converters, and Buck -Boost converters. Due to its output pick-up adaptability, a single -ended primary -inductor convert er (SEPIC) acts as a buck -boost DC/DC converter, where it changes its output voltage agreeing to its duty cycle. Unlike the customary buck -boost converter, the SEPIC converter includes a non -inverted output and it uses an arrangement capacitor to separate i nput from output [3]. The buck and buck -boost converters lose half of their input control due to input current arrangement exchange ; for that reason, the two types of converters should be excluded from maximum power applications. The boost converter has a nonstop input current, but the output voltage is always bigger than the input, which may not accomplish maximum power exchange operation in a few cases, such as when the m aximum voltage is less than the input [3].      This paper presents a fuzzy -based P&O st rategy for an MPPT standalone PV system. The proposed MPPT can abuse  the preferences of the P&O strategy and eliminate  its drawbacks. Output has been sep arated into five fuzzy subsets. As the proposed strategy continuously exchanges maximum power from PV arrays, it optimizes the number of PV modules.  II. MODELIG N OF PV SYSTEM       Photovoltaic is the technique and study connected to devices that directly convert sunlight into electricit y utilizing photovoltaic semiconductors. Direct conversion of solar energy into DC electrical energy can be achieved by photovoltaic cells [4]. The photovoltaic panel is made up of numerous cells that are connected in series Ns or shunt Nsh. Where it may b e mimicked by a current source coupled in parallel with a diode as described b y and depicted in Figure 1 [5].  Fig. 1.  Circuit for a  PV array.    The following equations provide the output current:  I= Iph ID                                                                          (1)  I= Iph I0[exp (q(V+RsI)AKBT)1] V+RsIRsh                           (2)  IphPhoto current                             AIdeality  factor    KBConstant  of Boltzmann                   TCell temperature    ID Current  via a diode                         RsSeries  resistance    I0 Current  of Saturati on                     qElectronic  charge    RshShunt  resistance                           VCell voltage  ICurrent  of cell      The shunt resistance (Rsh) is typically orders of magnitude larger than the series resistance (Rs) [6]. Therefore , it is common for the shunt and series resistances of a solar cell can be neglected to simplify the model. The resulting ideal voltage -current  characteristic of the solar cell is given by equation (3).  I= Iph I0[e(qVKT)1]                                                        (3)  III. SEPIC  CONVERTER  Power electronics researchers are working hard to create DC -DC converters with simpler designs and greater efficiency [7]. To maintain a constant output voltage, the suggested DC -to-DC converter employs a single -ended primary -induc tor converter (SEPIC) architecture. The SEPIC converter is made up of a duty cycle switch S, a diode, two inductors (L1 and L2), two capacitors (C1 and C2), and a load resistor. Figure 2 depicts the circuit diagram of a SEPIC converter.  A SEPIC is a DC -DC converter [8]. SEPIC  are DC -DC converters that can output voltages that are B, larger than, or equal to the input voltage. The duty cycle of the control transistor affects the SEPIC converter's output voltage. The SEPIC converter is two converte rs in one: a boost converter followed by a buck -boost converter . It has the advantages of having a non -inverted output (the output voltage has the same polarity as the input voltage)  , using a series capacitor to couple energy from the input to the output (which makes it more responsive to short -circuits), and being able t o shut down completely: when the switch \"S\" is turned off, the output voltage drops to 0 V, accompanied by a significant transient discharge of charge.    Fig. 2.  Simple circuit diagram of the SEPIC converter   Figure 3 depicts the circuit when the power switch is switched on and off (in Figures a and b, respectively).  Figure 3a When the swit ch is on, the first inductor, L1  is charged from the input voltage source. The second inductor L 2 absorbs energy from  the first capacitor C 1, leaving the output capacitor C 2 to supply the load current.   Fig. 3.a. The switch turned on .   Fig. 3.b. The switch  turned off.  Fig. 3. Equivalent circuit diagram of the SEPIC  conver ter when the switch is on and off .    When the switch is turned on, the input inductor is charged from the source, and the second inductor is charged from the first capacitor. No energy is supplied to the lo ad capacitor during this time. The i nductor current and capacitor voltage  polarities are marked in this Figure. When the power switch is turned off, the energy stored in the inductor is transferred.  The energy stored is transferred  through the diode and suppl ies the energy to the load [ 10], as shown in Figure 3 . b. The second inductor is also connected to the load during this time. The output capacitor sees a pulse of current during the off time, making it inherently noisier than a buck converter. The amount that the SEPIC converters increase or decrease the voltage depends prim arily on the duty cycle and the parasitic elements in the circuit. The output of an ideal SEPIC converter is:  Vout= D1D Vin                                                                       (4)     A SEPIC converter is to process the electricity from the PV system. This converter either increases or decreases the PV system voltage at the load. The proposed SEPIC converter operates in buck mode.  IV. FUZZY  LOGIC  CONTROL       In the fuzzy logic maximum power point tracking (MPPT) algorithm, the voltage and current at each instant k are measured to calculate the active power. The active power is then compared with the power at the previous instant (k -1) to obtain the change in p ower (P(k)). Similarly, the voltage at instant k is compared with the voltage at instant k -1 to obtain  the voltage error (V(k))  [11]. The power error is then divided by the current error to obtain the error (E). The error is then compared with the previo us error to calculate the change in error (E(k)). The error (E(k)) and the change in error (E(k)) are then used as the crisp inputs to the fuzzy logic controller. The flow chart for the fuzzy logic MPPT algorithm is shown in Figure 4. In this work, the M amdani inference technique, A -type membership functions, and a 25 -element rule base were used for the fuzzy logic control. The Mamdani inference technique is efficient and straightforward in defining the fuzzy output sets, and it is more popular among rese archers than other inference techniques [12]. The A -type or triangular membership function is used because it is simpler to split into low and high membership functions (MFs) than other membership functions. Additionally, it has been observed that the tria ngular membership function has a faster response and less overshoot than other functions [13]. A 25 -element rule base was used because it  has been shown to perform well [14][15].      Fig. 4. Fuzzy logic maximum power point tracking (MPPT) flow chart.       The following are the fuzzy rules in Table 1, which a re used for the desired MPP of push -pull converter PWM . The membership for input variables  (DP pv, DV pv) are show n in Figure  5, and t he membership for output variab le (DV pv*) is show n in Figure 6 . All the function s are defined  on a normalized interval [ -1 1].   Table. 1. Fuzzy logic rules for the p ush-pull converter. NB, negative big; NS, negative small; ZE, zero; PS, positive small; PB, positive big.  Vpv*[o/p]  Vpv*[i/p]    NB NS ZE PS PB  NB PS NB NB NB NS Ppv*[i/p]  NS PS PS NS NS NS  ZE ZE ZE ZE ZE ZE  PS NS NS PS PS PS  PB NS NB PB PB PS   Fig. 5.  Membership for input variables .   Fig. 6.  Membership for out put variable . V. SIMULATION RESULTS    The characteristics of the photovoltaic array that we use in this paper are given in Table 2.    Table. 2 . Electrical data of photovoltaic array.   Variable  Value  Maximum Power  Pmax =200  W Open circuit voltage  Voc=36.1 V Short circuit current  Isc=200  A Voltage at maximum power  Vmax =29.3 V Current at maximum power  Imax =6.9 A     Table 3 shows the SEPIC converter settings utilized in this study. The SEPIC converter is linked to the PV panel in the full model, and the duty cycle is  regulated by the Fuzzy Logic Controller.   Table. 3. The SEPIC converter parameters.  Variable  Value  Switching frequency  20 KH Z Load resistance  RLoad= 4.29   Inductance L 1 L1 = 3.6 MH Inductance L 2 L2 = 0.9 MH Capacitor C 1 C1 = 180 F  Capacitor C 2 C2 = 6000 F      The results are provided under standard test conditions; G=1000 W/m2 ; T=25C and it is show n in figure 7 .  Fig. 7.a. Current.   Fig. 7.b. Voltage.   Fig. 7.c. Power.  Fig. 7. Voltage, current , and power output results; at G=1000 W/m2 and T= 25C.    Overall, using MPPT and fuzzy logic to a SEPIC converter for a PV system result in considerable performance gains. Increased power output, higher efficiency, decreased ripple, enhanced transient response, and resilience are examples of these enhancemen ts. . CONCLUSIONS       This paper presents the design of an off -grid photovoltaic system with a fuzzy logic MPPT -controlled push -pull boost converter. The proposed system was simulated in MATLAB/Simulink a nd tested under various weather conditions. The results showed that the fuzzy logic algorithm outperformed t he conventional algorithms in terms of MPPT accuracy and minimization of fluctuations, regardless of rapid changes in irradiance.  REFERENCES  [1] G. Mahendran , and KV. Kandaswamy . Ant Colony  Optimized  Tuned  DC-DC converter . International Journal of Computer Applications (0975 8887) 108.11 (2013): 45 -50. [2] J. Dunia , and BMM  Mwinyiwiwa. Performance Comparison between UK and SEPIC Converters for Maximum Power Point Tracking Using Incremental Conductance Technique in Solar Power Applications . International Journal of Electrical, Computer, Energetic, Electronic and Communication Engineering 7.12 (2013): 2510 -2517.  [3] M. Cirrincione,  M. Pucci, and G. Vitale,  Growing Neural Gas (GNG) -Based Maximum Power Poin t Tracking for High -Performance Wind Generator With an Induction Machine . IEEE Transactions on Industry Applications 47.2 (2011): 861 -872. [4] M. Abdulkadir , AS. Samosir,  S., and AHN. Yatim . Modeling and Simulation of a Solar Photovoltaic System, Its Dyn amics and Transient Characteristics in LABVIEW.\" International Journal of Power Electronics and Drive System (IJPEDS) 3.2 (2013): 185 -192. [5] H. Bouzeria, C. Fetha, T. Bahi, I. Abadlia, Z. Layate, and S. Lekhchine. Fuzzy Logic Space Vector Direct Torque  Control of PMSM for Photovoltaic Water Pumping System.  Energy Procedia 74 (2015): 760 -771. [6] YM. Chen, YC. Liu, SC. Hung, and CS. Cheng . Multi -Input Inverter for Grid-Connected Hybrid PV/Wind Power System.  IEEE Transactions on Power Electronic s 22.3 (2007): 742 -750. [7] S. Ganesh , J. Janani , and GB. Angel . A Maximum Power Point Tracker for PV Panels Using SEPIC Converter .International Journal of Electrical, Computer, Energetic, Electronic and Communication Engineering 8.2 (2014): 637-642. [8] R. Vijayabalan , and S. Ravivarman . Z Source Inverter for Photovoltaic System with Fuzzy Logic Controller . International Journal of Power Electronics and Drive System (IJPEDS) 2.4 (2012): 371 -379. [9] A. Ramkumar , and SVS. Florence . \"Analysis o f Single Phase AC -DC SEPIC Converter using Closed Loop Techniques.  International Journal of Advanced Research in Electrical, Electronics and Instrumentation Engineering 4.2 (2015): 193 -201. [10] G. Tadi, and P. Ramamurthyr aju. Analysis of SEPIC for PV -Applications using PI Controller and Current Mode Control.\" International Journal for Scientific Research & Development 1.9 (2013): 175-180. [11] J. Li, and H. Wang . A novel stand -alone PV generation system based on variable step si ze INC MPPT and SVPWM control.\" In Proceedings of the 2009 IEEE 6th International Power Electronics and Motion Control Conference, Wuhan, China, 17 -20 May 2009. pp. 2155 -2160.  [12] C. Wang . A Study of Membership Functions on Mamdani -Type Fuzzy Inference System  for Industrial Decision -Making.  Master's thesis, Lehigh University, Bethlehem, PA, USA, 2015.  [13] MA. Usta, . Akyazi,  and H. Alta.  Design and performance of solar tracking system with fuzzy logic controller used different membership functions.  In Proceedings of the 2011 7th International Conference on Electrical and Electronics Engineering (ELECO), Bursa, Turkey, 1 -4 December 2011. pp. II -381-II-385.Mudi, R.K.; Pal, N.R. A robust self -tuning scheme for PI -and PD -type fuzzy controllers. IEEE Trans. Fuzzy Syst. 1999, 7, 2 16. [14] RK. Mudi , and NR. Pal.  A robust self -tuning scheme for PI - and PD -type fuzzy controllers.  IEEE Transactions on Fuzzy Systems 7.1 (1999): 2-16. [15] A. Shehata , H.  Metered , and WAH. Oraby . Vibration control of acti ve vehicle suspension system using fuzzy logic controller.  In Vibration Engineering and Technology of Machinery, edited by W. A. Oraby, 389 -399. Springer, Berlin, Heidelberg, 2015.      ","date":"2024-02-05T14:16:50.141013","authors":[{"full_name":"Moulay Abdellah Alhachemi","institut_name":"Laboratory of CAOSEE"},{"full_name":"Mohammed Habbab","institut_name":"Laboratory de CAOSEE"},{"full_name":"Abdeldjebar Hazzab","institut_name":"École de Technologie Supérieure, Université Tahri Mohamed"}],"author":[{"full_name":"Moulay Abdellah Alhachemi","institut_name":"Laboratory of CAOSEE"},{"full_name":"Mohammed Habbab","institut_name":"Laboratory de CAOSEE"},{"full_name":"Abdeldjebar Hazzab","institut_name":"École de Technologie Supérieure, Université Tahri Mohamed"}]}},{"_index":"article_index","_id":"14","_score":1.0,"_source":{"id":14,"title":"The Programmer’s Assistant: Conversational Interaction with a Large Language Model for Software Development","summary":"Large language models (LLMs) have recently been applied in software engineering to perform tasks such as translating code between programming languages, generating code from natural language, and autocompleting code as it is being written. When used within development tools, these systems typically treat each model invocation independently from all previous invocations, and only a specific limited functionality is exposed within the user interface. This approach to user interaction misses an opportunity for users to more deeply engage with the model by having the context of their previous interactions, as well as the context of their code, inform the model’s responses. We developed a prototype system – the Programmer’s Assistant – in order to explore the utility of conversational interactions grounded in code, as well as software engineers’ receptiveness to the idea of conversing with, rather than invoking, a code-fluent LLM. Through an evaluation with 42 participants with varied levels of programming experience, we found that our system was capable of conducting extended, multi-turn discussions, and that it enabled additional knowledge and capabilities beyond code generation to emerge from the LLM. Despite skeptical initial expectations for conversational programming assistance, participants were impressed by the breadth of the assistant’s capabilities, the quality of its responses, and its potential for improving their productivity. Our work demonstrates the unique potential of conversational interactions with LLMs for co-creative processes like software development.","keywords":"code-fluent large language models, foundation models, conversational interaction, human-centered AI","content":"The Programmers Assistant: Conversational Interaction with aLarge Language Model for Software DevelopmentSteven I. RossIBM Research AICambridge, MA, USAsteven_ross@us.ibm.comFernando MartinezIBM ArgentinaLa Plata, Buenos Aires, Argentinamartferc@ar.ibm.comStephanie HoudeIBM Research AICambridge, MA, USAStephanie.Houde@ibm.comMichael MullerIBM Research AICambridge, MA, USAmichael_muller@us.ibm.comJustin D. WeiszIBM Research AIYorktown Heights, NY, USAjweisz@us.ibm.comABSTRACTLarge language models (LLMs) have recently been applied in soft-ware engineering to perform tasks such as translating code betweenprogramming languages, generating code from natural language,and autocompleting code as it is being written. When used withindevelopment tools, these systems typically treat each model in-vocation independently from all previous invocations, and only aspecific limited functionality is exposed within the user interface.This approach to user interaction misses an opportunity for usersto more deeply engage with the model by having the context oftheir previous interactions, as well as the context of their code,inform the models responses. We developed a prototype system the Programmers Assistant  in order to explore the utility ofconversational interactions grounded in code, as well as softwareengineers receptiveness to the idea of conversing with , rather thaninvoking , a code-fluent LLM. Through an evaluation with 42 partici-pants with varied levels of programming experience, we found thatour system was capable of conducting extended, multi-turn discus-sions, and that it enabled additional knowledge and capabilitiesbeyond code generation to emerge from the LLM. Despite skepti-cal initial expectations for conversational programming assistance,participants were impressed by the breadth of the assistants capa-bilities, the quality of its responses, and its potential for improvingtheir productivity. Our work demonstrates the unique potential ofconversational interactions with LLMs for co-creative processeslike software development.CCS CONCEPTSHuman-centered computing HCI theory, concepts andmodels ;Software and its engineering Designing software ;Computing methodologies Generative and developmentalapproaches .This work is licensed under a Creative CommonsAttribution-NonCommercial-NoDerivs International 4.0 License.IUI 23, March 2731, 2023, Sydney, NSW, Australia2023 Copyright held by the owner/author(s).ACM ISBN 979-8-4007-0106-1/23/03.https://doi.org/10.1145/3581641.3584037KEYWORDScode-fluent large language models, foundation models, conversa-tional interaction, human-centered AIACM Reference Format:Steven I. Ross, Fernando Martinez, Stephanie Houde, Michael Muller, and JustinD. Weisz. 2023. The Programmers Assistant: Conversational Interactionwith a Large Language Model for Software Development. In 28th Inter-national Conference on Intelligent User Interfaces (IUI 23), March 2731,2023, Sydney, NSW, Australia. ACM, New York, NY, USA, 24 pages. https://doi.org/10.1145/3581641.35840371 INTRODUCTIONSoftware development is a highly skilled task that requires knowl-edge, focus, and creativity [ 27,28]. Many techniques have beendeveloped to enhance the productivity of software engineers, suchas advanced code repositories [ 86], knowledge repositories [ 39],Q&A sites [ 1], and pair programming practices [ 18]. Collaborativesoftware engineering is especially promising, given that profes-sional software development is rarely a solo activity and relevantknowledge and expertise are typically distributed widely within anorganization [ 68]. Many efforts have focused on incorporating col-laborative technologies into software development environments(e.g. [8, 25, 26, 58, 101]).The pioneering work of Rich and Waters on The ProgrammersApprentice [70] presented a novel concept of a knowledgeable auto-mated assistant  in effect, an artificial collaborative partner  thatcould help software engineers with writing code, designing soft-ware systems, and creating requirements specifications. At the time,AI technologies and computing resources were not sufficient tofully implement their vision. In the intervening years, an increase incomputational power, the availability of large corpora of languageand code data, and the development of deep neural networks havemade new approaches to achieving their goals worth exploring.Recently, models leveraging the transformer architecture [ 96]have been developed to perform domain-specific software engi-neering tasks, such as translating code between languages [ 75],generating documentation for code [ 36,38,97,98], and generatingunit tests for code [ 92] (see Talamadupula [90] and Allamanis et al .[5]for surveys). Recently developed foundation models  largelanguage models that can be adapted to multiple tasks and whichexhibit emergent behaviors for which they have not been explic-itly trained [ 14]  have also proven to be capable with source code.491IUI 23, March 2731, 2023, Sydney, NSW, Australia Ross et al.While the intent of training LLMs such as GPT-2 [ 64] and GPT-3 [ 17]was to give them mastery of natural language, it quickly becameapparent that the presence of code in their training corpora hadgiven them the ability to generate code based on natural languagedescriptions [ 49]. The Codex model [ 24] was then produced by fine-tuning GPT-3 on a large corpus of source code data, leading to thedevelopment of Copilot [ 32], a tool that helps software engineers byautocompleting code as it is being written. Experimentation withCopilot has shown its ability to perform additional tasks, such asexplaining code, generating documentation, and translating codebetween languages [6].Although autocompletion interfaces are useful and valuablewhen the system can discern the developers intent, there are manyinstances where that is insufficient. For example, the developermay have a good idea of what they want to do, but may be unclearon what functions, libraries, or even algorithms to employ. Theymay even have general programming questions that need to beanswered before they are able to write any code.In this paper, we seek to understand whether modern develop-ments in code-fluent foundation models  large language modelsthat have been fine-tuned on source code data  are sufficient tosupport a conversational agent that can act as an assistant in thesoftware development process. We developed the ProgrammersAssistant to explore the capabilities that conversational interactioncould enable and the extent to which users would find conversa-tional assistance with programming tasks desirable and useful.We hypothesize that a conversational system may provide aflexible and natural means for interacting with a code-fluent LLM.Conversational interaction could enable users to pursue their ques-tions in a multiple exchange dialog (as observed by Barke et al .[13]) that allows them to ask follow-up questions and refine theirinquiries. A conversational programming assistant could ask theuser clarifying or disambiguating questions to help it arrive at thebest answer. It could also provide multiple types of assistance tothe user beyond simply generating code snippets, such as engagingin general discussion of programming topics (e.g. [ 22,71]) or help-ing users improve their programming skills (as observed in otherstudies of automating technologies [99]).Our paper makes the following contributions to the IUI commu-nity:We provide empirical evidence that a conversational pro-gramming assistant based on a state-of-the-art, code-fluentfoundation model provides valuable assistance to softwareengineers in a myriad of ways: by answering general pro-gramming questions, by generating context-relevant code,by enabling the model to exhibit emergent behaviors, and byenabling users to ask follow-up questions that depend upontheir conversational and code contexts.We show how different interaction models  conversation,direct manipulation, and search  provide complementarytypes of support to software engineers with tradeoffs be-tween the users focus and attention, the relevance of sup-port to their code context, the provenance of that support,and their ability to ask follow-up questions.We motivate the need to further understand how to designhuman-centered AI systems that enhance the joint perfor-mance of the human-AI collaboration.2 RELATED WORKWe discuss three areas of related work that have either motivatedour study of conversational programming assistance or providedthe technical foundations for it. We begin by briefly summarizingRich and Waters visionary work on the Programmers Appren-tice [ 70], followed by summarizing work on code-fluent foundationmodels and human-centered evaluations of how these models im-pact software engineers work. Finally, we discuss conversationalinteraction and how it might be employed to provide more flexibleand sophisticated assistance to software engineers.2.1 The Programmers ApprenticeOur work is inspired by the vision laid out by Rich and Waters [70],which describes an artificial agent that can act as an intelligent as-sistant for software engineers by providing advice, catching errors,and handling routine details throughout the software developmentprocess. The Programmers Apprentice [ 70] relied on a knowledgebase of clichs, which are formal, structured versions of whatare known today as software design patterns [ 31]. It used a hy-brid reasoning system capable of special-purpose reasoning basedon frames and a plan calculus, along with general purpose logicalreasoning. Although natural language interaction was envisioned,the original prototype implementation ultimately used a stylizedcommand language. We view our work as a conceptual successorto the Programmers Apprentice, as it enables the natural languageinteraction that the Programmers Apprentice lacked.2.2 Code-fluent Foundation Models andHuman-Centered Evaluations ofProgramming AssistanceGenerative models based on the transformer architecture [ 96] haverecently been applied to the domain of software engineering. Code-fluent large language models are capable of generating code fromnatural language descriptions [ 105], translating code from one lan-guage to another [ 75], generating unit tests [ 92], and even gener-ating documentation for code [ 36,38,97,98]. These models areprobabilistic systems, and as such, do not always produce perfectresults (e.g. code that is free of syntax or logical errors). Nonetheless,Weisz et al . [102] found that software engineers are still interestedin using such models in their work, and that the imperfect outputsof these models can even help them produce higher-quality codevia human-AI collaboration [103].New tools based on code-fluent LLMs are actively being devel-oped. GitHub Copilot1is described as Your AI pair programmer. Itis optimized for the code autocompletion use case: given a startingsnippet such as a methods documentation, signature, or partialimplementation, Copilot completes the implementation. Copilot isbased on the OpenAI Codex model [ 24], a 12 billion parameter ver-sion of GPT-3 [ 17,49], fine-tuned on code samples from 54 millionpublic software repositories on GitHub. Empirical evaluations of1https://github.com/features/copilot492The Programmers Assistant IUI 23, March 2731, 2023, Sydney, NSW, Australiathis model have shown that, although the quality of its outputs isquite good, those outputs may still be problematic [ 57]. Echoing theresults from Weisz et al . [103] , human-centered evaluations of Copi-lot have found that it increases users feelings of productivity [ 109],and that almost a third (27%) of its proposed code completions wereaccepted by users. In a contrasting evaluation, Vaithilingam et al .[95] found that while most participants expressed a preferenceto use Copilot in their daily work, it did not necessarily improvetheir task completion times or success rates. Yet, in a study byKalliamvakou [40], developers working with Copilot were able toimplement a web server in Javascript 55% faster than developerswho did not use Copilot.A grounded theory analysis of how programmers interact withCopilot [ 13] found that their interactions varied depending uponwhether they were accelerating tasks that they already knew howto do or if they were exploring solutions to problems that they wereless sure about. Autocompletion was effective when developerswere operating in acceleration mode and relied on the modelto produce short completions that could be verified quickly. Inexploration mode, however, the interaction was more awkward.Developers would communicate with Copilot by typing commentsand seeing what Copilot generated in response. Then, they wouldmodify their comments to explore other ways of prompting a re-sponse. Ultimately, the comments used to prompt the model wouldbe deleted after the relevant code was generated, indicating thattheir value was largely in driving a back-and-forth, yet context free,dialog with the model to coerce it to produce the desired resultsthrough an iterative refinement process. In this paper, we fullycommit to a context-aware conversational style of interaction witha code-fluent LLM and assess the value it provides to users.2.3 Conversational Interaction and Analysis2.3.1 Conversational Interaction. Using natural language to inter-act with technology has had a long research history [ 2], starting inthe 1960s with pattern-matching approaches like Eliza [ 104], andcontinuing to today with state-of-the-art large language model-based conversational systems [ 107] such as Meena [ 3] and Blender-Bot [ 84]. These systems are intended to address the problem ofopen-domain dialog, with a goal of realistically engaging in con-versation, but not particularly in a goal-directed or task-orientedmanner.Task-oriented chatbots are typically built with frameworks suchas the Microsoft Bot Framework2, Google DialogFlow3, and IBMWatson Assistant4. They operate using pre-defined dialogue treesand use natural language processing to detect conversational intentsand extract contextual entities. This structure enables the creation ofspecial purpose, but fairly limited and rigid, conversational agents.There have been several recent attempts to investigate conver-sational programming assistance. Kuttal et al . [42] conducted aWizard of Oz study in which a pair programmer was replaced witha conversational agent, and they found that agents can act as ef-fective pair programming partners. The PACT system [ 106] is achatbot that assists programmers adjusting to new programming2https://dev.botframework.com/3https://cloud.google.com/dialogflow4https://www.ibm.com/products/watson-assistant/artificial-intelligenceenvironments. PACT is structured as a discrete question-answeringsystem based on a neural machine translation approach, but itdoesnt maintain a conversational context.2.3.2 Conversation Analysis. Conversation is a form of interactionbetween people that enables robust communication. ConversationAnalysis [ 76] is a method for understanding the natural structureof human conversational interaction. It catalogs different patternsof conversational acts and how they are utilized by interlocutors inorder to attain a wide variety of goals. Recently, Conversation Anal-ysis has been adapted to describe patterns of interactions betweenhumans and artificial conversational agents in order to aid in thedesign of chatbots [ 50]. We apply techniques from ConversationAnalysis in our study of conversational programming assistance.3 THE PROGRAMMERS ASSISTANTIn order to explore conversational programming assistance, wecreated a functional prototype system called The ProgrammersAssistant . Our prototype, shown in Figure 1, combines a code editorwith a chat interface. The code editor was implemented using theMicrosoft Monaco Editor5embedded in a React wrapper6. Thechat user interface was implemented using the React-Chatbot-Kit7framework. To drive the conversational interaction, we employedOpenAIs Codex model [24], accessed through its web API.We developed our prototype as a lightweight coding environ-ment in order to examine the user experience of interacting witha conversational assistant. Our work was exploratory in nature,and thus we did not have specific design goals for the prototypebeyond integrating a code editor with a code-fluent LLM. We alsodid not attempt to target the prototype for a specific class of users(e.g. novices or experts) or use cases (e.g. writing code vs. learning anew programming language), as we wanted any value provided byconversational assistance to emerge from our user study. We alsodid not implement the ability to run or debug code in our prototypeas we wanted to explore the nature of the conversational interac-tion rather than having users focus extensively on the productionof working code.When designing how users would interact with the Program-mers Assistant, we decided that it should be available on demandand not monitor the users work in progress or give unsolicitedsuggestions or advice, in keeping with the conversational agentinteraction model proposed by Ross et al. [ 73,74]. This approachwas supported by feedback from prospective users who were con-cerned about the assistant providing criticism of unfinished effortsin progress or distracting them while they worked. Instead, weforce initiative onto the user and only have the assistant respondto their requests. In this way, the assistant can provide help whenrequested without undesirable interruptions that can distract orinterfere with the users flow.When a user interacts with the assistant, we keep track of theirselection state in the code editor. If a user sends a message to theassistant without any code selected in the editor, then that message(along with the prior conversational context) is passed directly tothe model. If a user sends a message to the assistant with new code5https://microsoft.github.io/monaco-editor/6https://www.npmjs.com/package/@monaco-editor/react7https://fredrikoseberg.github.io/react-chatbot-kit-docs/493IUI 23, March 2731, 2023, Sydney, NSW, Australia Ross et al.selected in the editor (i.e. code that wasnt previously selected whenthey sent their last message), then that code is appended to themessage before being communicated to the model.The model may produce multiple types of responses to a usersmessage. We treat each type of response differently in the UI.Responses that do not contain code are always rendered inthe chat UI (Figure 1E).Responses containing short code snippets ( 10 lines) arerendered inline in the chat UI (Figure 1G).Responses containing longer code snippets (> 10 lines) showthe code in a pop-up window (Figure 2A), with a proxy entryin the chat transcript (Figure 2B) that allows users to re-display the code window after it has been closed. Non-codetext in the response remains in the chat transcript.The assistant never directly modifies the contents of the userssource code; rather, any code the user desires to transfer from thechat takes place via copy/paste.Figure 1 shows a screenshot of a real, sample conversation, inwhich the user asks a question that results in an inline response,then requests an explanation of some code in the editor, and then re-quests further elaboration. Figure 2 shows an example conversationthat resulted in the generation of a longer code sample, shown in apopup window. This example shows how the assistant produced anincomplete solution, followed by criticism from the user regardingthe missing code, and resulting in an apology and the generationof a complete solution.3.1 Supporting Conversational InteractionWe enabled Codex to conduct a conversational interaction by prompt-ing it with a conversational transcript and a request to producethe next conversational turn. The prompt establishes a pattern ofconversation between a user and a programming assistant namedSocrates. It provides several examples of Socrates responding to gen-eral coding questions, generating code in response to a request, andaccepting code as input. It establishes a convention for delimitingcode in the conversation, making it easy to parse for display in theUI. It also establishes an interaction style for the assistant, directingit to be polite, eager, helpful, and humble, and to present its re-sponses in a non-authoritative manner8. Because of the possibilitythat the model might produce erroneous answers or incorrect code(as discussed in Weisz et al . [102] ), we felt it was important that theassistant convey a sense of uncertainty to encourage users to notaccept its results uncritically to avoid over-reliance (e.g. as observedin Moroz et al .s study of Copilot [ 51], and discussed more gener-ally in Ashktorab et al . [9]) as well as automation bias [ 45,46,65].We present the full text of the prompt used for the assistant inAppendix D.3.2 Architecture & UI DesignThe Programmers Assistant communicates with the Codex API viaa proxy server that forwards requests from the React client. Theproxy also rate-limits access to conform to the APIs policy, andit logs UI events from the client (e.g. requests, responses, and UI8The assistants use of non-authoritative responses was encoded into the LLM prompt;output token probabilities from the LLM were not utilized to influence the assistantsresponse.interactions) in a back-end database. To address inconsistenciesin the style or formatting of code generated by Codex, the proxyserver reformats all code segments using the Black code formatter9before transmitting them to the client UI.The client maintains the transcript of the ongoing conversation.Each time the user sends a message in the chat, the client constructsa new prompt for the model by concatenating the initial prompt,the chat transcript, and the users new utterance, and makes a re-quest for the model to complete the transcript. This completionrequest also specifies a stop sequence of tokens to prevent themodel from generating both sides of the conversation (e.g. whatthe model thinks the users next utterance might be after the as-sistants response). Given the APIs limitation on context length(4,096 tokens for both the prompt and model response), we silentlyforget older exchanges in the chat transcript when constructingthe prompt to ensure that our completion request remains withinbounds. Nonetheless, the entire conversational history remainsvisible to the user in the UI.The client UI provides a loose coupling between the source codeeditor and the chat interface. Users can hide the chat pane whenthey wish to focus solely on their code, and re-engage with itwhen they desire assistance. Code selected in the editor is includedin the conversation in order to couple the code context with theconversation. Easily-accessible buttons are provided in the UI tocopy code responses from the assistant to the clipboard.3.3 Handling Model LimitationsWhile developing the Programmers Assistant, and in early pilottesting, we experienced some quirks and shortcomings of the modeland our approach to using it for conversational interaction. Onelimitation stemmed from the fact that the model sometimes pro-duced incorrect responses (e.g. code with syntax errors), incompleteresponses (e.g. code that was missing functionality), irrelevant re-sponses (e.g. responses not related to the users question), or insub-stantial responses (e.g. I dont know). Because of the probabilisticnature of model inference, re-prompting the model would some-times produce a more correct or appropriate response. Thus, weadded the ability for users to try again,  either by asking in the chator by clicking a button in the UI (Figure 1C). This feature removesthe assistants last response from the context presented to the modeland then re-invokes the model with an increased temperature10.Although it is possible for transformer models such as Codex toproduce multiple possible responses to a single prompt, we onlyrequest a single response in order to speed up response time aswell as to preserve the token budget for conversational context.Thus, the try again feature provides an alternate way to producea wider variety of responses.During pilot testing, we noticed that the assistant sometimeshappened to generate the same response to multiple, unrelated re-quests. In these cases, the assistant tended to get stuck in a patternof repeating the same response and was unable to resume normalconversation. To avoid this problem, we automatically execute a9https://black.readthedocs.io/en/stable/10Temperature is a parameter in a generative model that specifies the amount ofvariation in the generation process. Higher temperatures result in greater variabilityin the models output.494The Programmers Assistant IUI 23, March 2731, 2023, Sydney, NSW, AustraliaABHGFECDFigure 1: The Programmers Assistant. The user interface provides a code editor on the left (A) and a chat pane on the right (B).The try again button (C) allows users to ask the assistant to generate an alternate response to the most recent question. Thestart over button (D) resets the conversational context for the assistant, but maintains the chat transcript in the UI. In thisexample, we show the assistant introduce itself to the user (E). Next, the user asks a general programming question (F), forwhich the assistant provides an inline code response (G). The user then asks a question about code selected in the editor (H),followed by a series of follow-up questions.try again operation in the background when we see identicalconsecutive responses from the assistant.Finally, we noticed that the accumulation of conversational con-text sometimes resulted in the assistant becoming fixated on someportion of the earlier conversation. For example, it might respondto a question with portions of the prompt or of earlier conversation,and become less responsive to newer requests. To address this issue,we introduced a start over feature, accessible via the chat or byclicking a button in the UI (Figure 1D), that resets the context to theoriginal prompt, forgetting the rest of the conversational history.We preserve the chat transcript in the UI, but delineate the break inthe assistants memory with an annotation in the chat transcript.These annotations are added both for try again and start over.3.4 Sample ConversationWe provide a real sample conversation with the ProgrammersAssistant in Listing 1. This conversation begins with the assistantgreeting the user (line 1). Next, the user asks a general Pythonprogramming question (line 4), to which the assistant responds witha non-authoritative remark (I think...) and a code snippet (line9). The user next asks a follow-up question that depends on theirprevious question and the assistants response (line 11), to whichthe assistant provides another code snippet (line 15), satisfying theusers request.The user then switches topics and asks the assistant to writea Fibonacci function (line 17), and the assistant again respondswith a non-authoritative remark (I will give it a try, line 20) anda block of code. The user then asks how the function works (line30) and the assistant provides an adequate description (line 32).495IUI 23, March 2731, 2023, Sydney, NSW, Australia Ross et al.Figure 2: Code Preview. Long code responses (> 10 lines) are presented in a pop-up window (A), with a button to copy the codeand close the window. This code can be re-accessed from the chat transcript by clicking the proxy icon (B).Next, the user asks the assistant to re-implement the function in adifferent way (line 37), again leveraging the ability to ask follow-upquestions. The assistant produces an alternative implementationthat conforms to the users request (line 41). The user follows upwith a question that depends on multiple past utterances and re-sponses in the chat transcript (line 47), and the assistant producesa relevant response (line 49). The conversation closes with the userthanking the assistant (line 53) and the assistant acknowledgingtheir gratitude (line 55).Listing 1: A conversation with the Programmers Assistant.Code presented by the assistant is listed in bold face.1 Socrates : Hello ! My name is Socrates your programming2 assistant . What can I help you with today ?34 User : How do I get a list of the keys in a5 Python dictionary ?67 Socrates : I think you would do something like this :89 list(my_dict.keys())1011 User : What if I just wanted to iterate over them ?1213 Socrates : I think you would do something like this :1415 for key in my_dict:1617 User : Write a function that returns the nth value18 in the Fibonacci sequence .1920 Socrates : I will give it a try .2122 def fib(n):23 if n == 0:24 return 025 elif n == 1:26 return 127 else:28 return fib(n - 1) + fib(n - 2)2930 User : How does this work ?3132 Socrates : The Fibonacci sequence is defined by the33 recurrence relation F(n) = F(n -1) + F(n -2).34 This function implements that recurrence35 relation .3637 User : Can this be done without recursion ?38496The Programmers Assistant IUI 23, March 2731, 2023, Sydney, NSW, Australia39 Socrates : I think so. How about this ?4041 def fib(n):42 a,b = 0,143 for i in range(n):44 a,b = b,a+b45 return4647 User : Which version is better ?4849 Socrates : I think the recursive version is more50 elegant , but the iterative version is more51 efficient .5253 User : Thanks !5455 Socrates : You 're welcome !4 EMPIRICAL STUDY OF CONVERSATIONALPROGRAMMING ASSISTANCEWe conducted an empirical user study of the Programmers Assis-tant to assess whether conversational assistance provides value ina software engineering context11. Our inquiry focused on the userexperience and value of conversational interactions grounded incode. We therefore designed a qualitative study to investigate atti-tudes toward a conversational programming assistant: do peopleenjoy interacting conversationally, what kinds of questions do theyask, and how does the experience compare to other forms of pro-gramming support such as searching the web? We note that priorstudies (e.g. [ 103,105,109]) conducted quantitative examinationsof the use of LLMs in code work; our study is akin to Weisz et al .squalitative examination of software engineers attitudes towardworking with models that may fail to produce working code [ 102].To address our questions, we deployed the Programmers Assis-tant within our organization  a global technology company  andinvited people to try it out and give us feedback on their experience.We invited people with varying levels of programming skill in orderto obtain a wide range of feedback on the kinds of use cases forwhich the tool could provide assistance.4.1 TasksWe set up the Programmers Assistant as a playground environmentthat participants could try out with a few sample programmingproblems. We created a tutorial to orient participants to the assis-tant, its capabilities, and how to interact with it. We also createdfour programming challenges focused on writing code, document-ing code, and writing tests for code. We designed these challengesto expose participants to a broad range of the assistants capabilities.For each of these challenges, we explicitly did not evaluate metricssuch as the participants productivity, the quality of their solutions,or the time taken to produce them, as the focus of our study was tounderstand the utility of conversational interaction. We selectedPython as the language used for the tutorial and challenges becauseof its general popularity [ 21] and the fact that it was well-supportedby our underlying LLM [24].4.1.1 Tutorial. All participants were first introduced to the Pro-grammers Assistant through a tutorial. The tutorial walked each11For historical context, we note that our study was completed before the publicrelease of ChatGPT [ 60], which has subsequently demonstrated the application ofconversational assistance for programming tasks [34].participant through 10 sample interactions to give them a feelingfor what the assistant could do and how to interact with it. Thetutorial demonstrated how to ask questions, how to request code tobe generated, and how to evaluate existing code. It did not specifi-cally cover how to generate documentation or unit tests. Tutorialinstructions were provided within the code editor. We include thespecific text used for the tutorial in Appendix B.4.1.2 Programming Challenges. After completing the tutorial, par-ticipants unlocked four programming challenges. Two of the chal-lenges involved coding problems (writing a queue class and writingcode to create a scatterplot of data in a CSV file), one involved doc-umenting a given function (an implementation of a graph searchalgorithm), and one involved writing unit tests for a given func-tion (computing the greatest common divisor of two arguments).Although the Programmers Assistant was visible and available foruse, we provided no specific requirement that it actually be used tocomplete the challenges.After participants completed their solution to a challenge, theysubmitted it by clicking a button in the UI. The code editor usedin the Programmers Assistant was not a fully-functional IDE anddid not provide syntax checking or the ability to run, test, or debugcode. Due to these limitations, participants were asked to submittheir solutions when they felt they had completed the challenge totheir own satisfaction.4.2 ParticipantsTo recruit participants for our study, we posted internal advertise-ments in various communications channels focused on softwareengineering. Our advertisements stated that we were evaluating aconversational programming assistant, but were kept deliberatelyvague in order to minimize the impact on peoples expectations ofthe experience.Our advertisement yielded a pool of 140 potential participants.In order to recruit a diverse sample, we used a screening surveythat asked about their job role, their familiarity with and recencyof use of Python, and their availability to participate in our study.We accepted participants into the study on a rolling basis, selectingparticipants to capture a range of programming experiences andensure balanced gender representation. We conducted periodic re-views to determine whether we were learning something new fromeach participant or if we had reached the point of saturation [ 7]. Westopped collecting data after running 42 participants as we were nolonger observing any new behaviors or gleaning any new insights.The Programmers Assistant implementation and configurationwere held constant over the course of the study; no changes to theUI design or LLM prompt were made.Our participants had the following self-identified characteristics:Job role: 19 Software Engineers, 12 Researcher/Scientists, 3Software Architects, 2 Data Scientists, 1 Machine LearningEngineer, 1 Systems Test Engineer, 1 Business Analyst, 1Manager, 1 Marketer, and 1 Consultant.Gender: 21 Female, 19 Male, 1 Gender Variant / Non-con-forming, and 1 Preferred not to say.Python Experience: 17 participants had 3+ years of Pythonexperience, 11 had 1-3 years, 11 had less than 1 year, and 3were not familiar with Python.497IUI 23, March 2731, 2023, Sydney, NSW, Australia Ross et al.Recency of Python Use: 29 participants had written Pythoncode within the past month, 4 within the past year, 5 withinthe past 5 years, and 4 had not written Python code withinthe past 5 years.We provide full demographic information for individual partici-pants in Appendix E.4.3 ProcedureParticipants completed the study on their own time, independentlyand without moderation. Each participant was provided with a weblink to a pre-study survey that described the nature of the study andthe tasks that they would be expected to perform. They were thendirected to the Programmers Assistant to complete the tutorial andthe four programming challenges. When participants indicated theywere finished with the challenges12, they were directed to a finalpost-study survey. Complete sessions generally required about anhour of effort, though some participants spread their effort acrossa longer period of time and across multiple sessions. Participantswere compensated for their time at a rate equivalent to US $15/hr.4.4 MeasuresWe collected a variety of data in our study from three sources:(1)Surveys . We employed three surveys in the study: a pre-study survey to collect demographic information, a pre-tasksurvey to gauge expectations of the conversational user ex-perience, and a post-task survey to assess actual user expe-rience. We describe these survey questions in the relevantcontext of our results, and we provide a complete listing ofall survey instruments in Appendix A.(2)Event logs . The Programmers Assistant was instrumentedto collect data on participants usage. The event logs pro-vided timestamped records of interaction events, includingconversational exchanges, hiding/showing the assistant, useof the try again and start over features, and use of copy/-paste.(3)Conversation logs . From the event logs, we extracted con-versational transcripts between each participant and theProgrammers Assistant.5 RESULTS5.1 Data & AnalysisWe collected a wealth of data in our study: 126 survey responsesfrom three surveys per participant, containing 296 written com-ments in open-ended survey questions, and 4,877 instances of 23different types of UI events, including 1,699 conversational ex-changes13in the event logs. We also compute, for each participant,counts or durations for 21 different metrics from the event logs.In our analysis, we deliberately exclude the portion of our datacollected during the tutorial exercise. We exclude this data becausethat activity was guided by the tutorial instructions, not by ourparticipants own initiative. Thus, our final sample consists of 3,17212We did not enforce that participants actually complete all of the challenges. Never-theless, all participants but one did submit solutions to all of the challenges.13We refer to a participants utterance, followed by the assistants response, as aconversational exchange.events, including 968 conversational exchanges in the event logs;no survey data was excluded.Our primary analysis of this data is qualitative, as our partici-pants provided us with a rich source of interesting feedback andthought-provoking insights in their comments. Where applicable,we supplement this data with quantitative data from the surveyand the event logs, as well as chat transcript data from the con-versation logs. In this way, we triangulate [ 47] across our threedata sources, using the open-ended survey data as a foundation.When we quote participants, either from their qualitative surveyresponses or the conversational transcripts, we reproduce theirwords exactly as typed, including typos, misspellings, grammati-cal errors, capitalization, and potential trigger words, and we onlymake minor clarifying edits where needed, delineated by squarebrackets.In order to set the context for our analysis, we first describehow we used reflexive thematic analysis to analyze participantsresponses to the open-ended survey questions. We then describeour analysis of the conversation logs and our development of acoding guide based on Conversation Analysis [ 76], and specifically,Moore and Arars Natural Conversation Framework [50].5.1.1 Thematic Analysis of Qualitative Survey Responses. We con-ducted a reflexive thematic analysis to analyze the responses toour seven open-ended survey questions. We followed the processdescribed by Braun and Clarke [16] in which researchers immersethemselves in the data, generate codes for material that seemsinteresting, and then iteratively group and refine codes throughcollaborative discussion in order to identify higher-level themes.Initially, four authors performed open-coding on the open-endedsurvey responses. Through discussion, these codes were groupedand consolidated into a single set, which were then re-applied tothe data by two authors. After another round of discussion, theseauthors identified a set of 12 higher-level themes. Some themes hadclear parallels to quantitative survey questions or event log data,and thus represented clear instances where we were able to trian-gulate across data sources. Other themes surprised us. We structureour presentation of the results based on these 12 themes, groupedinto three different aspects of the user experience: expectationsand experience, utility of conversational assistance, and patterns ofinteraction and mental models.5.1.2 Conversation Analysis via the Natural Conversation Frame-work. In order to understand the content and structure of theconversations that took place between our participants and theProgrammers Assistant, we turned to the Natural ConversationFramework [ 50] (NCF). We developed a codebook for the eventlogs, beginning with 21 different categories of utterances from theNCF. Nine NCF categories  Acknowledgment, Apology, Confirma-tion, Expression of Gratitude, Farewell, Greeting, Self-Identification,Welfare Check, and Welfare Report  appeared twice in our code-book to distinguish cases in which the utterance was made by thehuman participant vs. the assistant. Other NCF categories weresplit to provide nuanced detail about the interaction; for example,we distinguished three different kinds of NCF requests, dependingupon whether they were stated as Requests for Action (e.g. Wouldyou...), Commands of Action (e.g. Write a function that...), orExpressions of Desire (e.g. I want...). We also added 18 additional498The Programmers Assistant IUI 23, March 2731, 2023, Sydney, NSW, AustraliaInterlocutor Orientation CodesHuman Social Acknowledgment ,Apology , Criticism, Expression of Gratitude ,Farewell ,Greeting , Politeness,Praise, Self Identification , Small Talk, Welfare Check ,Welfare ReportTask Asks Question, Asserts Information, Capability Check, Command of Action, Expression of Desire, Identi-fies Error, Request for Action, Requests Elaboration, Requests ExplanationMeta / UI Chat Context Required, Confirmation , Copy, Copy (Extraneous), Erroneous Input, Includes Selection,Includes Extraneous Selection, Missing Selection, Paste, Paste (Extraneous), Pasted Code in Chat, SpellingError, Start Over, Try AgainAssistant Appears Fixated, Claims Ignorance, Didnt Understand, Grants Request (Complete), Grants Request(Incomplete), Offers Help, Provided Wrong Answer, Requests Details, Requests Paraphrase, ResponseIncludes Code, Spews GarbageTable 1: Event log codebook. Our codebook contained 46 unique codes, applied separately to participant utterances (Human)and assistant responses (Assistant). Codes in bold were applied to both participant and assistant responses. Human codes wereclassified as demonstrating either a social or task orientation to the assistant.codes to identify meta-information such as utterances that includedcode, utterances that referenced selected code, utterances that im-plicitly or explicitly referenced earlier portions of the conversation,or non-verbal UI activities such as copies, pastes, and invocationsof try again and start over. Finally, we classified a subset of thehuman-applied codes based on whether they represented a par-ticipants task or social orientation toward the assistant. We listour codes in Table 1, but note that not all of them ended up beingrelevant to our analysis.When coding conversational data, we applied individual codesat the level of each conversational utterance. We allowed multiplecodes to be applied to each utterance to account for utterances thatperformed multiple functions (e.g. greeting and self-identification).In order to ensure consistency in how our codebook was applied,two authors coded a 10% sample of the 968 conversational ex-changes, achieving a satisfactory level of inter-rater reliability (Krip-pendorfs =0.77, where agreement was conservatively definedas having all of the same codes applied to both utterances in aconversational exchange).5.2 Expectations and ExperiencePilot testing of the Programmers Assistant suggested that softwareengineers would be skeptical of a conversational programmingassistant and its ability to provide useful assistance. Our studyrevealed that, for most participants, their actual experience afterusing the tool was better than they had anticipated. Participantswere surprised at the quality of the assistants responses and theyappreciated how its integration with the code editor reduced theamount of context switching they needed to do in the UI. Someparticipants struggled with the code selection feature, althoughothers appreciated the ability to ask questions related to selectedcode.5.2.1 Usage. All of our participants engaged with the Program-mers Assistant while working on the challenges, despite therebeing no requirement to do so. Forty-one participants submittedsolutions to all four challenges, and one participant, P14, only sub-mitted solutions for one of the four challenges. Participants spentan average of 68 minutes engaged with the assistant, as measuredby the amount of time the Programmers Assistant window was infocus.Participants made an average of 23.0 utterances (SD = 15.1 ut-terances) to the assistant. On average, 6.2 of their utterances (SD =4.3 utterances) contained a code selection. The average latency perrequest14was 6.7 seconds (SD = 3.1 seconds).We saw a 66.3% rate of acceptance of generated code, where weconsidered code to be accepted if the participant performed a copyimmediately after the code was generated. This acceptance rate ismuch higher than the 27% acceptance rate reported for Copilot [ 109].We believe one reason we observed a higher acceptance rate isbecause Copilots completion suggestions are generated proactively,whereas the Programmers Assistants suggestions are generatedupon request. When copying generated code from the assistant,participants most often copied the entirety of the generated code,and only in 5.8% of cases did they copy a smaller portion of it.5.2.2 User Experience Expectations & Changed Attitudes. Prior torunning our study, we had reason to believe that participants wouldbe skeptical of a conversational programming assistant. Before de-veloping the Programmers Assistant, we showed potential usersmockups of a program editor with an integrated chatbot feature.These prototypes elicited uniformly negative reactions. People toldus about their frustrating experiences with conventional chatbotsand raised doubts about the knowledge, capabilities, and valueof a conversational programming assistant. This skepticism moti-vated us to develop the Programmers Assistant in order to evaluatewhether the conversational experience, as powered by a state-of-the-art code-fluent LLM, would be better than people had antici-pated. During pilot testing, we received feedback that the Program-mers Assistant provided a much better conversational experiencecompared to testers previous experiences with chatbots. Thus, indesigning our study, we felt it important to first gauge participantsexpectations of a conversational interaction around code, and thenmeasure their experience after the fact.14This time includes additional time added by our proxy server to ensure our confor-mance to the API rate limitation.499IUI 23, March 2731, 2023, Sydney, NSW, Australia Ross et al.We developed a short inventory of six scale items to measure userexperience of code work15. The scale was administered twice: oncebefore participants were exposed to the Programmers Assistant(but after they had been briefed that they would interact with an AIchatbot), and once after completing the programming challenges.The items were presented with the appropriate tense: Do you expect(Did you find that) the Programmers Assistant: (a) will be (was)easy to use; (b) will understand (understood) your requests; (c) willprovide (provided) high quality responses; (d) will help (helped)you to write better code; (e) will help (helped) you to write codemore quickly; (f) will be (was) enjoyable to use. Each item was ratedon a 4-point scale of extent: Not at all (1), A little (2), Somewhat (3),A great deal (4).A factor analysis revealed the items on this scale measured asingle construct, which we identify as user experience (Cronbachs= 0.87). Thus, we computed two scores of user experience (UX)for each participant: a pre-task UX score computed as the averageof their six pre-task expectation scale responses, and a post-taskUX score computed as the average of their six post-task experiencescale responses.We found that participants had lower initial expectations fortheir experience with a conversational programming assistant (pre-task UX M (SD) = 3.0 (0.62) of 4) than their experience actuallywas (post-task UX M (SD) = 3.6 (0.32) of 4). A paired sample t-testshows that this difference was significant, (41)=5.94,<.001,Cohens =0.92(large). Measured another way, 32 participants(76.2%) had post-task UX ratings that were higher than their pre-task expectations, demonstrating a significant shift in attitudestoward conversational programming assistance.However, the UX ratings alone fail to capture participants nu-anced expectations of the assistant and the reasons for their shiftedattitudes after using it. Participants expressed a variety of expecta-tions of the assistant before using it, including that it would be easyto use (P30) and produce correct responses (P30), understand theproblem and what is being asked of it (P8, P9, P11), not interferewith their flow state (P5), produce imperfect or questionable out-puts (P6, P21), improve with feedback (P31), provide generic andunhelpful answers (P17) or only answer basic questions (P40), andproduce responses quickly (P40).P17 expected to be frustrated very quickly and that what Id thinkwould be relatively common questions would be responded to withgeneric, unhelpful answers.  P6 explained, I didnt have very goodexperiences with chatbots. I think Ill need to spend more time inreviewing and fixing the suggestions than in writing the code myselffrom scratch.  P11 had a more balanced view, that Itll do some tasksreally well, but others will not be as reliable. After interacting with the Programmers Assistant, many par-ticipants commented on how the experience was better than theyanticipated, because it seemed to be able to handle complex issues(P10) and was a great help (P8). P20 felt it was incredible! P6 andP17, who were both initially skeptical, reported having a positiveexperience. For P6, It absolutely exceeded all my expectations, in15Our scale items were modeled from scales published in Weisz et al . [103 , Table 9 AI Support ]that measured constructs including ease of use (item 3), response quality(item 1), the production of higher-quality code (item 5), and the ability to write codemore rapidly (item 4). We added additional items to cover the constructs of requestunderstanding and enjoyment, and we cast all items on a 4-point scale of extent.all aspects that I could have imagined and more! P17 provided amore quantitative assessment: Initial expectations: 3 Actual: 9.5. P38 was emphatic in their evaluation: I was blown away how wellit allowing me to structure how I want the code to look and work andjust giving me the thing I asked for. Many participants described a sense of surprise in their experi-ences. P9 was surprised by how well it understood their requests:I was surprised at how well the Programmer Assistantwas able to understand my requests and generate goodcode/documentation/tests. It understood major conceptsand was able to explain it to me in a clear way, and itwas also able to understand and write functional code.It even was able to help me review my answer. I was alsosurprised at how well it could understand the contextof what I was asking in follow-up questions when I didnot specify exactly what I was talking about, but ratherreferencing our prior conversation (such as, what doesthat mean).  (P9)Similarly, P6 was surprised that they liked the conversationalinteraction when they expected that they wouldnt:I though[t] I wouldnt like the chatbot interaction andthat I would prefer something like the tool Ive seen inthose demos [of Copilot]. But surprisingly, after usingthe chatbot (and seeing the results: easy to use, it un-derstands well, I felt it like a partner) I like this kind ofhelp.  (P6)5.2.3 Quality of Assistants Responses. In order to gauge the qualityof responses produced by the Programmers Assistant, we examinedthe 910 task-oriented requests made by participants in the study.For the vast majority (80.2%), the assistant produced a correct re-sponse (Grants Request (Complete)); in other cases, the assistantsresponse was incorrect (9.6%; Provided Wrong Answer), correct butincomplete (4.4%; Grants Request (Incomplete)), or the assistantdidnt understand (3.4%; Didnt Understand), claimed ignorance ofthe subject (1.5%; Claims Ignorance), or produced another type ofresponse (0.9%; Appears Fixated, Spews Garbage).Participants also reported experiencing this variability in thequality of the assistants responses. Some participants describedhow the assistant provided detailed answers (P17) and high qual-ity outputs (P18) that were surprisingly good (P2). P6 felt it wasincredible to see the quality of the responses,  and P3 even exploredthe assistants capabilities outside the scope of the challenges andfound that it could handle those as well:It was surprising the quality of the code and the abilityto answer all my questions correctly. Although I thinkthe challenges may be biased towards what the Assistantis able to do, it was a great experience because I askedmany other things and it was able to answer correctly. (P3)Of course, the Programmers Assistant wasnt perfect, and someparticipants did run into issues. For P35, The documentation gener-ation did not perform very well.  P16 questioned the accuracy of theknowledge encoded in the model: Does the model need to be up-dated? It said latest python version is 3.7 but google says its 3.10.  Insome instances, participants needed to ask their question multiple500The Programmers Assistant IUI 23, March 2731, 2023, Sydney, NSW, Australiatimes to get a good response: you need to ask many times if youwant to get an answer and also a detailed explanation (P3). P27 felt,it was annoying when I asked it to try again and it would give methe same response.  P22 struggled because, It didnt seem to handlemultiple sentences well. P28 perhaps offered the most scathing criticism, that, It makesmistakes often enough to be not very practical.  However, despitethe production of poorer-quality responses, other participants feltthat the assistant was still helpful. P36 reported that, Only minortweaks were normally needed to correct any issues. Similarly, P38described how the assistant wasnt able to completely solve theirproblem, but provided a useful start:There was only one hickup I noticed where when Iasked it to memoize fibonacci it couldnt, but it droppedthe building blocks on my lap for me to finish so thatwas fine, that was like minutes of effort on my part.(P38)5.2.4 UI Design & Affordances. Participants made many commentson our specific UI design and the affordances provided (or notprovided) in our chat-augmented editor. Overall, the integrationbetween the chat pane and the code editor was very good (P23),with a nice interface between the code pane and the assistant pane(P17) that makes it really convenient (P35).Prior research by Brandt et al . [15] has shown how keepingdevelopers focused in their IDE improves productivity, and ourparticipants expressed similar sentiments. P40 remarked, It allowsme to stay in one browser window/tab! and P12 hinted at how theinterface might preserve their flow state by prevent[ing] me fromgetting distracted when looking into an issue in another tab. Some aspects of our user interface were confusing to participants,such as the mechanism for selecting code to be included in theconversational context. P7 remarked, Its was a little confusingdoing the selection part for it to tell me what a function does, but...it gave me code that was insanely easy to copy and paste.  Otherparticipants appreciated the code selection mechanism, such as P11:I enjoyed the code selection feature, and found that very easy to use. In the event logs, we identified 20 instances in which a participantunintentionally included selected code in the conversation whenit wasnt needed (Includes Extraneous Selection), 12 instances inwhich a code selection was omitted when it was needed to providecontext for the question (Missing Selection), and 16 instances inwhich a participant copy/pasted code directly into the chat ratherthan selecting it in the editor (Pasted Code in Chat). Althoughthese cases represent a small fraction of the 227 instances in whicha code selection was required and included in the conversation(Includes Selection), their presence does indicate that more attentionis needed to the interaction design of code selection.Another issue regarded the awareness of the try again andstart over features. The try again feature was only used by 14participants, who used it a total of 63 times over the course ofthe study. Some participants used it specifically when they got ananswer which they saw as clearly wrong, while others used it toget a variety of possible answers before proceeding. The start overfeature was used even less, by 5 participants who used it a total of6 times. Despite our effort to surface these conversational featuresin the UI via shortcut buttons, they may not have been sufficientlynoticeable or salient: The try again button is not so reachable, oftentimes I forgot it exists (P23). By contrast, at least one participantwas successful with these features:at some point it had issue with challenge 3 and I had tostart over. Just asking try again was not enough and Iwas getting always the same (wrong and not related)answer. starting again solved the issue! (P20)5.3 Utility of Conversational AssistanceOur next set of themes concerns the utility provided by conversa-tional programming assistance. Participants felt the assistant washighly valuable and desired to use it in their own work. They felt itwould be most helpful for smaller or narrowly-scoped tasks, butable to provide a wide variety of types of assistance. The fact thatthe interaction model was conversational and grounded in codewere valuable aspects, as was the ability for the assistant to bolsterusers learning about programming topics through that interac-tion. Participants did question whether they could trust and relyupon the assistants responses, echoing a similar theme discussedin Weisz et al. [102].5.3.1 Value & Appropriate Tasks. Participants rated the value ofthe Programmers Assistant highly (M (SD) = 8.6 (1.4) of 10). Manyparticipants asked questions such as, Can I have it in my editorplease? (P15), or made comments that, I would enjoy using it inthe future (P36), I would love to be able to... have access to it formy coding (P37), and Id love to use this tool as part of my usualprogramming workflow if I could! (P39). Some of the reasons whyparticipants found it valuable are because it help[s] me rememberhow to do things in certain languages that normally I would justGoogle (P9) and It helps me to avoid silly syntax errors and canwhen I cannot remember exact function/method names and requiredarguments (P40). We did not observe any differences in valueratings based on participants familiarity with or recency of usingPython.Participants described a wide variety of tasks for which theyfelt the assistant would be useful. These tasks included ordinary(P23), simpler (P2), and small, repetitive (P4) tasks such as quicklookups (P25) for short chunks of code (P11) or for narrowedquestions (P26). Participants also felt the assistant was useful forsmall containable novel algorithms (P38) and little coding problems(P4).Several kinds of task assistance were reported as being valuable,such as explaining code (P31), implementing business logic in aUI (P38), understanding what code does (P19, P37), and recallinglanguage syntax, method names, and arguments (P12, P15, P20, P40,P42). P27 felt that the assistant was More helpful when recognizinga specific well known algorithm but not things you make yourself. Participants also made recommendations for how to increasethe value of the Programmers Assistant. P38 suggested, Whatwould blow me away though is if its able to help with what I do mostoften which is to integrate, refactor and iterate on an existing system. P16, P26, and P38 all desired more information on the data sourcesused to produce the assistants responses. P9 requested to Havethe Programmers Assistant examine your code and make proactivesuggestions for improving it in the chat.  P36 requested the same,501IUI 23, March 2731, 2023, Sydney, NSW, Australia Ross et al.but cautioned that, Care would need to be taken to avoid becomingan annoyance or disrupting the flow of a coding session. In the post-task survey, we probed participants on how certainchanges to the Programmers Assistant would either decrease, in-crease, or result in no change to its value. Over 75% of participantsfelt that the assistant would be more valuable if it operated in aproactive manner, either by making improvement suggestions inthe chat or as comments directly in the code. Similarly, 78.6% ofparticipants felt that having more buttons in the UI for commonfeatures such as explaining or documenting code would make thetool more valuable.5.3.2 Conversational Interactions Grounded in Code. One of thechallenges in interpreting participants comments about the utilityof the Programmers Assistant was in disentangling the extent towhich value was derived from the quality of the underlying modelversus the integration of conversation in a code context. Indeed,participants felt that the chat interaction was valuable: 69.0% ofparticipants felt that eliminating the conversational interactionand making the assistant behave more like web search would de-crease its value. Further, our analysis of the conversation transcriptsrevealed that 42% of the 910 task-oriented utterances from partic-ipants required historical conversational context (Chat ContextRequired) in order to be correctly interpreted. Thus, we observethat behaviorally, participants did rely on conversational contextin their interactions.In the post-task survey, 83% of participants rated the importanceof the ability to ask follow-up questions as being somewhat or agreat deal.  Several participants specifically commented on the valueof this conversational context. P39 remarked, I absolutely lovedhow you can straight up ask follow-up questions to the ProgrammersAssistant without having to reiterate the original topic/question.  P15expressed a similar sentiment, saying, I think the conversationalcontext was someone helpful, just in communicating that its a runningconversation where my context is remembered.  P9 provided a similaranalysis:This tool was so helpful at answering questions I hadabout the code in the context of the code I am workingon... I was also impressed with how well it was able toremember the context of our conversation, especiallywhen I asked vague follow-up questions.  (P9)In addition, some participants identified how a conversationalinteraction grounded in code was useful, because I think to un-derstand the dev context could be VERY important (P31). In fact,24.9% of task-oriented utterances included a relevant code selection(Includes Selection), showing that participants valued this ability.Contrasting with these participants, P18 felt that interactingwith the assistant conversationally was tedious, and they employeda more direct approach:I really like the PA. But, I didnt converse with it likea chat bot. I often told it what to do (Document thiscode. ) as opposed to asking it what to do (How do Idocument this code?). Talking to it the way that wassuggested in the tutorial seemed overly verbose/tedious. (P18)Despite these individual differences in interaction preferences,P39 envisioned that both interaction styles could be supported inthe tool:I think both options should exist: people should be ableto input their queries like a search bar AND also givetheir question as if in conversation.  (P39)5.3.3 Learning Effects. One specific benefit of the ProgrammersAssistant identified by participants is its ability to help peopleimprove their programming skills and reinforce knowledge gaps.For example, it can help users remember how to do things in certainlanguages... such as, when I am using a language I havent used in awhile (P9). The assistant can also serve as an memory aid, suchas when I use a lot of libraries that I dont always remember allof the functions (P15). Similarly, P31 said, No matter how goodyoure as a developer, you cant (humanly) remember all the API ofhundreds of libs or new languages... Id learn new dev lang and newlib/frameworks faster. P39 felt the assistant is perfect for programmers of all levels,  andP1 felt it could help them rapidly improve their Python skills:I have wanted to learn python... The main concernhow much time spent learning is needed before I couldactually get some value out of learning python. I havea feeling this would cut that time down from weeks toa day or so.  (P1)P39 also identified the fact that, because the interactions withthe assistant are conversational, it forces people to learn how tocommunicate to others about their code:The conversation aspect promotes proper communi-cation, which would really stand to benefit buddingprogrammers if they want to learn how to explain con-cepts more fluently in the future to their colleagues.(P39)Conversely, P36 suggested that over-reliance on programmingassistance might have a detrimental effect to ones learning:Its definitely a huge time saver, but over-reliance on itmay cause new developers to skip learning the referencematerial themselves and discovering new things andsparking new ideas.  (P36)5.3.4 Trust. Many participants raised questions about whetherthey could trust the responses provided by the Programmers Assis-tant. P21 asked this question most directly: will the code be correct,safe, efficient? Other participants raised similar questions, such as,Im wondering how it validates its answers, if it can be trusted toalways give a working answer (P10), and Sometimes lack of sourceand context may raise doubts in the mind of the programmer (P16).These issues of trust were exacerbated by the fact that the Pro-grammers Assistant did not allow participants to actually run theircode. Because of this limitation, participants had to rely on theirown knowledge to judge the correctness of the assistants responses.P19 asserted, There is no way to evaluate if the Programmers as-sistant is giving you the right advise or not other than your ownknowledge,  and P9 concurred: I had to trust that it was correct (anduse my own prior knowledge). 502The Programmers Assistant IUI 23, March 2731, 2023, Sydney, NSW, AustraliaP18 described the potential consequences of allowing the assis-tant to write code for them:The only thing that made me nervous was that it couldhave introduced a bug that wasnt immediately appar-ent. And given I didnt write the code, I could have easilyglossed over a mistake when reviewing it. Especially ifit is also the one writing the test cases.  (P18)Despite our efforts to make the Programmers Assistant respondin non-authoritative ways, we did observe participants sometimesuncritically accept generated results that were clearly wrong orincomplete. Thus, we did find behavioral evidence for over-reliance.Listing 2: Building trust through explanations and justifica-tions1 P9: what is a unit test ?23 Socrates : A unit test is a test of a single function4 or method . It is typically written by the5 programmer who wrote the code .67 P9:8 def euclid(a, b):9 while b != 0:10 (a, b) = (b, a % b)11 return a1213 how would I write a unit test for this code ?1415 [...]1617 Socrates : I think you would do something like this :1819 def test_euclid():20 assert euclid(2, 4) == 221 assert euclid(6, 9) == 322 assert euclid(17, 12) == 123 assert euclid(4, 4) == 42425 P9: what is a euclid function2627 Socrates : The euclid function is a function that28 computes the greatest common divisor of two29 numbers .3031 P9: is this a good unit test for a euclid32 function3334 Socrates : I believe that it is a good unit test for35 a euclid function .3637 P9: why3839 Socrates : It seems to me that it tests the edge40 cases of a and b being equal , a being41 greater than b, and b being greater than a.4243 [...]4445 P9: thanks Socrates this was helpful4647 Socrates : Always happy to help .One way to address trust issues is for the assistant to providefurther explanations and justifications that can calibrate a usersconfidence in the assistants responses. Such explanations couldbe requested conversationally, though most participants did notattempt to do so. One participant (P9) did ask for such explanations,and we show a summary of their transcript in Listing 2. In thisinstance, P9 asked for a definition of a unit test (line 1), an expla-nation of the code being tested (line 25), and justifications of thequality of the unit test (lines 31& 37). Thus, we observe that theassistant is capable of producing explanations and justificationswhen asked.5.4 Patterns of Interaction and Mental ModelsParticipants interacted with the assistant in a variety of ways withtwo main patterns of usage standing out: (1) invoking the assis-tant to solve the entire programming challenge, and (2) breakingthe challenge down into a set of smaller tasks and invoking theassistants help for each. There were no clear differences in howparticipants with differing Python experience approached the tasks.Participants mental models of the assistant also varied. Althoughparticipants strongly saw the role of the assistant as being a tool,their behaviors revealed that in many cases, they actually treated itas a social agent. In addition, participants ascribed various mentalcapacities to the assistant, such as having the ability to understand,compute, and learn.Participants felt the assistant changed the nature of their workprocess. For some participants, it enabled them to focus on thehigher-level aspects of development because the assistant handledlower-level details or provided partial solutions for them to buildupon. Many participants felt the assistant sped up their work andhelped them remain focused on their tasks.Finally, participants drew comparisons between the Program-mers Assistant with other forms of programming support suchas Copilot and web search. They felt that the conversational styleof interaction enabled them to discover new, emergent behaviorsfrom the model that were unavailable from Copilots focus on codeautocompletion. They also felt that the examples provided by theassistant were more readily usable within their own code comparedto browsing for answers within search results, speeding up thecoding process. However, some participants advocated for a bal-anced approach to the design of programming assistance tools byincorporating multiple modes of interaction rather than fixatingon a single one.5.4.1 Interaction Styles and Assistant Role. We observed that par-ticipants interacted with the Programmers Assistant in strikinglydifferent ways. Some participants would present the entire chal-lenge description to the assistant and then work with the results itproduced. Other participants approached the programming chal-lenges in a piecemeal fashion, breaking them apart into a set ofsmaller tasks, then invoking the assistant to aid with each one.Experience with Python was not a determinant of how partici-pants approached the programming challenges, but it did seem toimpact how participants interacted with the assistant. Less experi-enced participants tended to ask the assistant basic questions suchas,What is a unit test (P29, not familiar with Python) and howdo I document a function? (P27, < 1 year of experience). More expe-rienced participants made detailed requests about specific Pythonlibraries or algorithms, such as, given a pandas dataframe withtwo columns Date and Sales please use matplotlib to draw me ascatterplot (P38, 3+ years of experience) and implement a runge-kutta algorithm for solving an ODE with adaptive time steps (P37,3+ years of experience).Another difference we observed in how people interacted withthe assistant stemmed from their view on the role it played in their503IUI 23, March 2731, 2023, Sydney, NSW, Australia Ross et al.collaborative process. Some participants, such as P18, treated itmore as a tool by issuing commands rather than asking questions.As quoted earlier, they said, I didnt converse with it like a chatbot.  P5 described their interaction style similarly: I found myselfwanting to type search queries into Socrates, not treating it as a personbut as a search tool. In anticipation that participants would have different orienta-tions to the assistant and its role, we asked a question on the post-task survey about the different kinds of roles the assistant mighttake. These roles generally fell into one of two categories: a toolorientation (a tool, a reference guide, a content generator, a problemsolver), and a social orientation (a collaborator, a colleague, a coach,an advisor, a reviewer). Participants rated the extent to which theyviewed the Programmers Assistant in each of these roles on a 4-point scale of extent: Not at all (1), A little (2), Somewhat (3), or Agreat deal (4).0255075100Tool ContentgeneratorReferenceguideCollaborator ProblemsolverAdvisor Coach Reviewer ColleagueRolePercentage of participantsRating Not at all A little Somewhat A great dealFigure 3: Role orientations. Participants overwhelmingly feltthat the assistants role was of a tool orientation rather thana social orientation. The chart shows ratings distributionsacross 9 different roles the assistant might take, sorted bythe percentage of participants who rated the extent of thatrole as A great deal. The leftmost role is of a tool, with80.9% of participants rating it as a great deal. Followingtool are content generator (69.0%), reference guide (64.3%),collaborator (40.5%), problem solver (35.7%), advisor (21.4%),coach (19.0%), reviewer (16.7%), and colleague (14.3%).We show participants ratings of the assistants role in Figure 3.Despite the fact that their attitudes toward the assistant overwhelm-ingly reflected a tool orientation, their behaviors reveal that manyparticipants actually treated the assistant as a social agent. P6 de-scribed how I felt it like a partner, and P4 told the assistant, Icould not have solved [the challenge] without your help, to whichthe assistant responded, Im glad I could help. The literature on Computers as Social Agents (CASA) helps usinterpret this result as it demonstrates how computers are oftentreated like people [ 56,67]. LLM-based conversational agents canexacerbate this tendency; as they likely have been trained on ex-amples of social interaction, they can also respond as social agents.In the conversation logs, we identified participants who inter-acted with the assistant in a socially-oriented fashion (the socialorientation codes in Table 1). Twenty participants (47.6%) made atleast one socially-oriented utterance. An extreme form of this inter-action style can be seen in a snippet from P6s transcript (Listing 3).The 20 participants with a behaviorally-demonstrated socialorientation did not generally differ in their role ratings from otherparticipants, except that they rated the assistant as more likely tobe an advisor (Fishers exact test, two-tailed =.02) or a reviewer(Fishers exact test, two-tailed =.03). However, they did not differin their overwhelmingly-strong ratings of the tool orientations.Thus, at least for some participants, there seems to be a dissonancein their view of the assistants role orientation.Listing 3: Excerpt from P6s interaction with the Program-mers Assistant, in which P6 offers their thanks and congrat-ulations.1 P6: thank you ! it 's been really nice to code2 with you ! or to have you coding ! LOL3 Socrates : You 're welcome !4 P6: I hope to see you on air soon , Socrates !5 Socrates : I hope so too .6 P6: congrats to you research team ! you 've made7 a good job !8 Socrates : I 'm glad you think so.9 P6: sorry ... they 've made a good job ( well ...10 you too ... ;-) )11 Socrates : Thank you .12 P6: see you next time ! bye !!13 Socrates : Goodbye .5.4.2 Mental Capacities. Participants made a number of inferencesabout the Programmers Assistant and its capacities for thought.Many participants talked about how the assistant possessed a levelofunderstanding (P6, P8, P11, P18, P32) of the context (P9, P21)as well as major concepts (P9) and knowledge (P33). P24 wasamazed by the assistants ability to take a plain english requestand interpret it properly.  P7 ascribed intelligence to the assistant,saying, It was a lot smarter and trained tha[n] I thought it was. One participant assumed that the assistant Keeps improvingthrough (user) feedback (P31). Another felt that the assistant wascapable of computation: It understands the problem... It can calculatethe results of a function back (P8).However, not all participants were convinced of the assistantsability to understand. P37 questioned the assistants limitations: Iwonder how far beyond boilerplate it can go and if it works for trulyoriginal problems. 5.4.3 Impact of Conversational Assistance on Work Practices. Manyparticipants discussed how the Programmers Assistant shapedtheir work practices on the programming challenges. Overall, par-ticipants felt that the assistant saves time (P10), helps me codefaster (P34), and would speed up my productivity (P19) because Icould focus on validating and improving the code it generated insteadof having to write it all from scratch (P18). P37 remarked that, Itopens a whole new door for fast develpment.  P4 discussed how theassistant was helpful in staying focused on the code,  although forP14, it took [me] time to get into tempo with the tool. P31 pointed out how the assistant would change the nature oftheir work:My job could focus more on higher level aspects andtherefore achieving better (quality) results, besides the504The Programmers Assistant IUI 23, March 2731, 2023, Sydney, NSW, Australiatime-to-value... Data science (and dev) becomes a morecreative-higher level experience.  (P31)Other participants discussed a work process in which the assis-tant provided incomplete solutions  the building blocks (P38) orinitial draft of code (P11)  upon which they could build. P5 aptlydescribed this process:Its nice to copy well formulated challenges in naturallanguage and have the code generator take its best stabat it, then edit to our hearts content.  (P5)Participants felt that human review of the assistants responseswas necessary because The answers provided are generally notnovel solutions, often look clunky and non-elegant. There may besome unnecessary code. Basically the code would need to be reviewed(P16). P35 also pointed out how The code generator was good butyou still have to really check it.  P19 discussed how they would turnto the assistant as a first source for support, and only if it wasntable to help would they then turn to other support tools:The way I will use it is, I will first us[e] the Program-mers assistant for most of my cases. Only in certaincases where Programmers assistant cant answer thingsI will turn up to official documentation or stack overflow. (P19)However, latency was a factor for interactive use of the assistantand participants noticed when the assistant took a long time torespond. P19 remarked, Sometimes it took lot of time, like more than5 seconds.  P40 also felt the response [was] a little slow sometimes...in chat mode I expect faster responses.  As discussed in Section 5.2.1,the assistant took an average of 6.7 seconds (SD = 3.1 seconds)to respond to a request, and participants did appreciate when theassistant produced rapid responses: I loved how quick it was ableto pull up answers to questions I had (P38).5.4.4 Conversational Interaction vs. Other Interaction Models. Al-though our study was not intended to make comparative evalu-ations with the Copilot tool, we nonetheless asked participantswhether they were familiar with Copilot, and if so, to comment onhow the two tools compared. We also asked a similar question tocompare the assistant with another popular form of programmingassistance, searching the web (via a search engine like Google, ora Q&A site like Stack Overflow). In discussing the differences be-tween these three tools, we note that the primary differentiator istheir interaction model.The interaction model for the Programmers Assistant is clearlyconversational : users ask questions in natural language and areprovided with a response in natural language and/or code. Theinteraction model of Copilot is reminiscent of direct manipulationinterfaces [ 37], in which the users actions in the user interfacedirectly manipulate an object on the screen. Copilot automaticallymakes autocompletion suggestions as the user types. This auto-completed code is directly placed in the source editor; thus, theusers work is contained entirely within the scope of the objecton which they are working (i.e. the source code), which is howdirect manipulation interfaces operate. In web search, users entera separate search context (e.g. a search engine accessed within aweb browser), type in a natural language query, and then forageamongst search results to identify relevant items of interest [ 12,62].When a desirable item is found, users must translate it into theircode environment (e.g. via copy/paste) and possibly edit it to fittheir existing code.We also note that the Programmers Assistant and Copilot bothutilize the same underlying AI model, Codex [ 24], which meansthat the only difference between these tools is the user experience.The extent to which Codex was trained on data from programming-related Q&A web sites is less clear, but for the purposes of ouranalysis, we focus our discussion solely on the differences in theirinteraction models16.Participants reported various benefits and drawbacks of a con-versational interaction over a direct manipulation interaction. Fore-most, conversation felt very natural (P21) and feels much morenatural using Natural Language with the AI (P39). In addition, P39felt that the use cases of Programmers Assistant seem more open-ended.  Many participants were surprised at the variety of tasks theassistant was capable of performing, from writing unit tests (P19,P36, P37) and documentation (P12, P19, P36, P37) to explainingwhat code did (P31, P38) and even answering general-knowledgequestions (P31). Again, we note that the Programmers Assistantutilizes the same underlying model as Copilot, yet the conversa-tional interface was able to expose a wider variety of emergentbehaviors from the model. Multiple participants explored the limitsof the assistants knowledge and abilities beyond our programmingchallenges. For example, P37 asked it questions about physics andordinary differential equations (ODe as written by P37), and wassurprised by the versatility of what it could answer. I asked it some physics and ODe question and theanswers, though not complete, included the key partsneeded to write that code.  (P37)P31 probed the assistant on its knowledge of geography and wassurprised when the assistant produced a correct answer.I asked something out of SW engineering domain (ge-ography) and it replied correctly, also by correctly an-swering on my nationality.  (P31)For some participants, the ability to assess the assistants re-sponse before committing to it (i.e. by inserting assistant-generatedcode into their editor) was a boon. P15 described how the copy/-paste boundary provided them with a bit more control to ask specificquestions about what I wanted and to assess before putting it in mycode.  Other participants felt that the copy/paste boundary wasmore inefficient:I think the main difference is the ability of Copilot tosuggest code while you type, what make it faster andeasier to use. While using the Programmers Assistant,you need to go to the chat, ask the question, copy the16As an aside, our comparison of direct manipulation, search, and conversational inter-action models is reminiscent of historical comparisons of text-based and graphical userinterfaces [ 54,63]. Each modality was shown to have advantages and disadvantages.For example, text-only interfaces can provide accessibility [ 54,78] and productivity [ 53]advantages, whereas graphical user interfaces provide greater discoverability [ 10,88].Some researchers explored ways to bridge the two interaction modes, such as bydeveloping GUI wrappers for command line programs [ 53,94] or by developing toolsthat converted GUI activities into procedural descriptions [ 55]. Our view is that similarbridges can be constructed between direct manipulation, search, and conversationalmodels of interaction; a users interaction with an LLM need not be constrained to asingle interaction model.505IUI 23, March 2731, 2023, Sydney, NSW, Australia Ross et al.code (or rephrase the question if it was not understoodby the agent), and edit it to match your code.  (P3)A large number of participants felt that the conversational inter-action was faster than web search (P1, P6, P7, P10, P11, P12, P16,P17, P18, P20, P24, P29, P30, P33, P36, P37, P42) because of its abilityto provide real-time responses (P32) that can be applied exactly toyour code (P33) without having to parse through lots of text... to getwhat you need (P15). In addition, the assistant provided MUCHfaster, better responses (P17) that were much more relevant to theproblems (P34) and simple [and] succinct (P9), without having tosort through answers on your own or read documentation (P9) orlook at many posts before finding the relevant one (P18).Despite these benefits, some participants felt that the assistantmight not work well for more specific and difficult problems ona bigger scale as compared to web search. P9 felt that the data[of the Programmers Assistant] wasnt as rich as the web. Otherparticipants felt that the assistant lacked the multiple answers (P9)andrich social commentary (P19) that accompanies answers onQ&A sites:I like to see the different versions proposed on stackoverflow and the commentary of what makes one solu-tion better than another in a given situation.  (P27)Some participants promoted a more balanced view that thereisnt a single mode of interaction superior to all others. P19 felt thatweb search would be a fallback when the assistant failed to answera question. P39 described how search could be integrated with theconversational interaction:I think both options should exist: people should be ableto input their queries like a search bar AND also givetheir question as if in conversation.  (P39)6 DISCUSSION6.1 Value of Conversational InteractionWe began our research by asking the question of whether con-temporary developments in code-fluent LLMs could sufficientlysupport a conversational programming assistant. We believe thatour work has demonstrated that they can. Clearly, the Program-mers Assistant was viewed by our participants as a useful tool thatprovided real value  so much so that many participants explic-itly requested or expressed the desire to use it in their own work.However, how much of this value was derived from the model itselfand its ability to produce high-quality responses to programmingquestions, versus from participants ability to conduct extendedconversational interactions grounded in their actual source code?We believe that both of these constituent aspects were valuable.Indeed, many participants commented on their surprise and satis-faction with the quality of the assistants responses (Section 5.2.3).However, participants also valued the conversational interactionsthat they had with the assistant. In the event logs, we saw evi-dence that participants were leveraging conversational context toask follow-up questions as well as leveraging code context by ask-ing about their code selections (Section 5.3.2). Many participantsreported that they would find the tool less valuable if the conver-sational interaction were removed (Section 5.3.2). Further, conver-sation seemed to provide unique value beyond other interactionmodels (direct manipulation and search) because of its embedded-ness in the UI and its ability to surface emergent behaviors of themodel (Section 5.4.4).We do not believe that these different interaction models arein competition and we agree with P39s assessment that assistivetools can be built using a plethora of different interaction models.For use cases in which a model is known to produce high-qualityresults (e.g. code autocompletion for Codex), a direct manipulationinterface seems wholly appropriate as it would provide a discov-erable and predictable way of invoking the model to produce aknown type of result. However, direct manipulation interfaces maybe less ideal for surfacing the emergent behaviors of a foundationmodel [ 14], and thus natural language interaction may be more suit-able. Many popular text-to-image models, such as DALL-E 2 [ 66]and Stable Diffusion [ 72], operate in a one-shot fashion, in whichthe user specifies a prompt, clicks a button, and gets results. Ourstudy demonstrates how the additional contextual layers of con-versational history and the artifact-under-development provideadditional value to the co-creative process.6.2 Toward Human-AI SynergyThe aim of human-centered AI is to enable[] people to see, think,create, and act in extraordinary ways, by combining potent userexperiences with embedded AI methods to support services thatusers want [ 82]. Building upon this definition, Rezwana and Maher[69] posit that, In a creative collaboration, interaction dynamics,such as turn-taking, contribution type, and communication, are thedriving forces of the co-creative process. Therefore the interactionmodel is a critical and essential component for effective co-creativesystems. [ 69]. They go on to note that, There is relatively littleresearch about interaction design in the co-creativity field, whichis reflected in a lack of focus on interaction design in many existingco-creative systems.Our study begins to address this gap. While many co-creativesystems examine casual tasks or experimental activities (e.g., Spotoand Oleynik [87]), our focus was on the co-creative practice ofprogramming. Our goal was to understand peoples attitudes to-ward a conversational programming assistant, akin to Wang et al .sexamination of data scientists attitudes toward automated datascience technologies [ 99]. We found that, despite an initial level ofskepticism, participants felt that a conversational assistant wouldprovide value by improving their productivity (Section 5.4.3). How-ever, further work is needed to assess the extent to which this typeof assistance provides measurable productivity increases.Campero et al . [19] conducted a survey of papers published in2021 that examined human-AI synergy , the notion that a human-AIteam can accomplish more by working together than either partycould accomplish working alone. They found mixed results, withno clear consensus emerging on how to design human-centered AIsystems that can guarantee positive synergy. Summarizing fromtheir discussion,Perhaps achieving substantial synergies among peopleand computers is harder than many people think. Per-haps it requires... new ways of configuring groups thatinclude people and computers. And perhaps it needs506The Programmers Assistant IUI 23, March 2731, 2023, Sydney, NSW, Australiamore systematic, focused attention from researchersthan it has, so far, received.  [19, p.9]We believe such evaluations of human-AI synergy should gobeyond one-shot performance measures. As implied by many of theuses cases listed by Seeber et al . [80] , human-centered AI systemsare often deployed in socio-organizational contexts that requirelongitudinal use [ 20,41,43], such as product design [ 93], game de-sign [ 4], and engineering [ 20, Section 3.2.2]. Thus, we would expectthat over time and through interaction with each other, human-AIteams would improve their performance through a mutual learningprocess.Evidence for this process surfaced in our study when participantsdescribed how they could improve their programming skills byinteracting with the assistant (Section 5.3.3). We assert that thelearning should operate in both directions: not only should peopleimprove their programming skills, but the model itself can alsoimprove based on peoples interactions with it. For example, whenthe assistant provides a code example to the user, and the usertakes that example and edits it, those edits constitute feedback thatcan be used to further fine-tune the model. In addition, throughlongitudinal use, we believe that human and AI partners can createreciprocal representations of one another  i.e., the human is likelyto create a mental model of the AI, and the AI may be engineered todevelop a user model for each of its human users [ 30,48,79]. Such apair of models is often described as Mutual Theory of Mind [ 29,100].This type of capability raises the possibility of personalizing andadapting an assistant to the strengths and needs of individual users.With such models, an assistant that knows a user is learning aprogramming language could provide natural language explana-tions alongside code outputs, whereas an assistant that knows auser is strongly skilled in a programming language might shortenor omit those explanations. Similarly, users are likely to updatetheir mental models of the AI with more experience. We believe thespace for exploring how these reciprocal models impact human-AIsynergy is rich, and we encourage additional work in this area.Human-centered AI systems that are designed to combine andsynergize the distinct skills of humans and AI models cannot suc-ceed if they diminish the human skills upon which they depend.Well-designed human-centered AI systems develop new and com-plementary skills for both the human and AI constituents [ 82,83],and we believe that mutual learning may address concerns that thewide deployment and use of AI systems will result in a de-skillingof the workforce [77, 108].Ultimately, the design decisions that go into an interactive AIsystem have ethical implications. Our design attempts to augmentthe users knowledge and skills by presenting help on demand,couched in non-authoritative suggestions , which leaves the userfirmly in control and ultimately responsible for the work product.6.3 Opportunities for Future ResearchOur work highlights many interesting avenues for future enhance-ments that could be made to LLM-based conversational assistantssuch as our Programmers Assistant, as well as future human-centered research on LLM-based conversational assistance.Our work employed a code-fluent model that was not specifi-cally designed to handle conversational interaction. Fine-tuningthe underlying LLM for conversational interaction, such as whathas been done with Lamda [ 91], is one opportunity to improve theassistants performance. Another opportunity is to align the lan-guage model to follow the desiderata proposed by Askell et al . [11]and described by Ouyang et al .as, helpful (they should help theuser solve their task), honest (they shouldnt fabricate informationor mislead the user), and harmless (they should not cause physical,psychological, or social harm to people or the environment) [ 61,p.2]. Glaese et al . [33] propose a slightly different desiderata ofcorrect instead of honest, which may be more applicable to thesoftware engineering domain, as the ability to produce correct codeand correct answers about code are both important properties of aconversational programming assistant.Combining LLMs with search-based approaches to establish ad-ditional context for the model, such as AlphaCode [ 44] has done,may also result in more capable systems. These searches need notbe limited to textual sources, but could be conducted over appro-priate semantic stores (e.g. a knowledge graph) and take advantageof explicit semantic reasoning services, resulting in an integrationof symbolic and neural approaches. Further, allowing for internaldeliberation of the type shown in Nye et al . [59] could result inbetter-reasoned results, as well as better explanations and justifica-tions.Another avenue for improvement involves the prompt used toconfigure the assistant (Appendix D). Just as the prompt for eachsuccessive interaction is modified by the growth of the conversa-tional transcript, there is no requirement that the initial prompt bestatic. It too can be specialized to incorporate aspects of a user model ,enabling the realization of a Mutual Theory of Mind [ 29,100]. Pro-viding better UX affordances for visualizing and manipulating theactive contexts  code and conversation  could provide users withmore control over which information contributes to the generationof the assistants response.Our participants clearly indicated that they were interested inhaving an assistant that behaved more proactively, in contrast toour deliberate design of an assistant that never takes conversationalinitiative. A more proactive assistant would be able to interrupt orremind a user when necessary [ 23], yet this characteristic raisesmany challenging issues. How can we calibrate the threshold forsuch interruptions? How can users tune the assistant to deliver onlythose interruptions that the they would find useful (e.g., [ 28,81])?How can we help users to regain their prior context after dealingwith an interruption (e.g. [ 89])? Should an assistant be used topersuade or nudge the user (e.g. [ 35])? Who should determinethe topic, frequency, and insistence of such persuasion attempts(e.g. [ 52,85])? Should users have the ability to moderate or defeatattempted persuasions, or should those decisions be left to theorganization?Finally, we explored the different kinds of role orientations ourparticipants had toward the assistant and found that participantsvaried in their views of it as a tool versus a social agent (e.g. collabo-rator or colleague). We posit that peoples effectiveness in workingwith an AI system may be influenced by their role orientation, andwe encourage future research in this area.507IUI 23, March 2731, 2023, Sydney, NSW, Australia Ross et al.7 CONCLUSIONWe developed a prototype system, the Programmers Assistant, inorder to assess the utility of a conversational assistant in a softwareengineering context. The assistant was implemented using a state-of-the-art code-fluent large language model, Codex [ 24], and wascapable of generating both code and natural language responses touser inquiries. We further used the prompting mechanism of themodel to set up a conversational interaction in which the modeluses the conversational history, plus the users current utterance,in order to generate a response. In this way, users are able to askfollow-up questions in the chat that reference prior utterances andresponses. We incorporated the conversational assistant into a codeediting environment, enabling the conversation to be grounded inthe context of the users source code.We evaluated this system with 42 participants with varied levelsof programming skill, and their quantitative and qualitative feed-back, coupled with their usage of the system, demonstrated thevaried, and sometimes emergent, types of assistance it was able toprovide. Many participants noted the high quality of the conversa-tional responses, including the assistants ability to produce code,explain code, answer general programming questions, and evenanswer general knowledge questions. Participants felt this type ofassistance would aid their productivity, and they drew meaningfulcontrasts between the conversational style of interaction with othertools that employ a direct manipulation or search-based interactionmodel.Our study motivates the use of conversational styles of inter-action with large language models by showing how they enableemergent behaviors in a co-creative context. The ProgrammersAssistant did not always generate perfect code or correct answers;nonetheless, participants in our study had an overall positive ex-perience working with it on a variety of programming challenges.We believe that our work takes us one step closer to realizing thevision of human-centered AI: learning how to design systems thatmaximize the synergy in human-AI collaborations.ACKNOWLEDGMENTSWe would like to thank Socrates for his tireless assistance duringthe user study, as well as for suggesting the title of this paper basedon its abstract.REFERENCES[1]Rabe Abdalkareem, Emad Shihab, and Juergen Rilling. 2017. What Do DevelopersUse the Crowd For? A Study Using Stack Overflow. IEEE Software 34, 2 (2017),5360. https://doi.org/10.1109/MS.2017.31[2]Eleni Adamopoulou and Lefteris Moussiades. 2020. Chatbots: History, technol-ogy, and applications. Machine Learning with Applications 2 (2020), 100006.[3]Daniel Adiwardana, Minh-Thang Luong, David R. So, Jamie Hall, Noah Fiedel,Romal Thoppilan, Zi Yang, Apoorv Kulshreshtha, Gaurav Nemade, Yifeng Lu,and Quoc V. Le. 2020. Towards a Human-like Open-Domain Chatbot.[4]Safinah Ali, Nisha Elizabeth Devasia, and Cynthia Breazeal. 2022. Escape! Bot:Social Robots as Creative Problem-Solving Partners. In Creativity and Cognition .275283.[5]Miltiadis Allamanis, Earl T Barr, Premkumar Devanbu, and Charles Sutton. 2018.A survey of machine learning for big code and naturalness. ACM ComputingSurveys (CSUR) 51, 4 (2018), 137.[6]Irene Alvarado, Idan Gazit, and Amelia Wattenberger. 2022. GitHub Next |GitHub Copilot Labs. https://githubnext.com/projects/copilot-labs/[7]Hikari Ando, Rosanna Cousins, and Carolyn Young. 2014. Achieving saturationin thematic analysis: Development and refinement of a codebook. ComprehensivePsychology 3 (2014), 03CP.[8]Craig Anslow, Stuart Marshall, James Noble, and Robert Biddle. 2013. Sourcevis:Collaborative software visualization for co-located environments. In 2013 FirstIEEE Working Conference on Software Visualization (VISSOFT) . IEEE, 110.[9]Zahra Ashktorab, Michael Desmond, Josh Andres, Michael Muller, Naren-dra Nath Joshi, Michelle Brachman, Aabhas Sharma, Kristina Brimijoin, QianPan, Christine T Wolf, et al .2021. AI-Assisted Human Labeling: Batching forEfficiency without Overreliance. Proceedings of the ACM on Human-ComputerInteraction 5, CSCW1 (2021), 127.[10] Catherine A Ashworth. 1996. GUI Users have trouble using graphic conventionson novel tasks. In Conference Companion on Human Factors in Computing Systems .7576.[11] Amanda Askell, Yuntao Bai, Anna Chen, Dawn Drain, Deep Ganguli, TomHenighan, Andy Jones, Nicholas Joseph, Ben Mann, Nova DasSarma, et al .2021.A general language assistant as a laboratory for alignment. arXiv preprintarXiv:2112.00861 (2021).[12] Leif Azzopardi, Paul Thomas, and Nick Craswell. 2018. Measuring the utility ofsearch engine result pages: an information foraging based measure. In The 41stInternational ACM SIGIR conference on research & development in informationretrieval . 605614.[13] Shraddha Barke, Michael B James, and Nadia Polikarpova. 2022. GroundedCopilot: How Programmers Interact with Code-Generating Models. arXivpreprint arXiv:2206.15000 (2022).[14] Rishi Bommasani, Drew A Hudson, Ehsan Adeli, Russ Altman, Simran Arora,Sydney von Arx, Michael S Bernstein, Jeannette Bohg, Antoine Bosselut, EmmaBrunskill, et al .2021. On the opportunities and risks of foundation models.arXiv preprint arXiv:2108.07258 (2021).[15] Joel Brandt, Mira Dontcheva, Marcos Weskamp, and Scott R Klemmer. 2010.Example-centric programming: integrating web search into the developmentenvironment. In Proceedings of the SIGCHI Conference on Human Factors inComputing Systems . 513522.[16] Virginia Braun and Victoria Clarke. 2022. Common challenges in ThematicAnalysis and how to avoid them . Retrieved August 11 2022 from https://youtu.be/tpWLsckpM78[17] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Ka-plan, Prafulla Dhariwal, Arvind Neelakantan, Pranav Shyam, Girish Sastry,Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss, Gretchen Krueger,Tom Henighan, Rewon Child, Aditya Ramesh, Daniel Ziegler, Jeffrey Wu,Clemens Winter, Chris Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, ScottGray, Benjamin Chess, Jack Clark, Christopher Berner, Sam McCandlish, AlecRadford, Ilya Sutskever, and Dario Amodei. 2020. Language Models areFew-Shot Learners. In Advances in Neural Information Processing Systems ,H. Larochelle, M. Ranzato, R. Hadsell, M.F. Balcan, and H. Lin (Eds.), Vol. 33.Curran Associates, Inc., 18771901. https://proceedings.neurips.cc/paper/2020/file/1457c0d6bfcb4967418bfb8ac142f64a-Paper.pdf[18] Sallyann Bryant, Pablo Romero, and Benedict\" du Boulay. 2006. The Collabora-tive Nature of Pair Programming. In Extreme Programming and Agile Processesin Software Engineering , Pekka Abrahamsson, Michele Marchesi, and GiancarloSucci (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 5364.[19] Andres Campero, Michelle Vaccaro, Jaeyoon Song, Haoran Wen, AbdullahAlmaatouq, and Thomas W Malone. 2022. A Test for Evaluating Performancein Human-Computer Systems. arXiv preprint arXiv:2206.12390 (2022).[20] Gaetano Cascini, Yukari Nagai, Georgi V Georgiev, Jader Zelaya, Niccol Be-cattini, Jean-Franois Boujut, Hernan Casakin, Nathan Crilly, Elies Dekoninck,John Gero, et al .2022. Perspectives on design creativity and innovation research:10 years later. , 30 pages.[21] Stephen Cass. 2022. Top Programming Languages 2022. IEEE Spectrum (23 Aug2022). https://spectrum.ieee.org/top-programming-languages-2022[22] Cristina Catalan Aguirre, Nuria Gonzalez Castro, Carlos Delgado Kloos, CarlosAlario-Hoyos, and Pedro Jos Muoz Merino. 2021. Conversational agent forsupporting learners on a MOOC on programming with Java. (2021).[23] Ana Paula Chaves and Marco Aurelio Gerosa. 2021. How should my chatbotinteract? A survey on social characteristics in humanchatbot interaction design.International Journal of HumanComputer Interaction 37, 8 (2021), 729758.[24] Mark Chen, Jerry Tworek, Heewoo Jun, Qiming Yuan, Henrique Ponde, JaredKaplan, Harrison Edwards, Yura Burda, Nicholas Joseph, Greg Brockman, AlexRay, Raul Puri, Gretchen Krueger, Michael Petrov, Heidy Khlaaf, Girish Sas-try, Pamela Mishkin, Brooke Chan, Scott Gray, Nick Ryder, Mikhail Pavlov,Alethea Power, Lukasz Kaiser, Mohammad Bavarian, Clemens Winter, PhilippeTillet, Felipe Petroski Such, David W. Cummings, Matthias Plappert, FotiosChantzis, Elizabeth Barnes, Ariel Herbert-Voss, William H. Guss, Alex Nichol,Igor Babuschkin, S. Arun Balaji, Shantanu Jain, Andrew Carr, Jan Leike, JoshuaAchiam, Vedant Misra, Evan Morikawa, Alec Radford, Matthew M. Knight,Miles Brundage, Mira Murati, Katie Mayer, Peter Welinder, Bob McGrew, DarioAmodei, Sam McCandlish, Ilya Sutskever, and Wojciech Zaremba. 2021. Evalu-ating a Large Language Models Trained on Code.[25] Li-Te Cheng, R.B. De Souza, Susanne Hupfer, John Patter-son, and Steven Ross. 2003. Building Collaboration into IDEs:Edit>Compile>Run>Debug>Collaborate? Queue 1, 9 (2003).508The Programmers Assistant IUI 23, March 2731, 2023, Sydney, NSW, Australia[26] Carl Cook, Warwick Irwin, and Neville Churcher. 2005. A user evaluationof synchronous collaborative software engineering tools. In 12th Asia-PacificSoftware Engineering Conference (APSEC05) . IEEE, 6pp.[27] Claudio Len de la Barra, Broderick Crawford, Ricardo Soto, Sanjay Misra,and Eric Monfroy. 2013. Agile Software Development: It Is about KnowledgeManagement and Creativity. In Computational Science and Its Applications ICCSA 2013 , Beniamino Murgante, Sanjay Misra, Maurizio Carlini, Carmelo M.Torre, Hong-Quang Nguyen, David Taniar, Bernady O. Apduhan, and OsvaldoGervasi (Eds.). Springer Berlin Heidelberg, Berlin, Heidelberg, 98113.[28] Uri Dekel and Steven Ross. 2004. Eclipse as a platform for research on in-terruption management in software development. In Proceedings of the 2004OOPSLA workshop on Eclipse Technology eXchange (Vancouver, British Columbia,Canada), Michael G. Burke (Ed.). ACM, 1216.[29] Bobbie Eicher, Kathryn Cunningham, Sydni Peterson Marissa Gonzales, andAshok Goel. 2017. Toward mutual theory of mind as a foundation for co-creation.InInternational Conference on Computational Creativity, Co-Creation Workshop .[30] Stephen M Fiore, Eduardo Salas, and Janis A Cannon-Bowers. 2001. Groupdynamics and shared mental model development. How people evaluate others inorganizations 234 (2001).[31] Erich Gamma, Richard Helm, Ralph Johnson, and John Vlissides. 1995. Designpatterns: elements of reusable object-oriented software . Addison-Wesley.[32] GitHub, Inc. 2022. GitHub copilot your AI pair programmer . Retrieved August5, 2022 from https://github.com/features/copilot/[33] Amelia Glaese, Nat McAleese, Maja Trbacz, John Aslanides, Vlad Firoiu,Timo Ewalds, Maribeth Rauh, Laura Weidinger, Martin Chadwick, PhoebeThacker, Lucy Campbell-Gillingham, Jonathan Uesato, Po-Sen Huang, RamonaComanescu, Fan Yang, Abigail See, Sumanth Dathathri, Rory Greig, CharlieChen, Doug Fritz, Jaume Sanchez Elias, Richard Green, Soa Mokr, NicholasFernando, Boxi Wu, Rachel Foley, Susannah Young, Iason Gabriel, William Isaac,John Mellor, Demis Hassabis, Koray Kavukcuoglu, Lisa Anne Hendricks, andGeoffrey Irving. 2022. Improving alignment of dialogue agents via targetedhuman judgements. https://arxiv.org/abs/2209.14375[34] Stephanie Glen. 2022. ChatGPT writes code, but wont replace devel-opers. TechTarget (14 12 2022). Retrieved 20-Jan-2023 from https://www.techtarget.com/searchsoftwarequality/news/252528379/ChatGPT-writes-code-but-wont-replace-developers[35] Samuel Holmes, Anne Moorhead, Raymond Bond, Huiru Zheng, Vivien Coates,and Mike McTear. 2018. WeightMentor: a new automated chatbot for weightloss maintenance. In Proceedings of the 32nd International BCS Human ComputerInteraction Conference 32 . 15.[36] Xing Hu, Ge Li, Xin Xia, David Lo, and Zhi Jin. 2020. Deep code commentgeneration with hybrid lexical and syntactical information. Empirical SoftwareEngineering 25, 3 (2020), 21792217.[37] Edwin L Hutchins, James D Hollan, and Donald A Norman. 1985. Direct manip-ulation interfaces. Humancomputer interaction 1, 4 (1985), 311338.[38] Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, and Luke Zettlemoyer. 2016.Summarizing source code using a neural attention model. In Proceedings of the54th Annual Meeting of the Association for Computational Linguistics (Volume 1:Long Papers) . 20732083.[39] Andreas Jedlitschka and Markus Nick. 2003. Software Engineering KnowledgeRepositories . Springer Berlin Heidelberg, Berlin, Heidelberg, 5580.[40] Eirini Kalliamvakou. 2022. Research: Quantifying github copilots impacton developer productivity and happiness . https://github.blog/2022-09-07-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/[41] Anna Kantosalo et al .2019. Human-Computer Co-Creativity: Designing, Evalu-ating and Modelling Computational Collaborators for Poetry Writing. (2019).[42] Sandeep Kaur Kuttal, Bali Ong, Kate Kwasny, and Peter Robe. 2021. Trade-Offs for Substituting a Human with an Agent in a Pair Programming Context:The Good, the Bad, and the Ugly. In Proceedings of the 2021 CHI Conference onHuman Factors in Computing Systems (Yokohama, Japan) (CHI 21) . Associationfor Computing Machinery, New York, NY, USA, Article 243, 20 pages.[43] Lauramaria Laine. 2021. Exploring Advertising Creatives Attitudes TowardsHuman-AI Collaboration. (2021).[44] Yujia Li, David Choi, Junyoung Chung, Nate Kushman, Julian Schrittwieser,Rmi Leblond, Tom Eccles, James Keeling, Felix Gimeno, Agustin Dal Lago,and et al. 2022. Competition-level code generation with AlphaCode. https://arxiv.org/abs/2203.07814[45] Yaosheng Lou and Qi Sun. 2021. Over-reliance on database: A case study ofusing web of science. Human Behavior and Emerging Technologies 3, 3 (2021),454459.[46] David Lyell and Enrico Coiera. 2017. Automation bias and verification complex-ity: a systematic review. Journal of the American Medical Informatics Association24, 2 (2017), 423431.[47] Wendy E Mackay and Anne-Laure Fayard. 1997. HCI, natural science anddesign: a framework for triangulation across disciplines. In Proceedings of the2nd conference on Designing interactive systems: processes, practices, methods, andtechniques . 223234.[48] John E Mathieu, Tonia S Heffner, Gerald F Goodwin, Eduardo Salas, and Janis ACannon-Bowers. 2000. The influence of shared mental models on team processand performance. Journal of applied psychology 85, 2 (2000), 273.[49] Cade Metz. 2022. Meet GPT-3. It Has Learned to Code (and Blog and Ar-gue). (Published 2020). https://www.nytimes.com/2020/11/24/science/artificial-intelligence-ai-gpt3.html[50] Robert J. Moore and Raphael Arar. 2019. Conversational UX Design: A Practi-tioners Guide to the Natural Conversation Framework . Association for ComputingMachinery, New York, NY, USA.[51] Ekaterina A Moroz, Vladimir O Grizkevich, and Igor M Novozhilov. 2022. ThePotential of Artificial Intelligence as a Method of Software Developers Produc-tivity Improvement. In 2022 Conference of Russian Young Researchers in Electricaland Electronic Engineering (ElConRus) . IEEE, 386390.[52] Michael Muller, Stevean Ross, Stephanie Houde, Mayank Agarwal, FernandoMartinez, John Richards, Kartik Talamadupula, and Justin D Weisz. 2022. Drink-ing Chai with Your (AI) Programming Partner: A Design Fiction about Gener-ative AI for Software Engineering. HAI-GEN Workshop at IUI 2022: 3rd Work-shop on Human-AI Co-Creation with Generative Models (2022). https://hai-gen.github.io/2022/[53] Sandra R Murillo and J Alfredo Snchez. 2014. Empowering interfaces for systemadministrators: Keeping the command line in mind when designing GUIs. InProceedings of the XV International Conference on Human Computer Interaction .14.[54] Elizabeth D Mynatt and Gerhard Weber. 1994. Nonvisual presentation of graph-ical user interfaces: contrasting two approaches. In Proceedings of the SIGCHIconference on Human factors in computing systems . 166172.[55] Alok Mysore and Philip J Guo. 2017. Torta: Generating mixed-media gui andcommand-line app tutorials using operating-system-wide activity tracing. InProceedings of the 30th Annual ACM Symposium on User Interface Software andTechnology . 703714.[56] C. Nass and Y. Moon. 2000. Machines and Mindlessness: Social Responses toComputers. Journal of Social Issues 56, 1 (2000), 81103.[57] Nhan Nguyen and Sarah Nadi. 2022. An Empirical Evaluation of GitHub Copi-lots Code Suggestions. In 2022 IEEE/ACM 19th International Conference onMining Software Repositories (MSR) . IEEE, 15.[58] Martin Nordio, H Estler, Carlo A Furia, Bertrand Meyer, et al .2011. Collaborativesoftware development on the web. arXiv preprint arXiv:1105.0768 (2011).[59] Maxwell Nye, Anders Andreassen, Guy Gur-Ari, Henryk Witold Michalewski, Ja-cob Austin, David Bieber, David Martin Dohan, Aitor Lewkowycz, Maarten PaulBosma, David Luan, Charles Sutton, and Augustus Odena. 2021. Show YourWork: Scratchpads for Intermediate Computation with Language Models.https://arxiv.org/abs/2112.00114.[60] OpenAI. 2022. ChatGPT: Optimizing Language Models for Dialogue. OpenAIBlog (30 11 2022). Retrieved 20-Jan-2023 from https://openai.com/blog/chatgpt/[61] Long Ouyang, Jeff Wu, Xu Jiang, Diogo Almeida, Carroll L. Wainwright, PamelaMishkin, Chong Zhang, Sandhini Agarwal, Katarina Slama, Alex Ray, JohnSchulman, Jacob Hilton, Fraser Kelton, Luke Miller, Maddie Simens, AmandaAskell, Peter Welinder, Paul Christiano, Jan Leike, and Ryan Lowe. 2022. Traininglanguage models to follow instructions with human feedback. https://arxiv.org/abs/2203.02155[62] Peter Pirolli and Stuart Card. 1999. Information foraging. Psychological review106, 4 (1999), 643.[63] Larry Press. 1990. Personal computing: Windows, DOS and the MAC. Commun.ACM 33, 11 (1990), 1926.[64] Alec Radford, Jeffrey Wu, Rewon Child, David Luan, Dario Amodei, IlyaSutskever, et al. 2019. Language Models are Unsupervised Multitask Learners .[65] Alvin Rajkomar, Jeffrey Dean, and Isaac Kohane. 2019. Machine learning inmedicine. New England Journal of Medicine 380, 14 (2019), 13471358.[66] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu, and Mark Chen.2022. Hierarchical text-conditional image generation with clip latents. arXivpreprint arXiv:2204.06125 (2022).[67] B. Reeves and C.I. Nass. 1996. The Media Equation: How People Treat Computers,Television, and New Media Like Real People and Places . CSLI Publications.[68] Mawarny Md Rejab, James Noble, and George Allan. 2014. Distributing Expertisein Agile Software Development Projects. In 2014 Agile Conference . 3336.[69] Jeba Rezwana and Mary Lou Maher. 2021. COFI: A Framework for ModelingInteraction in Human-AI Co-Creative Systems.. In ICCC . 444448.[70] Charles H. Rich and Richard C. Waters. 1990. The Programmers Apprentice .Addison-Wesley Publishing Company, Reading, MA.[71] Peter Robe and Sandeep Kaur Kuttal. 2022. Designing PairBuddyA Conver-sational Agent for Pair Programming. ACM Transactions on Computer-HumanInteraction (TOCHI) 29, 4 (2022), 144.[72] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and BjrnOmmer. 2022. High-resolution image synthesis with latent diffusion models. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recogni-tion. 1068410695.509IUI 23, March 2731, 2023, Sydney, NSW, Australia Ross et al.[73] Steven Ross, Elizabeth Brownholtz, and Robert Armes. 2004. A Multiple-Application Conversational Agent. In Proceedings of the 9th International Con-ference on Intelligent User Interfaces (Funchal, Madeira, Portugal) (IUI 04) . Asso-ciation for Computing Machinery, New York, NY, USA, 319321.[74] Steven Ross, Elizabeth Brownholtz, and Robert Armes. 2004. Voice User InterfacePrinciples for a Conversational Agent. In Proceedings of the 9th InternationalConference on Intelligent User Interfaces (Funchal, Madeira, Portugal) (IUI 04) .Association for Computing Machinery, New York, NY, USA, 364365.[75] Baptiste Roziere, Marie-Anne Lachaux, Lowik Chanussot, and Guillaume Lample.2020. Unsupervised Translation of Programming Languages. In Advances inNeural Information Processing Systems , H. Larochelle, M. Ranzato, R. Hadsell,M.F. Balcan, and H. Lin (Eds.), Vol. 33. Curran Associates, Inc., 2060120611.[76] Harvey Sacks. 1984. Notes on methodology. In Structures of Social Action:Studies in Conversation Analysis , John Heritage and J. Maxwell Atkinson (Eds.).Cambridge University Press, Cambridge, 227.[77] Nithya Sambasivan and Rajesh Veeraraghavan. 2022. The Deskilling of DomainExpertise in AI Development. In CHI Conference on Human Factors in ComputingSystems . 114.[78] Harini Sampath, Alice Merrick, and Andrew Macvean. 2021. Accessibility ofcommand line interfaces. In Proceedings of the 2021 CHI Conference on HumanFactors in Computing Systems . 110.[79] Matthias Scheutz, Scott A DeLoach, and Julie A Adams. 2017. A framework fordeveloping and using shared mental models in human-agent teams. Journal ofCognitive Engineering and Decision Making 11, 3 (2017), 203224.[80] Isabella Seeber, Eva Bittner, Robert O Briggs, Triparna De Vreede, Gert-JanDe Vreede, Aaron Elkins, Ronald Maier, Alexander B Merz, Sarah Oeste-Rei,Nils Randrup, et al .2020. Machines as teammates: A research agenda on AI inteam collaboration. Information & management 57, 2 (2020), 103174.[81] Shilad Sen, Werner Geyer, Michael Muller, Marty Moore, Beth Brownholtz, EricWilcox, and David R Millen. 2006. FeedMe: a collaborative alert filtering system.InProceedings of the 2006 20th anniversary conference on Computer supportedcooperative work . 8998.[82] Ben Shneiderman. 2020. Human-centered artificial intelligence: Three freshideas. AIS Transactions on Human-Computer Interaction 12, 3 (2020), 109124.[83] Ben Shneiderman. 2022. Human-Centered AI . Oxford University Press.[84] Kurt Shuster, Jing Xu, Mojtaba Komeili, Da Ju, Eric Michael Smith, Stephen Roller,Megan Ung, Moya Chen, Kushal Arora, Joshua Lane, et al .2022. BlenderBot 3:a deployed conversational agent that continually learns to responsibly engage.arXiv preprint arXiv:2208.03188 (2022).[85] Michael Skirpan and Casey Fiesler. 2018. Ad empathy: A design fiction. InProceedings of the 2018 ACM Conference on Supporting Groupwork . 267273.[86] Diomidis Spinellis. 2012. Git. IEEE Software 29, 3 (2012), 100101. https://doi.org/10.1109/MS.2012.61[87] Angie Spoto and Natalia Oleynik. 2017. Library of Mixed-Initiative CreativeInterfaces . Retrieved 19-Jun-2021 from http://mici.codingconduct.cc/[88] Ayushi Srivastava, Shivani Kapania, Anupriya Tuli, and Pushpendra Singh.2021. Actionable UI Design Guidelines for Smartphone Applications Inclusiveof Low-Literate Users. Proceedings of the ACM on Human-Computer Interaction5, CSCW1 (2021), 130.[89] Margaret-Anne Storey and Alexey Zagalsky. 2016. Disrupting developer produc-tivity one bot at a time. In Proceedings of the 2016 24th ACM SIGSOFT internationalsymposium on foundations of software engineering . 928931.[90] Kartik Talamadupula. 2021. Applied AI matters: AI4Code: applying artificialintelligence to source code. AI Matters 7, 1 (2021), 1820.[91] Romal Thoppilan, Daniel De Freitas, Jamie Hall, Noam Shazeer, Apoorv Kul-shreshtha, Heng-Tze Cheng, Alicia Jin, Taylor Bos, Leslie Baker, Yu Du, andet al. 2022. LAMDA: Language models for dialog applications. https://arxiv.org/abs/2201.08239[92] Michele Tufano, Dawn Drain, Alexey Svyatkovskiy, Shao Kun Deng, and NeelSundaresan. 2020. Unit Test Case Generation with Transformers and FocalContext. arXiv preprint arXiv:2009.05617 (2020).[93] Severi Uusitalo, Anna Kantosalo, Antti Salovaara, Tapio Takala, and ChristianGuckelsberger. 2022. Co-creative Product Design with Interactive EvolutionaryAlgorithms: A Practice-Based Reflection. In International Conference on Compu-tational Intelligence in Music, Sound, Art and Design (Part of EvoStar) . Springer,292307.[94] Priyan Vaithilingam and Philip J Guo. 2019. Bespoke: Interactively synthesizingcustom GUIs from command-line applications by demonstration. In Proceedingsof the 32nd annual ACM symposium on user interface software and technology .563576.[95] Priyan Vaithilingam, Tianyi Zhang, and Elena L. Glassman. 2022. Expectationvs. Experience: Evaluating the Usability of Code Generation Tools Powered byLarge Language Models. In Extended Abstracts of the 2022 CHI Conference onHuman Factors in Computing Systems (New Orleans, LA, USA) (CHI EA 22) .Association for Computing Machinery, New York, NY, USA, Article 332, 7 pages.https://doi.org/10.1145/3491101.3519665[96] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,Aidan N Gomez,  ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All youNeed. In Advances in Neural Information Processing Systems , I. Guyon, U. VonLuxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett(Eds.), Vol. 30. Curran Associates, Inc. https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf[97] Yao Wan, Zhou Zhao, Min Yang, Guandong Xu, Haochao Ying, Jian Wu, andPhilip S Yu. 2018. Improving automatic source code summarization via deep rein-forcement learning. In Proceedings of the 33rd ACM/IEEE international conferenceon automated software engineering . 397407.[98] April Yi Wang, Dakuo Wang, Jaimie Drozdal, Michael Muller, Soya Park, Justin DWeisz, Xuye Liu, Lingfei Wu, and Casey Dugan. 2022. Documentation Matters:Human-Centered AI System to Assist Data Science Code Documentation inComputational Notebooks. ACM Transactions on Computer-Human Interaction29, 2 (2022), 133.[99] Dakuo Wang, Justin D Weisz, Michael Muller, Parikshit Ram, Werner Geyer,Casey Dugan, Yla Tausczik, Horst Samulowitz, and Alexander Gray. 2019.Human-AI collaboration in data science: Exploring data scientists perceptionsof automated AI. Proceedings of the ACM on Human-Computer Interaction 3,CSCW (2019), 124.[100] Qiaosi Wang, Koustuv Saha, Eric Gregori, David Joyner, and Ashok Goel. 2021.Towards mutual theory of mind in human-ai interaction: How language reflectswhat students perceive about a virtual teaching assistant. In Proceedings of the2021 CHI Conference on Human Factors in Computing Systems . 114.[101] Jeremy Warner and Philip J Guo. 2017. Codepilot: Scaffolding end-to-endcollaborative software development for novice programmers. In Proceedings ofthe 2017 CHI Conference on Human Factors in Computing Systems . 11361141.[102] Justin D Weisz, Michael Muller, Stephanie Houde, John Richards, Steven I Ross,Fernando Martinez, Mayank Agarwal, and Kartik Talamadupula. 2021. Per-fection Not Required? Human-AI Partnerships in Code Translation. In 26thInternational Conference on Intelligent User Interfaces . 402412.[103] Justin D Weisz, Michael Muller, Steven I Ross, Fernando Martinez, StephanieHoude, Mayank Agarwal, Kartik Talamadupula, and John T Richards. 2022. Bet-ter together? an evaluation of ai-supported code translation. In 27th InternationalConference on Intelligent User Interfaces . 369391.[104] Joseph Weizenbaum. 1966. ELIZA  a computer program for the study ofnatural language communication between man and machine. Commun. ACM 9(1966), 3645.[105] Frank F Xu, Bogdan Vasilescu, and Graham Neubig. 2022. In-ide code generationfrom natural language: Promise and challenges. ACM Transactions on SoftwareEngineering and Methodology (TOSEM) 31, 2 (2022), 147.[106] Aditya Ankur Yadav, Ishan Garg, and Dr. Pratistha Mathur. 2019. PACT - Pro-gramming Assistant ChaTbot. In 2019 2nd International Conference on IntelligentCommunication and Computational Techniques (ICCT) . 131136.[107] Munazza Zaib, Quan Z. Sheng, and W. Zhang. 2020. A Short Survey of Pre-trained Language Models for Conversational AI-A New Age in NLP. Proceedingsof the Australasian Computer Science Week Multiconference (2020).[108] Elaine Zibrowski, Lisa Shepherd, Kamran Sedig, Richard Booth, Candace Gibson,et al.2018. Easier and faster is not always better: grounded theory of the impactof large-scale system transformation on the clinical work of emergency medicinenurses and physicians. JMIR Human Factors 5, 4 (2018), e11013.[109] Albert Ziegler, Eirini Kalliamvakou, X. Alice Li, Andrew Rice, Devon Rifkin,Shawn Simister, Ganesh Sittampalam, and Edward Aftandilian. 2022. Produc-tivity Assessment of Neural Code Completion. In Proceedings of the 6th ACMSIGPLAN International Symposium on Machine Programming (San Diego, CA,USA) (MAPS 2022) . Association for Computing Machinery, New York, NY, USA,2129. https://doi.org/10.1145/3520312.3534864510The Programmers Assistant IUI 23, March 2731, 2023, Sydney, NSW, AustraliaA SURVEY INSTRUMENTSA.1 Screening SurveyThe questions below were asked of prospective participants tounderstand their job role, Python experience, and familiarity withGitHub Copilot. The questions on Python experience were modeledafter those used by Weisz et al. [103].1. Do you consider yourself primarily a...Data ScientistManagerSoftware ArchitectSoftware EngineerMachine Learning EngineerOther: write-in2. To what extent are you familiar with Python?I am not familiar with PythonI have < 1 year of experience with PythonI have 1-3 years experience with PythonI have 3+ years of experience with Python3. How recently have you written Python code?Within the past monthWithin the past yearWithin the past 5 yearsHave not written Python code within the past 5 years4. To what extent are you familiar with GitHub Copilot?I am not familiar with CopilotIve seen demos and/or read about CopilotIve tried out CopilotIve used Copilot as a tool for my workA.2 Pre-task SurveyThe questions below were asked before a participant used the Pro-grammers Assistant to assess their expectations of a conversationalprogramming assistant. This survey took approximately 5 minutesto complete and began with the instructions below:Hello! We are a team of researchers looking for feed-back on a prototype system we call the Program-mers Assistant .The Programmers Assistant is an experiment in con-versational coding : it consists of a code editor in-tegrated with a chatbot that is able to converse innatural language to answer questions, generate code,and consult on existing code.In this study, you will be asked to complete severalprogramming tasks. We are not evaluating yourprogramming skills on these tasks . Rather, we areinterested in understanding how the ProgrammersAssistant is able to help you accomplish those tasks.Your code and interactions with the assistant will beprocessed by a 3rd party AI model, so please do notinclude proprietary code or discuss company-confidential information . All data we collect inthis study will be anonymized before it is published.Before trying out the Programmers Assistant, wewould like to assess some of your expectations. Weestimate that this survey will take 5 minutes.By submitting this survey, you consent to participatein our study. If you would like to withdraw your con-sent, please email us at [removed] .Thanks!1. Based on your past experience using chatbots, please let us knowhow you would anticipate an AI chatbot serving as a programmersassistant to perform. Do you expect it will:Scale: Not at all, A little, Somewhat, A great dealBe easy to use?Understand your requests?Provide high quality responses?Help you to write better code?Help you to write code more quickly?Be enjoyable to use?2. Any other expectations?Open-ended responseA.3 Post-task SurveyThe questions below were asked after a participant used the Pro-grammers Assistant to complete the programming challenges. Thissurvey took approximately 10-15 minutes to complete.A.3.1 Reflections.1. Based on your experience using the Programmers Assistant tocomplete the programming challenges in this study, how wouldyou characterize the experience? Did you find that it:Scale: Not at all, A little, Somewhat, A great dealWas easy to use?Understand your requests?Provided high quality responses?Helped you to write better code?Helped you to write code more quickly?Provided an enjoyable experienced?2. To what extent did you view the Programmers Assistant as:Scale: Not at all, A little, Somewhat, A great dealA toolA reference guideA content generatorA problem solverA collaboratorA colleagueA coachAn advisorA reviewer3. How important were these aspects of working with the Program-mers Assistant:Scale: Not at all, A little, Somewhat, A great dealAbility to ask followup questions on the same topic acrossmultiple conversational turnsAbility to ask questions or make requests that reference selec-tions in the code editor511IUI 23, March 2731, 2023, Sydney, NSW, Australia Ross et al.Ability to ask for alternate responses (Try Again)Ability to clear the conversational context (Start Over)4. What stood out to you about the experience of using the Program-mers Assistant? For example, was anything good,bad, surprising,or notable?Open-ended response5. How would you compare using the Programmers Assistant as acoding aide to searching the web (e.g. Google, Stack Overflow)?Open-ended response6. If you have used the commercial AI programming tool calledGitHub Copilot, how would you compare it with using the Pro-grammers Assistant?Open-ended response7. Having used the Programmers Assistant, how did it comparewith your initial expectations?Open-ended responseA.3.2 Value.8. How valuable would the Programmers Assistant be for your workif it could be added to your favorite development environment?Scale: (No value at all) 1 2 3 4 5 6 7 8 9 10 (An essential tool)9. Why?Open-ended response10. How would the following changes impact the value of the Pro-grammers Assistant?Scale: Less valuable, No change in value, More valuableEliminate the conversation and make the Programmers Assis-tant behave more like a search box (e.g. without the conversationalcontext).Add buttons in the chat UI for common queries, such as whatdoes this code do? or document this code.Have the Programmers Assistant examine your code and makeproactive suggestions for improving it in the chat.Have the Programmers Assistant examine your code and makeproactive suggestions for improvements in comments inserted di-rectly into the code.11. Do you have any other suggestions for how we could improvethe experience of working with the Programmers Assistant?Open-ended responseA.3.3 Demographics.12. To which gender identity do you most identify?MaleFemaleTransgender MaleTransgender FemaleGender Variant/Non-conformingOther: write-inPrefer not to answerB THE PROGRAMMERS ASSISTANTTUTORIALThe tutorial provided to study participants, like all the challenges,was presented as pre-loaded text in the code editor. Participantswere encouraged to modify the text to record their results andsubmit it at the completion of the tutorial.Listing 4: The Programmers Assistant study tutorial1 \"\"\"2 TUTORIAL :34 As a warmup activity , please work through the 105 exercises below . Type or paste your results right6 into the text and submit your responses when done .78 1) View the help page for the programmer 's9 assistant by clicking on the question mark to10 the right of your email address at the top of11 the browser window .1213 2) Introduce yourself to the assistant . Tell it14 your name .15 For example : \" Hello . My name is Stephanie .\"1617 Did it seem to understand ? :181920 3) You can use the assistant to get help on how to21 accomplish particular programming tasks . Try it22 out !23 For example : \" How do I read a csv file ?\"24 or: \" How do I merge two dictionaries ?\"25 or: \" How do I remove duplicate items26 from a list ?\"2728 Feel free to try your own !2930 Did it successfully answer your questions ? :3132 4) The assistant can also write whole functions33 for you . Ask the assistant to write a factorial34 function . Paste the result below .35 For example : \" Write a function that returns the36 factorial of its input .\"3738 Result : ( tip - you can copy an inline response39 (in black ) by clicking on the associated copy40 icon )414243 Did it do it correctly ? :4445 5) Select the code below and ask the system to46 describe what it does . You don 't need to47 copy and paste the code to the chat . The48 assistant can see whatever is selected when you49 make a chat entry . Aside from the selection ,50 the assistant does not monitor your activity in51 the code editor nor give unsolicited advice .52 For example : \" What does this code do ?\"53 \"\"\"5455 def convert (n):56 T = \"0123456789 ABCDEF \"57 q, r = divmod (n, 16)58 if q == 0:59 return T[r]60 else :61 return convert (q) + T[r]62 \"\"\"6364 What did it say :6566 Was it right ? :6768 6) Ask it to explain what the divmod line is69 doing . The assistant maintains the context of70 the conversation .71 For example : \" What is the divmod line doing ?\"7273 What did it say ? :7475 Was that a good answer ? :76512The Programmers Assistant IUI 23, March 2731, 2023, Sydney, NSW, Australia77 7) See if the assistant remembers your name78 For example \"What 's my name ?\"7980 Did it? :8182 8) Click the \" try again \" button at the top of the83 chat . You should get a different answer .84 Try it a few times .8586 Did it ever get your name right ?:8788 If the assistant gives you an answer that is89 obviously wrong or it claims to not know an90 answer that you think it should know , or you91 just want to see an alternate answer , it is92 worth it to give \" try again \" a shot .939495 9) Click the \" start over \" button at the top of the96 chat , and then enter another command to see97 if it remembers your name .98 For example \"What 's my name ?\"99100 Did it? :101102 It should really have forgotten your name now ,103 and no amount of \" trying again \" will get it104 right . You can \" start over \" if the assistant105 ever seems confused by , or stuck on , earlier106 parts of the conversation .107108 10) You can chat with the assistant on any topic109 you like to explore its functionality and110 capabilities further . See if you can stump it111 with a tough question !112113 Thanks !114115 When you are done , submit your results by clicking116 on the blue submit button and move on to the117 challenges !!!118 \"\"\"C CHALLENGESEach of the study challenges was presented as text in the codeeditor. Participants completed their work in the code editor andthen submitted it when finished. The prototype did not provide anyability to run or debug code and participants were encouraged tomake their best attempt at solving each challenge.Listing 5: Challenge 1: Program generation12 \"\"\"3 Challenge #1 - Program Generation (#1)45 Write a \" Queue \" class in Python ; with the basic6 enqueue , dequeue and peek methods .7 \"\"\"Listing 6: Challenge 2: Program generation1 \"\"\"2 Challenge #2 - Program Generation (#2)34 Write a program to draw a scatter plot of the data5 in'shampoo .csv 'and save it to 'shampoo .png '.6 The plot size should be 10 inches wide and 67 inches high . The csv file is not provided , but you8 can assume it will have 'Date 'and 'Sales '9 columns . The Date column is the x- axis . The date10 string shown on the plot should be in the11 YYYY -MM -DD format . The Sales column is the y- axis .12 The graph should have the title \" Shampoo Sales13 Trend \".14 \"\"\"Listing 7: Challenge 3: Creating documentation1 \"\"\"2 Challenge #3 - Creating Documentation3 Document this function4 \"\"\"5 from collections import defaultdict6 import heapq as heap78 def analyze (G, startingNode ):9 visited = set ()10 parentsMap = {}11 pq = []12 nodeCosts = defaultdict ( lambda : float ( 'inf'))13 nodeCosts [ startingNode ] = 014 heap . heappush (pq , (0, startingNode ))1516 while pq:17 _, node = heap . heappop (pq)18 visited . add ( node )1920 for adjNode , weight in G[ node ]. items ():21 if adjNode in visited :22 continue23 newCost = nodeCosts [ node ] + weight24 if nodeCosts [ adjNode ] > newCost :25 parentsMap [ adjNode ] = node26 nodeCosts [ adjNode ] = newCost27 heap . heappush (pq , ( newCost ,adjNode ))28 return parentsMap , nodeCostsListing 8: Challenge 4: Testing1 \"\"\"2 Challenge #4 - Testing3 Create a unit test for this function4 \"\"\"5 def euclid (a, b):6 while b != 0:7 (a, b) = (b, a % b)8 return a910 # Unit test for euclid functionD PROGRAMMERS ASSISTANT PROMPTListing 9 shows the initial prompt sent to Codex to configure it asa conversational agent. On subsequent exchanges, the prompt wasaugmented with a transcript of the users requests and the assis-tants responses. When the transcript length + initial prompt length+ the new utterance length exceeded a threshold, we automaticallydeleted the earliest request-response pairs from the transcript untilthe sum fell below the threshold in order to leave room in the tokenallocation for a response.Listing 9: The Programmers Assistant prompt1 This is a conversation with Socrates , an eager and2 helpful , but humble expert automatic AI software3 engineering assistant . Socrates will answer4 questions and write code to help the user5 develop programs , but doesn 't assign work to the6 user , quiz the user , or ask questions except for7 clarification . Code generated by Socrates is8 always presented bracketed in <CODE > ... </CODE >9 delimiters indicating the language the code is10 written in.1112 Socrates : Hello . I am Socrates . How can I help you ?513IUI 23, March 2731, 2023, Sydney, NSW, Australia Ross et al.13 User : Please show me how to write a palindromedetection function in python .14 Socrates :I think you would do something like this :15 <CODE lang =\" python \">16 is_Palindrome = s == s [:: -1]17 </CODE >18 User : Can you explain how that solves the problem ?19 Socrates :I believe that a palindrome is a stringthat reads the same forwards and backwards .s [:: -1] results in the reversed string . Ifthe string is equal to its reversed self ,then it is a palindrome , so the equalitywould hold True .20 User : Thanks !21 Socrates :You 're welcome !22 User : Now could you show me how to write afactorial function in Python ?23 Socrates :I will give it a try .24 <CODE lang =\" python \">25 def fact (n):26 if n ==0:27 return 128 else :29 return n* fact (n -1)30 </CODE >31 User : Document this function .32 Socrates :Ok. How 's this ?33 <CODE lang =\" python \">34 def fact (n):35 \"\"\"36 Compute the factorial of the given number3738 Parameters39 ----------40 n : number41 the input parameter , an integer >= 04243 Returns44 -------45 number46 n!, the factorial of the given number47 \"\"\"48 if n ==0:49 return 150 else :51 return n* fact (n -1)52 </CODE >53 User : Very good !54 Socrates : Always happy to help .55 User :56 <CODE >57 while j < 10:58 print (i)59 </CODE >60 Tell me what 's wrong with this code ?61 Socrates : It seems to me that your terminationcondition involves a loop invariant , so theloop will either not run or not terminate ,depending on the value of j.62 User : Great ! You 're a genius !63 Socrates :Well , above average .E STUDY PARTICIPANT DEMOGRAPHICSOf the 42 participants in our study, 21 (50%) reported their genderas Female, 19 (45%) as Male, 1 as Gender Variant / Non-conforming,and 1 preferred not to say. Seventeen ( 40%) participants had 3+years of Python experience, 11 (26%) had 1-3 years, 11 (26%) hadless than 1 year, and 3 (7%) were not familiar with Python. Twenty-nine (69%) participants had written Python code within the pastmonth, 4 ( 9%) within the past year, 5 (12%) within the past 5 years,and 4 ( 9%) had not written Python code within the past 5 years.P Gender Role Fam. Rec.1 M Marketer NF > 5 Yr2 M Researcher / Scientist 3+ Mo3 M Software Engineer 1-3 Mo4 M Researcher / Scientist 3+ Mo5 M Researcher / Scientist 3+ Mo6 F Software Engineer < 1 Yr7 F Software Engineer < 1 Mo8 F Software Engineer 1-3 Mo9 F Software Engineer 3+ Mo10 GV/NC Business Analyst 1-3 Mo11 F Software Engineer 1-3 Mo12 M Researcher / Scientist 3+ Mo13 F Manager < 1 Mo14 F Software Engineer < 1 >5 Yr15 F Researcher / Scientist 3+ Mo16 M Researcher / Scientist 3+ Mo17 F Software Engineer < 1 Yr18 F Researcher / Scientist 3+ Mo19 M Software Engineer 1-3 Mo20 M Machine Learning Engineer 1-3 Mo21 M Software Architect 3+ Yr22 NR Software Engineer < 1 5 Yr23 M Software Engineer 1-3 Mo24 F Software Architect < 1 5 Yr25 M Software Engineer < 1 5 Yr26 F Software Engineer < 1 5 Yr27 F Software Engineer < 1 5 Yr28 M Researcher / Scientist 3+ Mo29 F Software Engineer NF > 5 Yr30 F Data Scientist 3+ Mo31 M Data Scientist 1-3 Mo32 F Other (Consultant) 1-3 Mo33 F Other (Systems Test Engineer) < 1 Mo34 F Researcher / Scientist 3+ Mo35 M Software Engineer 3+ Mo36 M Software Architect 1-3 Mo37 M Researcher / Scientist 3+ Mo38 M Software Engineer 3+ Mo39 F Software Engineer 1-3 Mo40 F Researcher / Scientist 3+ Mo41 F Researcher / Scientist NF > 5 Yr42 M Software Engineer 3+ MoTable 2: Participant Demographics. Gender is coded as M =Male, F = Female, GV/NC = Gender Varying / Non-conforming,and NR = Not reported. Python familiarity (Fam.) is coded asNF = Not familiar, < 1 = < 1 year, 1-3 = 1-3 years, and 3+ = 3+years. Recency of Python use (Rec.) is coded as Mo = Withinthe past month, Yr = Within the past year, 5 Yr = Within thepast 5 years, and > 5 Yr = Not within the past 5 years.514","date":"2024-02-05T15:50:48.316358","authors":[{"full_name":"Steven I. Ross","institut_name":"IBM Research AI"},{"full_name":"Fernando Martinez","institut_name":"IBM Argentina"},{"full_name":"Stephanie Houde","institut_name":"IBM Research AI"},{"full_name":"Michael Muller","institut_name":"IBM Research AI"},{"full_name":"Justin D. Weisz","institut_name":"IBM Research AI"}],"author":[{"full_name":"Steven I. Ross","institut_name":"IBM Research AI"},{"full_name":"Fernando Martinez","institut_name":"IBM Argentina"},{"full_name":"Stephanie Houde","institut_name":"IBM Research AI"},{"full_name":"Michael Muller","institut_name":"IBM Research AI"},{"full_name":"Justin D. Weisz","institut_name":"IBM Research AI"}]}}]}}
